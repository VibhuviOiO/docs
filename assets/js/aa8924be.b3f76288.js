"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5061],{8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},9623:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Build-ML-Tools/cuda-tensorrt","title":"CUDA & TensorRT","description":"CUDA and TensorRT are NVIDIA\'s platforms for GPU computing and deep learning inference optimization, enabling high-performance AI applications.","source":"@site/docs/Build-ML-Tools/cuda-tensorrt.md","sourceDirName":"Build-ML-Tools","slug":"/BuildMLTools/CUDA-TensorRT","permalink":"/docs/docs/BuildMLTools/CUDA-TensorRT","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Build-ML-Tools/cuda-tensorrt.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"CUDA & TensorRT","description":"CUDA and TensorRT are NVIDIA\'s platforms for GPU computing and deep learning inference optimization, enabling high-performance AI applications.","slug":"/BuildMLTools/CUDA-TensorRT","keywords":["CUDA","TensorRT","GPU computing","deep learning inference","NVIDIA","performance optimization","AI acceleration","neural network optimization"]},"sidebar":"tutorialSidebar","previous":{"title":"Kubeflow","permalink":"/docs/docs/BuildMLTools/Kubeflow"},"next":{"title":"\ud83d\ude80 Google Vertex AI - Unified ML Platform","permalink":"/docs/docs/Build-ML-Tools/vertex-ai"}}');var i=r(4848),o=r(8453);const s={sidebar_position:8,title:"CUDA & TensorRT",description:"CUDA and TensorRT are NVIDIA's platforms for GPU computing and deep learning inference optimization, enabling high-performance AI applications.",slug:"/BuildMLTools/CUDA-TensorRT",keywords:["CUDA","TensorRT","GPU computing","deep learning inference","NVIDIA","performance optimization","AI acceleration","neural network optimization"]},a="\u26a1 GPU-Accelerated AI with CUDA & TensorRT",l={},c=[{value:"Key Features",id:"key-features",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"\ud83e\uddf0 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udd27 Step 1: Setup CUDA Development Environment",id:"-step-1-setup-cuda-development-environment",level:2},{value:"Install NVIDIA Container Toolkit",id:"install-nvidia-container-toolkit",level:3},{value:"\ud83c\udfd7\ufe0f Step 2: CUDA Programming Fundamentals",id:"\ufe0f-step-2-cuda-programming-fundamentals",level:2},{value:"Basic CUDA Kernel Example",id:"basic-cuda-kernel-example",level:3},{value:"Matrix Multiplication with CUDA",id:"matrix-multiplication-with-cuda",level:3},{value:"\u25b6\ufe0f Step 3: TensorRT Model Optimization",id:"\ufe0f-step-3-tensorrt-model-optimization",level:2},{value:"Convert PyTorch Model to TensorRT",id:"convert-pytorch-model-to-tensorrt",level:3},{value:"\ud83d\udcca Step 4: Production Deployment",id:"-step-4-production-deployment",level:2},{value:"TensorRT Inference Server",id:"tensorrt-inference-server",level:3},{value:"Kubernetes Deployment",id:"kubernetes-deployment",level:3},{value:"\ud83d\udd0d What You&#39;ll See",id:"-what-youll-see",level:2},{value:"CUDA Performance Results",id:"cuda-performance-results",level:3},{value:"TensorRT Optimization Results",id:"tensorrt-optimization-results",level:3},{value:"Production Inference Server",id:"production-inference-server",level:3},{value:"Pros &amp; Cons",id:"pros--cons",level:2},{value:"\u2705 Pros",id:"-pros",level:3},{value:"\u274c Cons",id:"-cons",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"-gpu-accelerated-ai-with-cuda--tensorrt",children:"\u26a1 GPU-Accelerated AI with CUDA & TensorRT"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"CUDA"})," and ",(0,i.jsx)(n.strong,{children:"TensorRT"})," are ",(0,i.jsx)(n.strong,{children:"NVIDIA's"})," platforms for ",(0,i.jsx)(n.strong,{children:"GPU computing"})," and ",(0,i.jsx)(n.strong,{children:"deep learning inference optimization"}),". Perfect for ",(0,i.jsx)(n.strong,{children:"high-performance AI applications"}),", ",(0,i.jsx)(n.strong,{children:"neural network acceleration"}),", and ",(0,i.jsx)(n.strong,{children:"production inference"})," with ",(0,i.jsx)(n.strong,{children:"maximum throughput"})," and ",(0,i.jsx)(n.strong,{children:"minimal latency"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CUDA"}),": Parallel computing platform for GPU acceleration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TensorRT"}),": High-performance deep learning inference optimizer"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Mixed Precision"}),": FP16 and INT8 quantization for faster inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic Shapes"}),": Support for variable input sizes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-GPU"}),": Scale across multiple GPUs seamlessly"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Inference"}),": Low-latency AI applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Edge Deployment"}),": Optimized models for edge devices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High-Throughput"}),": Batch processing for large-scale inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom Kernels"}),": Optimized GPU operations for specific tasks"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-prerequisites",children:"\ud83e\uddf0 Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA GPU"})," with compute capability 6.0+"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Driver"})," 450.80.02+ (Linux) or 452.39+ (Windows)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CUDA Toolkit"})," 11.0+"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Docker"})," with NVIDIA Container Toolkit"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Python 3.8+"})," for development"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-step-1-setup-cuda-development-environment",children:"\ud83d\udd27 Step 1: Setup CUDA Development Environment"}),"\n",(0,i.jsx)(n.h3,{id:"install-nvidia-container-toolkit",children:"Install NVIDIA Container Toolkit"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Add NVIDIA package repositories\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n# Install nvidia-container-toolkit\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n\n# Test GPU access\ndocker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi\n```### Doc\nker Compose for CUDA Development\n\nCreate a comprehensive development environment:\n\n```yaml\nversion: \'3.8\'\n\nservices:\n  cuda-dev:\n    image: nvidia/cuda:11.8-cudnn8-devel-ubuntu20.04\n    container_name: cuda-dev\n    restart: unless-stopped\n    ports:\n      - "8888:8888"  # Jupyter\n      - "6006:6006"  # TensorBoard\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - JUPYTER_TOKEN=cuda123\n    volumes:\n      - ./workspace:/workspace\n      - ./models:/models\n      - ./data:/data\n    working_dir: /workspace\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    command: >\n      bash -c "\n        apt-get update &&\n        apt-get install -y python3 python3-pip wget curl &&\n        pip3 install --upgrade pip &&\n        pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 &&\n        pip3 install tensorflow[and-cuda] &&\n        pip3 install jupyter jupyterlab tensorboard &&\n        pip3 install numpy pandas matplotlib seaborn &&\n        pip3 install pycuda &&\n        jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n      "\n\n  tensorrt-dev:\n    image: nvcr.io/nvidia/tensorrt:23.08-py3\n    container_name: tensorrt-dev\n    restart: unless-stopped\n    ports:\n      - "8889:8888"\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - JUPYTER_TOKEN=tensorrt123\n    volumes:\n      - ./workspace:/workspace\n      - ./models:/models\n      - ./data:/data\n    working_dir: /workspace\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    command: >\n      bash -c "\n        pip install jupyter jupyterlab &&\n        jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n      "\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-step-2-cuda-programming-fundamentals",children:"\ud83c\udfd7\ufe0f Step 2: CUDA Programming Fundamentals"}),"\n",(0,i.jsx)(n.h3,{id:"basic-cuda-kernel-example",children:"Basic CUDA Kernel Example"}),"\n",(0,i.jsx)(n.p,{children:"Create a simple CUDA kernel for vector addition:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# cuda_vector_add.py\nimport numpy as np\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda.compiler import SourceModule\nimport time\n\n# CUDA kernel code\ncuda_kernel = """\n__global__ void vector_add(float *a, float *b, float *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n"""\n\ndef cuda_vector_addition(n=1000000):\n    """Perform vector addition using CUDA"""\n    \n    # Compile CUDA kernel\n    mod = SourceModule(cuda_kernel)\n    vector_add = mod.get_function("vector_add")\n    \n    # Create input data\n    a = np.random.randn(n).astype(np.float32)\n    b = np.random.randn(n).astype(np.float32)\n    c = np.zeros_like(a)\n    \n    # Allocate GPU memory\n    a_gpu = drv.mem_alloc(a.nbytes)\n    b_gpu = drv.mem_alloc(b.nbytes)\n    c_gpu = drv.mem_alloc(c.nbytes)\n    \n    # Copy data to GPU\n    drv.memcpy_htod(a_gpu, a)\n    drv.memcpy_htod(b_gpu, b)\n    \n    # Configure kernel launch parameters\n    block_size = 256\n    grid_size = (n + block_size - 1) // block_size\n    \n    # Launch kernel\n    start_time = time.time()\n    vector_add(\n        a_gpu, b_gpu, c_gpu, np.int32(n),\n        block=(block_size, 1, 1),\n        grid=(grid_size, 1)\n    )\n    drv.Context.synchronize()\n    cuda_time = time.time() - start_time\n    \n    # Copy result back to CPU\n    drv.memcpy_dtoh(c, c_gpu)\n    \n    # Verify result\n    c_cpu = a + b\n    max_error = np.max(np.abs(c - c_cpu))\n    \n    print(f"CUDA Vector Addition Results:")\n    print(f"Vector size: {n:,}")\n    print(f"CUDA time: {cuda_time:.6f} seconds")\n    print(f"Max error: {max_error:.2e}")\n    print(f"Performance: {n / cuda_time / 1e6:.2f} M elements/second")\n    \n    return c, cuda_time\n\nif __name__ == "__main__":\n    # Test different vector sizes\n    for size in [100000, 1000000, 10000000]:\n        print(f"\\n{\'=\'*50}")\n        cuda_vector_addition(size)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"matrix-multiplication-with-cuda",children:"Matrix Multiplication with CUDA"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# cuda_matrix_multiply.py\nimport numpy as np\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda.compiler import SourceModule\nimport time\n\n# CUDA kernel for matrix multiplication\ncuda_matmul_kernel = """\n__global__ void matrix_multiply(float *A, float *B, float *C, int M, int N, int K) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; k++) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// Optimized version with shared memory\n__global__ void matrix_multiply_shared(float *A, float *B, float *C, int M, int N, int K) {\n    __shared__ float As[16][16];\n    __shared__ float Bs[16][16];\n    \n    int bx = blockIdx.x, by = blockIdx.y;\n    int tx = threadIdx.x, ty = threadIdx.y;\n    \n    int row = by * 16 + ty;\n    int col = bx * 16 + tx;\n    \n    float sum = 0.0f;\n    \n    for (int m = 0; m < (K + 15) / 16; m++) {\n        // Load data into shared memory\n        if (row < M && m * 16 + tx < K)\n            As[ty][tx] = A[row * K + m * 16 + tx];\n        else\n            As[ty][tx] = 0.0f;\n            \n        if (col < N && m * 16 + ty < K)\n            Bs[ty][tx] = B[(m * 16 + ty) * N + col];\n        else\n            Bs[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int k = 0; k < 16; k++) {\n            sum += As[ty][k] * Bs[k][tx];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n"""\n\ndef cuda_matrix_multiplication(M=1024, N=1024, K=1024, use_shared=True):\n    """Perform matrix multiplication using CUDA"""\n    \n    # Compile CUDA kernel\n    mod = SourceModule(cuda_matmul_kernel)\n    if use_shared:\n        matmul = mod.get_function("matrix_multiply_shared")\n        kernel_name = "Shared Memory"\n    else:\n        matmul = mod.get_function("matrix_multiply")\n        kernel_name = "Global Memory"\n    \n    # Create input matrices\n    A = np.random.randn(M, K).astype(np.float32)\n    B = np.random.randn(K, N).astype(np.float32)\n    C = np.zeros((M, N), dtype=np.float32)\n    \n    # Allocate GPU memory\n    A_gpu = drv.mem_alloc(A.nbytes)\n    B_gpu = drv.mem_alloc(B.nbytes)\n    C_gpu = drv.mem_alloc(C.nbytes)\n    \n    # Copy data to GPU\n    drv.memcpy_htod(A_gpu, A)\n    drv.memcpy_htod(B_gpu, B)\n    \n    # Configure kernel launch parameters\n    block_size = (16, 16, 1)\n    grid_size = ((N + 15) // 16, (M + 15) // 16, 1)\n    \n    # Launch kernel\n    start_time = time.time()\n    matmul(\n        A_gpu, B_gpu, C_gpu,\n        np.int32(M), np.int32(N), np.int32(K),\n        block=block_size,\n        grid=grid_size\n    )\n    drv.Context.synchronize()\n    cuda_time = time.time() - start_time\n    \n    # Copy result back to CPU\n    drv.memcpy_dtoh(C, C_gpu)\n    \n    # Verify result with NumPy\n    start_time = time.time()\n    C_cpu = np.dot(A, B)\n    cpu_time = time.time() - start_time\n    \n    # Calculate metrics\n    max_error = np.max(np.abs(C - C_cpu))\n    gflops = (2.0 * M * N * K) / (cuda_time * 1e9)\n    speedup = cpu_time / cuda_time\n    \n    print(f"CUDA Matrix Multiplication Results ({kernel_name}):")\n    print(f"Matrix size: {M}x{K} * {K}x{N}")\n    print(f"CUDA time: {cuda_time:.6f} seconds")\n    print(f"CPU time: {cpu_time:.6f} seconds")\n    print(f"Speedup: {speedup:.2f}x")\n    print(f"Performance: {gflops:.2f} GFLOPS")\n    print(f"Max error: {max_error:.2e}")\n    \n    return C, cuda_time, gflops\n\nif __name__ == "__main__":\n    # Test different approaches\n    print("Testing Global Memory Kernel:")\n    cuda_matrix_multiplication(1024, 1024, 1024, use_shared=False)\n    \n    print(f"\\n{\'=\'*50}")\n    print("Testing Shared Memory Kernel:")\n    cuda_matrix_multiplication(1024, 1024, 1024, use_shared=True)\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"\ufe0f-step-3-tensorrt-model-optimization",children:"\u25b6\ufe0f Step 3: TensorRT Model Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"convert-pytorch-model-to-tensorrt",children:"Convert PyTorch Model to TensorRT"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# tensorrt_optimization.py\nimport torch\nimport torch.nn as nn\nimport tensorrt as trt\nimport numpy as np\nimport time\nfrom torch2trt import torch2trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\nclass SimpleConvNet(nn.Module):\n    """Simple CNN for demonstration"""\n    def __init__(self, num_classes=10):\n        super(SimpleConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        self.classifier = nn.Linear(256, num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\ndef convert_pytorch_to_tensorrt():\n    """Convert PyTorch model to TensorRT"""\n    \n    # Create and load model\n    model = SimpleConvNet(num_classes=10)\n    model.eval()\n    model.cuda()\n    \n    # Create example input\n    x = torch.randn(1, 3, 224, 224).cuda()\n    \n    # Convert to TensorRT\n    print("Converting PyTorch model to TensorRT...")\n    model_trt = torch2trt(model, [x], fp16_mode=True, max_workspace_size=1<<25)\n    \n    # Benchmark original PyTorch model\n    print("Benchmarking PyTorch model...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for _ in range(1000):\n        with torch.no_grad():\n            y_pytorch = model(x)\n    \n    torch.cuda.synchronize()\n    pytorch_time = time.time() - start_time\n    \n    # Benchmark TensorRT model\n    print("Benchmarking TensorRT model...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for _ in range(1000):\n        y_tensorrt = model_trt(x)\n    \n    torch.cuda.synchronize()\n    tensorrt_time = time.time() - start_time\n    \n    # Compare results\n    max_error = torch.max(torch.abs(y_pytorch - y_tensorrt)).item()\n    speedup = pytorch_time / tensorrt_time\n    \n    print(f"\\nBenchmark Results:")\n    print(f"PyTorch time: {pytorch_time:.4f} seconds (1000 inferences)")\n    print(f"TensorRT time: {tensorrt_time:.4f} seconds (1000 inferences)")\n    print(f"Speedup: {speedup:.2f}x")\n    print(f"Max error: {max_error:.2e}")\n    print(f"PyTorch FPS: {1000/pytorch_time:.1f}")\n    print(f"TensorRT FPS: {1000/tensorrt_time:.1f}")\n    \n    # Save TensorRT model\n    torch.save(model_trt.state_dict(), \'model_trt.pth\')\n    print("TensorRT model saved as \'model_trt.pth\'")\n    \n    return model_trt, speedup\n\ndef advanced_tensorrt_optimization():\n    """Advanced TensorRT optimization with custom calibration"""\n    \n    # TensorRT Logger\n    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n    \n    def build_engine_from_onnx(onnx_file_path, engine_file_path, precision=\'fp16\'):\n        """Build TensorRT engine from ONNX model"""\n        \n        builder = trt.Builder(TRT_LOGGER)\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, TRT_LOGGER)\n        \n        # Parse ONNX model\n        with open(onnx_file_path, \'rb\') as model:\n            if not parser.parse(model.read()):\n                print(\'ERROR: Failed to parse the ONNX file.\')\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return None\n        \n        # Build engine\n        config = builder.create_builder_config()\n        config.max_workspace_size = 1 << 30  # 1GB\n        \n        if precision == \'fp16\':\n            config.set_flag(trt.BuilderFlag.FP16)\n        elif precision == \'int8\':\n            config.set_flag(trt.BuilderFlag.INT8)\n            # Add calibration dataset for INT8\n            config.int8_calibrator = create_calibration_dataset()\n        \n        # Enable optimization profiles for dynamic shapes\n        profile = builder.create_optimization_profile()\n        profile.set_shape("input", (1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224))\n        config.add_optimization_profile(profile)\n        \n        engine = builder.build_engine(network, config)\n        \n        # Save engine\n        with open(engine_file_path, "wb") as f:\n            f.write(engine.serialize())\n        \n        return engine\n    \n    def create_calibration_dataset():\n        """Create calibration dataset for INT8 quantization"""\n        class CalibrationDataset:\n            def __init__(self, batch_size=1):\n                self.batch_size = batch_size\n                self.current_index = 0\n                self.calibration_data = [\n                    np.random.randn(batch_size, 3, 224, 224).astype(np.float32)\n                    for _ in range(100)  # 100 calibration samples\n                ]\n            \n            def get_batch_size(self):\n                return self.batch_size\n            \n            def get_batch(self, names):\n                if self.current_index < len(self.calibration_data):\n                    batch = self.calibration_data[self.current_index]\n                    self.current_index += 1\n                    return [batch]\n                else:\n                    return None\n        \n        return CalibrationDataset()\n    \n    print("Advanced TensorRT optimization example setup complete")\n    return build_engine_from_onnx\n\nif __name__ == "__main__":\n    # Run TensorRT optimization\n    model_trt, speedup = convert_pytorch_to_tensorrt()\n    print(f"\\nTensorRT optimization achieved {speedup:.2f}x speedup!")\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-step-4-production-deployment",children:"\ud83d\udcca Step 4: Production Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"tensorrt-inference-server",children:"TensorRT Inference Server"}),"\n",(0,i.jsx)(n.p,{children:"Create a production-ready inference server:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# tensorrt_inference_server.py\nimport torch\nimport numpy as np\nimport time\nfrom flask import Flask, request, jsonify\nimport io\nimport base64\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport threading\nimport queue\nimport logging\n\nclass TensorRTInferenceServer:\n    \"\"\"Production TensorRT inference server\"\"\"\n    \n    def __init__(self, model_path, batch_size=8, max_queue_size=100):\n        self.model_path = model_path\n        self.batch_size = batch_size\n        self.max_queue_size = max_queue_size\n        \n        # Load TensorRT model\n        self.model = self.load_model()\n        \n        # Setup request queue and batch processing\n        self.request_queue = queue.Queue(maxsize=max_queue_size)\n        self.response_dict = {}\n        self.batch_thread = threading.Thread(target=self._batch_processor, daemon=True)\n        self.batch_thread.start()\n        \n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        \n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n    \n    def load_model(self):\n        \"\"\"Load TensorRT optimized model\"\"\"\n        try:\n            # Load your TensorRT model here\n            # This is a placeholder - replace with actual TensorRT model loading\n            model = torch.jit.load(self.model_path)\n            model.eval()\n            model.cuda()\n            return model\n        except Exception as e:\n            self.logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def preprocess_image(self, image_data):\n        \"\"\"Preprocess image for inference\"\"\"\n        try:\n            # Decode base64 image\n            image_bytes = base64.b64decode(image_data)\n            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n            \n            # Apply transformations\n            tensor = self.transform(image).unsqueeze(0)\n            return tensor\n        except Exception as e:\n            self.logger.error(f\"Image preprocessing failed: {e}\")\n            raise\n    \n    def _batch_processor(self):\n        \"\"\"Background thread for batch processing\"\"\"\n        while True:\n            batch_requests = []\n            batch_tensors = []\n            \n            # Collect requests for batching\n            try:\n                # Wait for at least one request\n                request_id, tensor = self.request_queue.get(timeout=1.0)\n                batch_requests.append(request_id)\n                batch_tensors.append(tensor)\n                \n                # Collect additional requests up to batch size\n                while len(batch_requests) < self.batch_size:\n                    try:\n                        request_id, tensor = self.request_queue.get(timeout=0.01)\n                        batch_requests.append(request_id)\n                        batch_tensors.append(tensor)\n                    except queue.Empty:\n                        break\n                \n                # Process batch\n                if batch_tensors:\n                    self._process_batch(batch_requests, batch_tensors)\n                    \n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.logger.error(f\"Batch processing error: {e}\")\n    \n    def _process_batch(self, request_ids, tensors):\n        \"\"\"Process a batch of requests\"\"\"\n        try:\n            # Stack tensors into batch\n            batch_tensor = torch.cat(tensors, dim=0).cuda()\n            \n            # Run inference\n            start_time = time.time()\n            with torch.no_grad():\n                outputs = self.model(batch_tensor)\n                predictions = torch.softmax(outputs, dim=1)\n            \n            inference_time = time.time() - start_time\n            \n            # Store results\n            for i, request_id in enumerate(request_ids):\n                result = {\n                    'prediction': predictions[i].cpu().numpy().tolist(),\n                    'inference_time': inference_time / len(request_ids),\n                    'batch_size': len(request_ids)\n                }\n                self.response_dict[request_id] = result\n            \n            self.logger.info(f\"Processed batch of {len(request_ids)} requests in {inference_time:.4f}s\")\n            \n        except Exception as e:\n            self.logger.error(f\"Batch inference failed: {e}\")\n            # Store error for all requests in batch\n            for request_id in request_ids:\n                self.response_dict[request_id] = {'error': str(e)}\n    \n    def predict(self, image_data, timeout=5.0):\n        \"\"\"Make prediction on image\"\"\"\n        request_id = f\"{time.time()}_{threading.current_thread().ident}\"\n        \n        try:\n            # Preprocess image\n            tensor = self.preprocess_image(image_data)\n            \n            # Add to queue\n            self.request_queue.put((request_id, tensor), timeout=1.0)\n            \n            # Wait for result\n            start_time = time.time()\n            while time.time() - start_time < timeout:\n                if request_id in self.response_dict:\n                    result = self.response_dict.pop(request_id)\n                    return result\n                time.sleep(0.001)\n            \n            return {'error': 'Request timeout'}\n            \n        except queue.Full:\n            return {'error': 'Server overloaded'}\n        except Exception as e:\n            return {'error': str(e)}\n\n# Flask application\napp = Flask(__name__)\ninference_server = None\n\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({'status': 'healthy', 'model_loaded': inference_server is not None})\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        data = request.json\n        if 'image' not in data:\n            return jsonify({'error': 'No image provided'}), 400\n        \n        result = inference_server.predict(data['image'])\n        return jsonify(result)\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/metrics', methods=['GET'])\ndef metrics():\n    return jsonify({\n        'queue_size': inference_server.request_queue.qsize(),\n        'max_queue_size': inference_server.max_queue_size,\n        'batch_size': inference_server.batch_size\n    })\n\nif __name__ == '__main__':\n    # Initialize inference server\n    model_path = 'model_trt.pth'  # Path to your TensorRT model\n    inference_server = TensorRTInferenceServer(model_path, batch_size=8)\n    \n    # Start Flask server\n    app.run(host='0.0.0.0', port=8080, threaded=True)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"kubernetes-deployment",children:"Kubernetes Deployment"}),"\n",(0,i.jsx)(n.p,{children:"Create Kubernetes manifests for production deployment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# k8s-tensorrt-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tensorrt-inference\n  labels:\n    app: tensorrt-inference\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tensorrt-inference\n  template:\n    metadata:\n      labels:\n        app: tensorrt-inference\n    spec:\n      containers:\n      - name: tensorrt-inference\n        image: myregistry/tensorrt-inference:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            nvidia.com/gpu: 1\n            cpu: 2\n            memory: 4Gi\n          limits:\n            nvidia.com/gpu: 1\n            cpu: 4\n            memory: 8Gi\n        env:\n        - name: MODEL_PATH\n          value: "/models/model_trt.pth"\n        - name: BATCH_SIZE\n          value: "8"\n        volumeMounts:\n        - name: model-storage\n          mountPath: /models\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: model-storage\n        persistentVolumeClaim:\n          claimName: model-pvc\n      nodeSelector:\n        accelerator: nvidia-tesla-v100\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: tensorrt-inference-service\nspec:\n  selector:\n    app: tensorrt-inference\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  type: LoadBalancer\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: tensorrt-inference-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: tensorrt-inference\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"-what-youll-see",children:"\ud83d\udd0d What You'll See"}),"\n",(0,i.jsx)(n.h3,{id:"cuda-performance-results",children:"CUDA Performance Results"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"CUDA Vector Addition Results:\nVector size: 10,000,000\nCUDA time: 0.003245 seconds\nMax error: 0.00e+00\nPerformance: 3081.23 M elements/second\n\nCUDA Matrix Multiplication Results (Shared Memory):\nMatrix size: 1024x1024 * 1024x1024\nCUDA time: 0.045123 seconds\nCPU time: 2.341567 seconds\nSpeedup: 51.89x\nPerformance: 47.23 GFLOPS\nMax error: 1.23e-05\n"})}),"\n",(0,i.jsx)(n.h3,{id:"tensorrt-optimization-results",children:"TensorRT Optimization Results"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"Converting PyTorch model to TensorRT...\nBenchmarking PyTorch model...\nBenchmarking TensorRT model...\n\nBenchmark Results:\nPyTorch time: 2.3456 seconds (1000 inferences)\nTensorRT time: 0.4567 seconds (1000 inferences)\nSpeedup: 5.14x\nMax error: 1.23e-06\nPyTorch FPS: 426.3\nTensorRT FPS: 2190.1\n\nTensorRT optimization achieved 5.14x speedup!\n"})}),"\n",(0,i.jsx)(n.h3,{id:"production-inference-server",children:"Production Inference Server"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"INFO:__main__:Processed batch of 8 requests in 0.0234s\nINFO:__main__:Processed batch of 6 requests in 0.0187s\nINFO:__main__:Processed batch of 8 requests in 0.0241s\n\nServer Metrics:\n- Queue size: 12/100\n- Batch size: 8\n- Average latency: 23.4ms\n- Throughput: 342 requests/second\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"pros--cons",children:"Pros & Cons"}),"\n",(0,i.jsx)(n.h3,{id:"-pros",children:"\u2705 Pros"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High Performance"}),": Massive speedups for parallel computations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Production Ready"}),": TensorRT provides optimized inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexible"}),": Support for custom kernels and operations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalable"}),": Multi-GPU support for large-scale deployments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Industry Standard"}),": Widely adopted in AI/ML production"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"-cons",children:"\u274c Cons"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware Dependency"}),": Requires NVIDIA GPUs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning Curve"}),": CUDA programming requires specialized knowledge"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Management"}),": Manual memory management can be error-prone"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Debugging"}),": GPU debugging is more complex than CPU"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(n.p,{children:["CUDA and TensorRT are ",(0,i.jsx)(n.strong,{children:"essential tools"})," for ",(0,i.jsx)(n.strong,{children:"high-performance AI applications"}),". Choose CUDA/TensorRT when you need:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Maximum performance"})," for deep learning inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time AI"})," applications with low latency requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High-throughput"})," batch processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom GPU operations"})," for specialized workloads"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The combination of CUDA's parallel computing power and TensorRT's optimization capabilities makes them ideal for production AI systems requiring maximum performance."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"What You've Achieved:"}),"\n\u2705 Set up comprehensive CUDA development environment",(0,i.jsx)(n.br,{}),"\n","\u2705 Implemented high-performance CUDA kernels",(0,i.jsx)(n.br,{}),"\n","\u2705 Optimized models with TensorRT for production inference",(0,i.jsx)(n.br,{}),"\n","\u2705 Built scalable inference servers with batching",(0,i.jsx)(n.br,{}),"\n","\u2705 Deployed GPU-accelerated applications on Kubernetes",(0,i.jsx)(n.br,{}),"\n","\u2705 Achieved significant performance improvements for AI workloads"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);