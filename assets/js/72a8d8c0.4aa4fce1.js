"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5630],{8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>s});var t=a(6540);const r={},o=t.createContext(r);function i(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(o.Provider,{value:n},e.children)}},8539:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"CI-CD/CI/argo-workflows","title":"Argo Workflows","description":"Argo Workflows is a container-native workflow engine for Kubernetes. Learn how to create complex CI/CD pipelines and data processing workflows with Argo.","source":"@site/docs/CI-CD/CI/argo-workflows.md","sourceDirName":"CI-CD/CI","slug":"/CICD/ArgoWorkflows","permalink":"/docs/CICD/ArgoWorkflows","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/CI-CD/CI/argo-workflows.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Argo Workflows","description":"Argo Workflows is a container-native workflow engine for Kubernetes. Learn how to create complex CI/CD pipelines and data processing workflows with Argo.","slug":"/CICD/ArgoWorkflows","keywords":["Argo Workflows","Kubernetes workflows","container-native","CI/CD pipelines","workflow orchestration","DAG workflows","parallel processing","Kubernetes native","workflow automation","GitOps workflows"]},"sidebar":"tutorialSidebar","previous":{"title":"CodeArts (Huawei Cloud)","permalink":"/docs/CICD/CodeArts"},"next":{"title":"Bitbucket Pipelines","permalink":"/docs/CICD/BitbucketPipelines"}}');var r=a(4848),o=a(8453);const i={sidebar_position:12,title:"Argo Workflows",description:"Argo Workflows is a container-native workflow engine for Kubernetes. Learn how to create complex CI/CD pipelines and data processing workflows with Argo.",slug:"/CICD/ArgoWorkflows",keywords:["Argo Workflows","Kubernetes workflows","container-native","CI/CD pipelines","workflow orchestration","DAG workflows","parallel processing","Kubernetes native","workflow automation","GitOps workflows"]},s="\ud83d\ude80 Container-Native Workflows with Argo Workflows on Kubernetes",l={},c=[{value:"\ud83e\uddf0 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udd27 Step 1: Install Argo Workflows on Kubernetes",id:"-step-1-install-argo-workflows-on-kubernetes",level:2},{value:"Install using Helm",id:"install-using-helm",level:3},{value:"Install using kubectl (Alternative)",id:"install-using-kubectl-alternative",level:3},{value:"Verify Installation",id:"verify-installation",level:3},{value:"\ud83c\udfd7\ufe0f Step 2: Install Argo CLI",id:"\ufe0f-step-2-install-argo-cli",level:2},{value:"Install Argo CLI",id:"install-argo-cli",level:3},{value:"Configure CLI",id:"configure-cli",level:3},{value:"\ud83d\udcc1 Step 3: Create Your First Workflow",id:"-step-3-create-your-first-workflow",level:2},{value:"Simple Hello World Workflow",id:"simple-hello-world-workflow",level:3},{value:"Submit and Monitor Workflow",id:"submit-and-monitor-workflow",level:3},{value:"\u25b6\ufe0f Step 4: CI/CD Pipeline Workflow",id:"\ufe0f-step-4-cicd-pipeline-workflow",level:2},{value:"Complete CI/CD Pipeline",id:"complete-cicd-pipeline",level:3},{value:"Submit CI/CD Pipeline",id:"submit-cicd-pipeline",level:3},{value:"\ud83d\udcca Step 5: Data Processing Workflow",id:"-step-5-data-processing-workflow",level:2},{value:"Parallel Data Processing Pipeline",id:"parallel-data-processing-pipeline",level:3},{value:"\ud83d\udd0d Step 6: Machine Learning Workflow",id:"-step-6-machine-learning-workflow",level:2},{value:"ML Training Pipeline",id:"ml-training-pipeline",level:3},{value:"\ud83d\udcc8 Step 7: Advanced Workflow Patterns",id:"-step-7-advanced-workflow-patterns",level:2},{value:"Conditional Workflows",id:"conditional-workflows",level:3},{value:"Loop Workflows",id:"loop-workflows",level:3},{value:"Retry and Error Handling",id:"retry-and-error-handling",level:3},{value:"\ud83d\udee1\ufe0f Step 8: Security and RBAC",id:"\ufe0f-step-8-security-and-rbac",level:2},{value:"Create Service Account and RBAC",id:"create-service-account-and-rbac",level:3},{value:"Secure Workflow Template",id:"secure-workflow-template",level:3},{value:"\ud83d\udccb Common Use Cases",id:"-common-use-cases",level:2},{value:"1. <strong>CI/CD Pipelines</strong>",id:"1-cicd-pipelines",level:3},{value:"2. <strong>Data Processing</strong>",id:"2-data-processing",level:3},{value:"3. <strong>Machine Learning</strong>",id:"3-machine-learning",level:3},{value:"4. <strong>Infrastructure Automation</strong>",id:"4-infrastructure-automation",level:3},{value:"\u2705 What You&#39;ll Achieve",id:"-what-youll-achieve",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"-container-native-workflows-with-argo-workflows-on-kubernetes",children:"\ud83d\ude80 Container-Native Workflows with Argo Workflows on Kubernetes"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Argo Workflows"})," is a ",(0,r.jsx)(n.strong,{children:"container-native"})," workflow engine for orchestrating parallel jobs on ",(0,r.jsx)(n.strong,{children:"Kubernetes"}),". Perfect for ",(0,r.jsx)(n.strong,{children:"CI/CD pipelines"}),", ",(0,r.jsx)(n.strong,{children:"data processing"}),", ",(0,r.jsx)(n.strong,{children:"machine learning"}),", and ",(0,r.jsx)(n.strong,{children:"batch job"})," orchestration with powerful ",(0,r.jsx)(n.strong,{children:"DAG"})," (Directed Acyclic Graph) capabilities."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-prerequisites",children:"\ud83e\uddf0 Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Make sure you have the following:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kubernetes cluster"})," (v1.16+) with kubectl access"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Helm 3.0+"})," for installation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Docker"})," for building container images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Basic understanding"})," of Kubernetes concepts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Git repository"})," for storing workflow definitions"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-step-1-install-argo-workflows-on-kubernetes",children:"\ud83d\udd27 Step 1: Install Argo Workflows on Kubernetes"}),"\n",(0,r.jsx)(n.h3,{id:"install-using-helm",children:"Install using Helm"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Add Argo Helm repository\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\n\n# Create namespace\nkubectl create namespace argo\n\n# Install Argo Workflows\nhelm install argo-workflows argo/argo-workflows \\\n  --namespace argo \\\n  --set server.extraArgs[0]="--auth-mode=server" \\\n  --set server.extraArgs[1]="--secure=false" \\\n  --set controller.workflowNamespaces[0]="argo" \\\n  --set controller.workflowNamespaces[1]="default"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"install-using-kubectl-alternative",children:"Install using kubectl (Alternative)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Install Argo Workflows\nkubectl create namespace argo\nkubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.5.2/install.yaml\n\n# Patch server authentication\nkubectl patch deployment \\\n  argo-server \\\n  --namespace argo \\\n  --type=\'json\' \\\n  -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": [\n  "server",\n  "--auth-mode=server",\n  "--secure=false"\n]}]\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"verify-installation",children:"Verify Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check pods\nkubectl get pods -n argo\n\n# Port forward to access UI\nkubectl -n argo port-forward deployment/argo-server 2746:2746\n\n# Access UI at http://localhost:2746\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ufe0f-step-2-install-argo-cli",children:"\ud83c\udfd7\ufe0f Step 2: Install Argo CLI"}),"\n",(0,r.jsx)(n.h3,{id:"install-argo-cli",children:"Install Argo CLI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Linux/macOS\ncurl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.5.2/argo-linux-amd64.gz\ngunzip argo-linux-amd64.gz\nchmod +x argo-linux-amd64\nsudo mv ./argo-linux-amd64 /usr/local/bin/argo\n\n# macOS with Homebrew\nbrew install argo\n\n# Verify installation\nargo version\n"})}),"\n",(0,r.jsx)(n.h3,{id:"configure-cli",children:"Configure CLI"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Set default namespace\nexport ARGO_NAMESPACE=argo\n\n# Configure server endpoint\nexport ARGO_SERVER=localhost:2746\n\n# Test CLI connection\nargo list\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-step-3-create-your-first-workflow",children:"\ud83d\udcc1 Step 3: Create Your First Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"simple-hello-world-workflow",children:"Simple Hello World Workflow"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create hello-world.yaml:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-\n  namespace: argo\nspec:\n  entrypoint: hello-world\n  templates:\n  - name: hello-world\n    container:\n      image: alpine:3.18\n      command: [sh, -c]\n      args: [\"echo 'Hello World from Argo Workflows!'\"]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"submit-and-monitor-workflow",children:"Submit and Monitor Workflow"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Submit workflow\nargo submit hello-world.yaml\n\n# List workflows\nargo list\n\n# Get workflow details\nargo get @latest\n\n# Watch workflow logs\nargo logs @latest -f\n\n# Delete workflow\nargo delete @latest\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ufe0f-step-4-cicd-pipeline-workflow",children:"\u25b6\ufe0f Step 4: CI/CD Pipeline Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"complete-cicd-pipeline",children:"Complete CI/CD Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create ci-cd-pipeline.yaml:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: ci-cd-pipeline-\n  namespace: argo\nspec:\n  entrypoint: ci-cd-pipeline\n  arguments:\n    parameters:\n    - name: repo-url\n      value: "https://github.com/your-org/your-app.git"\n    - name: branch\n      value: "main"\n    - name: image-tag\n      value: "latest"\n    \n  volumeClaimTemplates:\n  - metadata:\n      name: workspace\n    spec:\n      accessModes: [ "ReadWriteOnce" ]\n      resources:\n        requests:\n          storage: 1Gi\n          \n  templates:\n  # Main pipeline template\n  - name: ci-cd-pipeline\n    dag:\n      tasks:\n      - name: clone-repo\n        template: git-clone\n        arguments:\n          parameters:\n          - name: repo-url\n            value: "{{workflow.parameters.repo-url}}"\n          - name: branch\n            value: "{{workflow.parameters.branch}}"\n            \n      - name: code-quality\n        template: code-quality-check\n        dependencies: [clone-repo]\n        \n      - name: unit-tests\n        template: run-unit-tests\n        dependencies: [clone-repo]\n        \n      - name: security-scan\n        template: security-scan\n        dependencies: [clone-repo]\n        \n      - name: build-image\n        template: build-docker-image\n        dependencies: [code-quality, unit-tests, security-scan]\n        arguments:\n          parameters:\n          - name: image-tag\n            value: "{{workflow.parameters.image-tag}}"\n            \n      - name: integration-tests\n        template: integration-tests\n        dependencies: [build-image]\n        \n      - name: deploy-staging\n        template: deploy-to-staging\n        dependencies: [integration-tests]\n        \n      - name: e2e-tests\n        template: e2e-tests\n        dependencies: [deploy-staging]\n        \n      - name: deploy-production\n        template: deploy-to-production\n        dependencies: [e2e-tests]\n        when: "{{workflow.parameters.branch}} == \'main\'"\n\n  # Git clone template\n  - name: git-clone\n    inputs:\n      parameters:\n      - name: repo-url\n      - name: branch\n    container:\n      image: alpine/git:2.40.1\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        git clone --branch {{inputs.parameters.branch}} --single-branch --depth 1 {{inputs.parameters.repo-url}} .\n        ls -la\n        echo "Repository cloned successfully"\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Code quality check template\n  - name: code-quality-check\n    container:\n      image: sonarsource/sonar-scanner-cli:5.0\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        echo "Running code quality analysis..."\n        # Add your SonarQube configuration\n        sonar-scanner \\\n          -Dsonar.projectKey=my-project \\\n          -Dsonar.sources=. \\\n          -Dsonar.host.url=$SONAR_HOST_URL \\\n          -Dsonar.login=$SONAR_TOKEN || echo "SonarQube analysis completed"\n        echo "Code quality check completed"\n      env:\n      - name: SONAR_HOST_URL\n        value: "http://sonarqube:9000"\n      - name: SONAR_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: sonar-secret\n            key: token\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Unit tests template\n  - name: run-unit-tests\n    container:\n      image: node:18-alpine\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        echo "Installing dependencies..."\n        npm ci\n        echo "Running unit tests..."\n        npm run test:unit\n        echo "Generating coverage report..."\n        npm run test:coverage\n        echo "Unit tests completed successfully"\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Security scan template\n  - name: security-scan\n    container:\n      image: securecodewarrior/docker-security-scanner:latest\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        echo "Running security scan..."\n        # Dependency vulnerability scan\n        npm audit --audit-level high\n        # SAST scan\n        semgrep --config=auto --json --output=security-report.json .\n        echo "Security scan completed"\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Docker build template\n  - name: build-docker-image\n    inputs:\n      parameters:\n      - name: image-tag\n    container:\n      image: gcr.io/kaniko-project/executor:v1.9.0\n      workingDir: /workspace\n      args:\n      - --dockerfile=/workspace/Dockerfile\n      - --context=/workspace\n      - --destination=your-registry/your-app:{{inputs.parameters.image-tag}}\n      - --destination=your-registry/your-app:{{workflow.creationTimestamp}}\n      - --cache=true\n      - --cache-ttl=24h\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n      - name: docker-config\n        mountPath: /kaniko/.docker/\n        readOnly: true\n\n  # Integration tests template\n  - name: integration-tests\n    container:\n      image: node:18-alpine\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        echo "Running integration tests..."\n        npm run test:integration\n        echo "Integration tests completed successfully"\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Deploy to staging template\n  - name: deploy-to-staging\n    container:\n      image: bitnami/kubectl:1.28\n      command: [sh, -c]\n      args:\n      - |\n        echo "Deploying to staging environment..."\n        kubectl config use-context staging\n        kubectl set image deployment/myapp myapp=your-registry/your-app:{{workflow.parameters.image-tag}}\n        kubectl rollout status deployment/myapp\n        echo "Deployment to staging completed"\n\n  # E2E tests template\n  - name: e2e-tests\n    container:\n      image: cypress/included:13.3.0\n      workingDir: /workspace\n      command: [sh, -c]\n      args:\n      - |\n        echo "Running end-to-end tests..."\n        cypress run --config baseUrl=https://staging.yourapp.com\n        echo "E2E tests completed successfully"\n      volumeMounts:\n      - name: workspace\n        mountPath: /workspace\n\n  # Deploy to production template\n  - name: deploy-to-production\n    container:\n      image: bitnami/kubectl:1.28\n      command: [sh, -c]\n      args:\n      - |\n        echo "Deploying to production environment..."\n        kubectl config use-context production\n        kubectl set image deployment/myapp myapp=your-registry/your-app:{{workflow.parameters.image-tag}}\n        kubectl rollout status deployment/myapp\n        echo "Deployment to production completed"\n\n  volumes:\n  - name: docker-config\n    secret:\n      secretName: docker-registry-secret\n'})}),"\n",(0,r.jsx)(n.h3,{id:"submit-cicd-pipeline",children:"Submit CI/CD Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Submit with parameters\nargo submit ci-cd-pipeline.yaml \\\n  -p repo-url="https://github.com/your-org/your-app.git" \\\n  -p branch="main" \\\n  -p image-tag="v1.0.0"\n\n# Monitor pipeline\nargo get @latest\nargo logs @latest -f\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-step-5-data-processing-workflow",children:"\ud83d\udcca Step 5: Data Processing Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"parallel-data-processing-pipeline",children:"Parallel Data Processing Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create data-processing.yaml:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: data-processing-\n  namespace: argo\nspec:\n  entrypoint: data-pipeline\n  arguments:\n    parameters:\n    - name: input-data\n      value: "s3://my-bucket/input-data/"\n    - name: output-data\n      value: "s3://my-bucket/processed-data/"\n    \n  templates:\n  # Main data pipeline\n  - name: data-pipeline\n    dag:\n      tasks:\n      - name: validate-input\n        template: validate-data\n        arguments:\n          parameters:\n          - name: data-path\n            value: "{{workflow.parameters.input-data}}"\n            \n      - name: extract-data\n        template: extract-transform\n        dependencies: [validate-input]\n        arguments:\n          parameters:\n          - name: input-path\n            value: "{{workflow.parameters.input-data}}"\n            \n      - name: process-batch-1\n        template: process-data-batch\n        dependencies: [extract-data]\n        arguments:\n          parameters:\n          - name: batch-id\n            value: "batch-1"\n          - name: data-subset\n            value: "{{tasks.extract-data.outputs.parameters.batch-1-path}}"\n            \n      - name: process-batch-2\n        template: process-data-batch\n        dependencies: [extract-data]\n        arguments:\n          parameters:\n          - name: batch-id\n            value: "batch-2"\n          - name: data-subset\n            value: "{{tasks.extract-data.outputs.parameters.batch-2-path}}"\n            \n      - name: process-batch-3\n        template: process-data-batch\n        dependencies: [extract-data]\n        arguments:\n          parameters:\n          - name: batch-id\n            value: "batch-3"\n          - name: data-subset\n            value: "{{tasks.extract-data.outputs.parameters.batch-3-path}}"\n            \n      - name: aggregate-results\n        template: aggregate-data\n        dependencies: [process-batch-1, process-batch-2, process-batch-3]\n        arguments:\n          parameters:\n          - name: output-path\n            value: "{{workflow.parameters.output-data}}"\n            \n      - name: generate-report\n        template: generate-report\n        dependencies: [aggregate-results]\n\n  # Data validation template\n  - name: validate-data\n    inputs:\n      parameters:\n      - name: data-path\n    container:\n      image: python:3.9-slim\n      command: [python, -c]\n      args:\n      - |\n        import boto3\n        import sys\n        \n        print("Validating input data...")\n        s3 = boto3.client(\'s3\')\n        \n        # Extract bucket and key from S3 path\n        path = "{{inputs.parameters.data-path}}"\n        bucket = path.split(\'/\')[2]\n        key = \'/\'.join(path.split(\'/\')[3:])\n        \n        try:\n            response = s3.list_objects_v2(Bucket=bucket, Prefix=key)\n            if \'Contents\' in response:\n                print(f"Found {len(response[\'Contents\'])} files")\n                print("Data validation successful")\n            else:\n                print("No files found")\n                sys.exit(1)\n        except Exception as e:\n            print(f"Validation failed: {e}")\n            sys.exit(1)\n\n  # Extract and transform template\n  - name: extract-transform\n    inputs:\n      parameters:\n      - name: input-path\n    outputs:\n      parameters:\n      - name: batch-1-path\n        valueFrom:\n          path: /tmp/batch-1-path.txt\n      - name: batch-2-path\n        valueFrom:\n          path: /tmp/batch-2-path.txt\n      - name: batch-3-path\n        valueFrom:\n          path: /tmp/batch-3-path.txt\n    container:\n      image: python:3.9-slim\n      command: [python, -c]\n      args:\n      - |\n        import os\n        \n        print("Extracting and partitioning data...")\n        \n        # Simulate data partitioning\n        batch_paths = [\n            "s3://my-bucket/temp/batch-1/",\n            "s3://my-bucket/temp/batch-2/",\n            "s3://my-bucket/temp/batch-3/"\n        ]\n        \n        for i, path in enumerate(batch_paths, 1):\n            with open(f\'/tmp/batch-{i}-path.txt\', \'w\') as f:\n                f.write(path)\n            print(f"Batch {i} path: {path}")\n        \n        print("Data extraction completed")\n\n  # Process data batch template\n  - name: process-data-batch\n    inputs:\n      parameters:\n      - name: batch-id\n      - name: data-subset\n    container:\n      image: apache/spark:3.4.1-python3\n      command: [python, -c]\n      args:\n      - |\n        import time\n        import random\n        \n        batch_id = "{{inputs.parameters.batch-id}}"\n        data_path = "{{inputs.parameters.data-subset}}"\n        \n        print(f"Processing {batch_id} from {data_path}")\n        \n        # Simulate data processing\n        processing_time = random.randint(30, 120)\n        print(f"Processing will take {processing_time} seconds...")\n        \n        for i in range(0, processing_time, 10):\n            time.sleep(10)\n            progress = (i + 10) / processing_time * 100\n            print(f"Progress: {progress:.1f}%")\n        \n        print(f"Batch {batch_id} processing completed")\n      resources:\n        requests:\n          memory: "2Gi"\n          cpu: "1"\n        limits:\n          memory: "4Gi"\n          cpu: "2"\n\n  # Aggregate results template\n  - name: aggregate-data\n    inputs:\n      parameters:\n      - name: output-path\n    container:\n      image: python:3.9-slim\n      command: [python, -c]\n      args:\n      - |\n        import time\n        \n        output_path = "{{inputs.parameters.output-path}}"\n        print(f"Aggregating results to {output_path}")\n        \n        # Simulate aggregation\n        time.sleep(30)\n        \n        print("Data aggregation completed")\n        print(f"Results saved to {output_path}")\n\n  # Generate report template\n  - name: generate-report\n    container:\n      image: python:3.9-slim\n      command: [python, -c]\n      args:\n      - |\n        import json\n        from datetime import datetime\n        \n        print("Generating processing report...")\n        \n        report = {\n            "workflow_id": "{{workflow.name}}",\n            "processing_date": datetime.now().isoformat(),\n            "status": "completed",\n            "batches_processed": 3,\n            "total_records": 1000000,\n            "processing_time": "{{workflow.duration}}"\n        }\n        \n        print("Report generated:")\n        print(json.dumps(report, indent=2))\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-step-6-machine-learning-workflow",children:"\ud83d\udd0d Step 6: Machine Learning Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"ml-training-pipeline",children:"ML Training Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create ml-training.yaml:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: ml-training-\n  namespace: argo\nspec:\n  entrypoint: ml-pipeline\n  arguments:\n    parameters:\n    - name: model-name\n      value: "sentiment-classifier"\n    - name: dataset-path\n      value: "s3://ml-bucket/datasets/sentiment/"\n    - name: model-registry\n      value: "mlflow-server:5000"\n    \n  templates:\n  # ML Pipeline\n  - name: ml-pipeline\n    dag:\n      tasks:\n      - name: data-validation\n        template: validate-dataset\n        \n      - name: data-preprocessing\n        template: preprocess-data\n        dependencies: [data-validation]\n        \n      - name: feature-engineering\n        template: feature-engineering\n        dependencies: [data-preprocessing]\n        \n      - name: model-training\n        template: train-model\n        dependencies: [feature-engineering]\n        \n      - name: model-evaluation\n        template: evaluate-model\n        dependencies: [model-training]\n        \n      - name: model-registration\n        template: register-model\n        dependencies: [model-evaluation]\n        when: "{{tasks.model-evaluation.outputs.parameters.accuracy}} > 0.85"\n\n  # Data validation\n  - name: validate-dataset\n    container:\n      image: python:3.9-slim\n      command: [pip, install, pandas, scikit-learn, && python, -c]\n      args:\n      - |\n        import pandas as pd\n        import numpy as np\n        \n        print("Validating dataset...")\n        \n        # Simulate dataset validation\n        print("\u2713 Dataset format validation passed")\n        print("\u2713 Data quality checks passed")\n        print("\u2713 Feature completeness verified")\n        print("Dataset validation completed")\n\n  # Data preprocessing\n  - name: preprocess-data\n    container:\n      image: python:3.9-slim\n      command: [pip, install, pandas, scikit-learn, nltk, && python, -c]\n      args:\n      - |\n        import pandas as pd\n        import nltk\n        from sklearn.model_selection import train_test_split\n        \n        print("Preprocessing data...")\n        \n        # Simulate data preprocessing\n        print("\u2713 Text cleaning completed")\n        print("\u2713 Data normalization completed")\n        print("\u2713 Train/validation/test split completed")\n        print("Data preprocessing completed")\n\n  # Feature engineering\n  - name: feature-engineering\n    container:\n      image: python:3.9-slim\n      command: [pip, install, pandas, scikit-learn, transformers, && python, -c]\n      args:\n      - |\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        import pickle\n        \n        print("Engineering features...")\n        \n        # Simulate feature engineering\n        print("\u2713 TF-IDF vectorization completed")\n        print("\u2713 Feature selection completed")\n        print("\u2713 Feature scaling completed")\n        print("Feature engineering completed")\n\n  # Model training\n  - name: train-model\n    outputs:\n      parameters:\n      - name: model-path\n        valueFrom:\n          path: /tmp/model-path.txt\n    container:\n      image: python:3.9-slim\n      command: [pip, install, pandas, scikit-learn, joblib, && python, -c]\n      args:\n      - |\n        from sklearn.linear_model import LogisticRegression\n        from sklearn.ensemble import RandomForestClassifier\n        import joblib\n        import os\n        \n        print("Training models...")\n        \n        # Simulate model training\n        models = [\'logistic_regression\', \'random_forest\', \'svm\']\n        \n        for model_name in models:\n            print(f"Training {model_name}...")\n            # Simulate training time\n            import time\n            time.sleep(30)\n            print(f"\u2713 {model_name} training completed")\n        \n        # Save best model path\n        best_model_path = "/tmp/models/best_model.pkl"\n        os.makedirs("/tmp/models", exist_ok=True)\n        \n        with open(\'/tmp/model-path.txt\', \'w\') as f:\n            f.write(best_model_path)\n        \n        print("Model training completed")\n      resources:\n        requests:\n          memory: "4Gi"\n          cpu: "2"\n        limits:\n          memory: "8Gi"\n          cpu: "4"\n\n  # Model evaluation\n  - name: evaluate-model\n    outputs:\n      parameters:\n      - name: accuracy\n        valueFrom:\n          path: /tmp/accuracy.txt\n      - name: f1-score\n        valueFrom:\n          path: /tmp/f1-score.txt\n    container:\n      image: python:3.9-slim\n      command: [pip, install, pandas, scikit-learn, && python, -c]\n      args:\n      - |\n        from sklearn.metrics import accuracy_score, f1_score, classification_report\n        import random\n        \n        print("Evaluating model...")\n        \n        # Simulate model evaluation\n        accuracy = round(random.uniform(0.80, 0.95), 3)\n        f1 = round(random.uniform(0.75, 0.90), 3)\n        \n        print(f"Model Evaluation Results:")\n        print(f"Accuracy: {accuracy}")\n        print(f"F1-Score: {f1}")\n        \n        # Save metrics\n        with open(\'/tmp/accuracy.txt\', \'w\') as f:\n            f.write(str(accuracy))\n        \n        with open(\'/tmp/f1-score.txt\', \'w\') as f:\n            f.write(str(f1))\n        \n        print("Model evaluation completed")\n\n  # Model registration\n  - name: register-model\n    container:\n      image: python:3.9-slim\n      command: [pip, install, mlflow, && python, -c]\n      args:\n      - |\n        import mlflow\n        import mlflow.sklearn\n        \n        model_name = "{{workflow.parameters.model-name}}"\n        accuracy = "{{tasks.model-evaluation.outputs.parameters.accuracy}}"\n        \n        print(f"Registering model {model_name}...")\n        \n        # Simulate model registration\n        print(f"\u2713 Model registered with accuracy: {accuracy}")\n        print(f"\u2713 Model version: v{{workflow.creationTimestamp}}")\n        print("Model registration completed")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-step-7-advanced-workflow-patterns",children:"\ud83d\udcc8 Step 7: Advanced Workflow Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"conditional-workflows",children:"Conditional Workflows"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# Conditional execution based on parameters\n- name: conditional-deploy\n  steps:\n  - - name: deploy-dev\n      template: deploy\n      when: \"{{workflow.parameters.environment}} == 'development'\"\n  - - name: deploy-staging\n      template: deploy\n      when: \"{{workflow.parameters.environment}} == 'staging'\"\n  - - name: deploy-prod\n      template: deploy\n      when: \"{{workflow.parameters.environment}} == 'production' && {{workflow.parameters.approved}} == 'true'\"\n"})}),"\n",(0,r.jsx)(n.h3,{id:"loop-workflows",children:"Loop Workflows"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# Process multiple items with loops\n- name: process-items\n  inputs:\n    parameters:\n    - name: items\n      value: \'["item1", "item2", "item3", "item4", "item5"]\'\n  steps:\n  - - name: process-item\n      template: process-single-item\n      arguments:\n        parameters:\n        - name: item\n          value: "{{item}}"\n      withParam: "{{inputs.parameters.items}}"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"retry-and-error-handling",children:"Retry and Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'- name: resilient-task\n  retryStrategy:\n    limit: 3\n    retryPolicy: "Always"\n    backoff:\n      duration: "1m"\n      factor: 2\n      maxDuration: "10m"\n  container:\n    image: alpine:3.18\n    command: [sh, -c]\n    args: ["echo \'Task with retry logic\'"]\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"\ufe0f-step-8-security-and-rbac",children:"\ud83d\udee1\ufe0f Step 8: Security and RBAC"}),"\n",(0,r.jsx)(n.h3,{id:"create-service-account-and-rbac",children:"Create Service Account and RBAC"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# service-account.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: argo-workflow-sa\n  namespace: argo\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: argo\n  name: argo-workflow-role\nrules:\n- apiGroups: [""]\n  resources: ["pods", "pods/log"]\n  verbs: ["get", "watch", "patch"]\n- apiGroups: [""]\n  resources: ["secrets"]\n  verbs: ["get"]\n- apiGroups: ["argoproj.io"]\n  resources: ["workflows", "workflowtemplates"]\n  verbs: ["get", "list", "watch", "update", "patch"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: argo-workflow-binding\n  namespace: argo\nsubjects:\n- kind: ServiceAccount\n  name: argo-workflow-sa\n  namespace: argo\nroleRef:\n  kind: Role\n  name: argo-workflow-role\n  apiGroup: rbac.authorization.k8s.io\n'})}),"\n",(0,r.jsx)(n.h3,{id:"secure-workflow-template",children:"Secure Workflow Template"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: secure-workflow-\nspec:\n  serviceAccountName: argo-workflow-sa\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 2000\n  templates:\n  - name: secure-task\n    container:\n      image: alpine:3.18\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n      command: [sh, -c]\n      args: [\"echo 'Secure task execution'\"]\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-common-use-cases",children:"\ud83d\udccb Common Use Cases"}),"\n",(0,r.jsxs)(n.h3,{id:"1-cicd-pipelines",children:["1. ",(0,r.jsx)(n.strong,{children:"CI/CD Pipelines"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multi-stage build and deployment"}),"\n",(0,r.jsx)(n.li,{children:"Parallel testing and quality checks"}),"\n",(0,r.jsx)(n.li,{children:"Container image building and scanning"}),"\n",(0,r.jsx)(n.li,{children:"Multi-environment deployments"}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"2-data-processing",children:["2. ",(0,r.jsx)(n.strong,{children:"Data Processing"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ETL pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Batch data processing"}),"\n",(0,r.jsx)(n.li,{children:"Parallel data transformation"}),"\n",(0,r.jsx)(n.li,{children:"Data validation and quality checks"}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"3-machine-learning",children:["3. ",(0,r.jsx)(n.strong,{children:"Machine Learning"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model training pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Hyperparameter tuning"}),"\n",(0,r.jsx)(n.li,{children:"Model evaluation and validation"}),"\n",(0,r.jsx)(n.li,{children:"Automated model deployment"}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"4-infrastructure-automation",children:["4. ",(0,r.jsx)(n.strong,{children:"Infrastructure Automation"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Infrastructure provisioning"}),"\n",(0,r.jsx)(n.li,{children:"Configuration management"}),"\n",(0,r.jsx)(n.li,{children:"Compliance checking"}),"\n",(0,r.jsx)(n.li,{children:"Disaster recovery workflows"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-what-youll-achieve",children:"\u2705 What You'll Achieve"}),"\n",(0,r.jsx)(n.p,{children:"After following this guide, you'll have:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83c\udfd7\ufe0f Container-Native Workflows"})," - Kubernetes-native workflow orchestration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udd04 Complex Pipeline Orchestration"})," - DAG-based workflow execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\u26a1 Parallel Processing"})," - Efficient parallel task execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udd0d Advanced Workflow Patterns"})," - Conditional, loop, and retry patterns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udcca Workflow Monitoring"})," - Real-time workflow tracking and logging"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udee1\ufe0f Security Integration"})," - RBAC and security best practices"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\ude80 Scalable Execution"})," - Auto-scaling workflow execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udcc8 Multi-Use Case Support"})," - CI/CD, data processing, ML, and more"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Argo Workflows is now configured for your container-native workflow orchestration!"})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);