"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1643],{3684:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"Build-ML-Tools/kubeflow","title":"Kubeflow","description":"Kubeflow is a machine learning toolkit for Kubernetes that enables scalable ML workflows, model training, and deployment on Kubernetes clusters.","source":"@site/docs/Build-ML-Tools/kubeflow.md","sourceDirName":"Build-ML-Tools","slug":"/BuildMLTools/Kubeflow","permalink":"/docs/BuildMLTools/Kubeflow","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Build-ML-Tools/kubeflow.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Kubeflow","description":"Kubeflow is a machine learning toolkit for Kubernetes that enables scalable ML workflows, model training, and deployment on Kubernetes clusters.","slug":"/BuildMLTools/Kubeflow","keywords":["Kubeflow","Kubernetes ML","ML pipelines","model training","ML workflows","Kubernetes","distributed training","MLOps"]},"sidebar":"tutorialSidebar","previous":{"title":"Jupyter Notebook","permalink":"/docs/BuildMLTools/Jupyter"},"next":{"title":"CUDA & TensorRT","permalink":"/docs/BuildMLTools/CUDA-TensorRT"}}');var s=t(4848),i=t(8453);const a={sidebar_position:7,title:"Kubeflow",description:"Kubeflow is a machine learning toolkit for Kubernetes that enables scalable ML workflows, model training, and deployment on Kubernetes clusters.",slug:"/BuildMLTools/Kubeflow",keywords:["Kubeflow","Kubernetes ML","ML pipelines","model training","ML workflows","Kubernetes","distributed training","MLOps"]},o="\ud83d\ude80 Scalable ML Workflows with Kubeflow",l={},p=[{value:"Key Features",id:"key-features",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"\ud83e\uddf0 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udd27 Step 1: Install Kubeflow",id:"-step-1-install-kubeflow",level:2},{value:"Install Kubeflow using Kustomize",id:"install-kubeflow-using-kustomize",level:3},{value:"Alternative: Install with Kubeflow Operator",id:"alternative-install-with-kubeflow-operator",level:3},{value:"\ud83c\udfd7\ufe0f Step 2: Setup Kubeflow Development Environment",id:"\ufe0f-step-2-setup-kubeflow-development-environment",level:2},{value:"\ud83d\udcc1 Step 3: Create Your First ML Pipeline",id:"-step-3-create-your-first-ml-pipeline",level:2},{value:"\u25b6\ufe0f Step 4: Execute and Monitor Pipelines",id:"\ufe0f-step-4-execute-and-monitor-pipelines",level:2},{value:"\ud83d\udcca Step 5: Model Serving with KServe",id:"-step-5-model-serving-with-kserve",level:2},{value:"\ud83d\udd0d What You&#39;ll See",id:"-what-youll-see",level:2},{value:"Kubeflow Dashboard",id:"kubeflow-dashboard",level:3},{value:"Pipeline Execution Output",id:"pipeline-execution-output",level:3},{value:"Model Serving Results",id:"model-serving-results",level:3},{value:"Pros &amp; Cons",id:"pros--cons",level:2},{value:"\u2705 Pros",id:"-pros",level:3},{value:"\u274c Cons",id:"-cons",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"-scalable-ml-workflows-with-kubeflow",children:"\ud83d\ude80 Scalable ML Workflows with Kubeflow"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Kubeflow"})," is a ",(0,s.jsx)(n.strong,{children:"machine learning toolkit"})," for ",(0,s.jsx)(n.strong,{children:"Kubernetes"})," that enables ",(0,s.jsx)(n.strong,{children:"scalable ML workflows"}),", ",(0,s.jsx)(n.strong,{children:"distributed training"}),", and ",(0,s.jsx)(n.strong,{children:"model deployment"})," on Kubernetes clusters. Perfect for ",(0,s.jsx)(n.strong,{children:"enterprise ML"})," with ",(0,s.jsx)(n.strong,{children:"pipeline orchestration"}),", ",(0,s.jsx)(n.strong,{children:"experiment tracking"}),", and ",(0,s.jsx)(n.strong,{children:"multi-user"})," environments."]}),"\n",(0,s.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ML Pipelines"}),": Build and orchestrate complex ML workflows"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Training"}),": Scale training across multiple nodes and GPUs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Serving"}),": Deploy models with KServe for production inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-User"}),": Secure multi-tenant ML platform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Notebook Servers"}),": Managed Jupyter environments"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enterprise ML"}),": Large-scale machine learning operations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Training"}),": Multi-GPU and multi-node model training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ML Pipeline Orchestration"}),": Complex workflow management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Deployment"}),": Production-ready model serving"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-prerequisites",children:"\ud83e\uddf0 Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kubernetes cluster"})," (1.21+) with sufficient resources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"kubectl"})," configured for cluster access"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Helm 3"})," for package management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Docker"})," for container builds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"16GB+ RAM"})," and ",(0,s.jsx)(n.strong,{children:"4+ CPUs"})," recommended per node"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-step-1-install-kubeflow",children:"\ud83d\udd27 Step 1: Install Kubeflow"}),"\n",(0,s.jsx)(n.h3,{id:"install-kubeflow-using-kustomize",children:"Install Kubeflow using Kustomize"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Install kustomize\ncurl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash\nsudo mv kustomize /usr/local/bin/\n\n# Clone Kubeflow manifests\ngit clone https://github.com/kubeflow/manifests.git\ncd manifests\n\n# Install Kubeflow components\nwhile ! kustomize build example | kubectl apply -f -; do echo "Retrying to apply resources"; sleep 10; done\n\n# Wait for all pods to be ready\nkubectl wait --for=condition=ready pod -l \'app in (ml-pipeline,metadata-grpc-server)\' --timeout=1800s -n kubeflow\n'})}),"\n",(0,s.jsx)(n.h3,{id:"alternative-install-with-kubeflow-operator",children:"Alternative: Install with Kubeflow Operator"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# kubeflow-operator.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubeflow\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeflow-operator\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubeflow-operator\n  template:\n    metadata:\n      labels:\n        app: kubeflow-operator\n    spec:\n      containers:\n      - name: operator\n        image: kubeflownotebookswg/kubeflow-operator:latest\n        env:\n        - name: WATCH_NAMESPACE\n          value: ""\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: "kubeflow-operator"\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-step-2-setup-kubeflow-development-environment",children:"\ud83c\udfd7\ufe0f Step 2: Setup Kubeflow Development Environment"}),"\n",(0,s.jsx)(n.p,{children:"Create a comprehensive development setup:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# kubeflow-dev-setup.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubeflow-dev\n---\n# Jupyter Notebook Server\napiVersion: kubeflow.org/v1\nkind: Notebook\nmetadata:\n  name: ml-notebook\n  namespace: kubeflow-dev\nspec:\n  template:\n    spec:\n      containers:\n      - name: notebook\n        image: jupyter/tensorflow-notebook:latest\n        resources:\n          requests:\n            cpu: "2"\n            memory: "4Gi"\n            nvidia.com/gpu: "1"\n          limits:\n            cpu: "4"\n            memory: "8Gi"\n            nvidia.com/gpu: "1"\n        volumeMounts:\n        - name: workspace\n          mountPath: /home/jovyan/work\n        - name: data\n          mountPath: /home/jovyan/data\n        env:\n        - name: JUPYTER_ENABLE_LAB\n          value: "yes"\n      volumes:\n      - name: workspace\n        persistentVolumeClaim:\n          claimName: workspace-pvc\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\n# Persistent Volume Claims\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: workspace-pvc\n  namespace: kubeflow-dev\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\n  namespace: kubeflow-dev\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n'})}),"\n",(0,s.jsx)(n.p,{children:"Apply the configuration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f kubeflow-dev-setup.yaml\n\n# Check notebook status\nkubectl get notebooks -n kubeflow-dev\n\n# Port forward to access notebook\nkubectl port-forward -n kubeflow-dev service/ml-notebook 8888:80\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-step-3-create-your-first-ml-pipeline",children:"\ud83d\udcc1 Step 3: Create Your First ML Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Create a comprehensive ML pipeline using Kubeflow Pipelines:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# ml_pipeline.py\nimport kfp\nfrom kfp import dsl\nfrom kfp.components import create_component_from_func\nfrom typing import NamedTuple\n\n# Component 1: Data Preprocessing\n@create_component_from_func\ndef preprocess_data(\n    input_data_path: str,\n    output_data_path: str,\n    test_size: float = 0.2\n) -> NamedTuple('Outputs', [('train_data_path', str), ('test_data_path', str), ('num_features', int)]):\n    \"\"\"Preprocess data for ML training\"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import joblib\n    import os\n    \n    # Load data\n    print(f\"Loading data from {input_data_path}\")\n    df = pd.read_csv(input_data_path)\n    \n    # Basic preprocessing\n    df = df.dropna()\n    \n    # Separate features and target\n    X = df.drop('target', axis=1)\n    y = df['target']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42, stratify=y\n    )\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Create output directory\n    os.makedirs(output_data_path, exist_ok=True)\n    \n    # Save processed data\n    train_data_path = os.path.join(output_data_path, 'train_data.npz')\n    test_data_path = os.path.join(output_data_path, 'test_data.npz')\n    \n    np.savez(train_data_path, X=X_train_scaled, y=y_train.values)\n    np.savez(test_data_path, X=X_test_scaled, y=y_test.values)\n    \n    # Save scaler\n    scaler_path = os.path.join(output_data_path, 'scaler.pkl')\n    joblib.dump(scaler, scaler_path)\n    \n    print(f\"Preprocessed data saved to {output_data_path}\")\n    print(f\"Number of features: {X_train_scaled.shape[1]}\")\n    print(f\"Training samples: {X_train_scaled.shape[0]}\")\n    print(f\"Test samples: {X_test_scaled.shape[0]}\")\n    \n    return (train_data_path, test_data_path, X_train_scaled.shape[1])\n\n# Component 2: Model Training\n@create_component_from_func\ndef train_model(\n    train_data_path: str,\n    model_output_path: str,\n    model_type: str = 'random_forest',\n    n_estimators: int = 100,\n    max_depth: int = 10\n) -> NamedTuple('Outputs', [('model_path', str), ('train_accuracy', float)]):\n    \"\"\"Train ML model\"\"\"\n    import numpy as np\n    import joblib\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC\n    from sklearn.metrics import accuracy_score\n    import os\n    \n    # Load training data\n    print(f\"Loading training data from {train_data_path}\")\n    data = np.load(train_data_path)\n    X_train, y_train = data['X'], data['y']\n    \n    # Initialize model based on type\n    if model_type == 'random_forest':\n        model = RandomForestClassifier(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            random_state=42\n        )\n    elif model_type == 'logistic_regression':\n        model = LogisticRegression(random_state=42, max_iter=1000)\n    elif model_type == 'svm':\n        model = SVC(random_state=42, probability=True)\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    \n    # Train model\n    print(f\"Training {model_type} model...\")\n    model.fit(X_train, y_train)\n    \n    # Calculate training accuracy\n    y_train_pred = model.predict(X_train)\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    \n    # Save model\n    os.makedirs(model_output_path, exist_ok=True)\n    model_path = os.path.join(model_output_path, f'{model_type}_model.pkl')\n    joblib.dump(model, model_path)\n    \n    print(f\"Model saved to {model_path}\")\n    print(f\"Training accuracy: {train_accuracy:.4f}\")\n    \n    return (model_path, train_accuracy)\n\n# Component 3: Model Evaluation\n@create_component_from_func\ndef evaluate_model(\n    model_path: str,\n    test_data_path: str,\n    metrics_output_path: str\n) -> NamedTuple('Outputs', [('test_accuracy', float), ('precision', float), ('recall', float)]):\n    \"\"\"Evaluate trained model\"\"\"\n    import numpy as np\n    import joblib\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n    import json\n    import os\n    \n    # Load model and test data\n    print(f\"Loading model from {model_path}\")\n    model = joblib.load(model_path)\n    \n    print(f\"Loading test data from {test_data_path}\")\n    data = np.load(test_data_path)\n    X_test, y_test = data['X'], data['y']\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    test_accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    \n    # Generate detailed report\n    report = classification_report(y_test, y_pred, output_dict=True)\n    \n    # Save metrics\n    os.makedirs(metrics_output_path, exist_ok=True)\n    metrics_file = os.path.join(metrics_output_path, 'evaluation_metrics.json')\n    \n    metrics = {\n        'test_accuracy': test_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'detailed_report': report\n    }\n    \n    with open(metrics_file, 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    print(f\"Evaluation completed!\")\n    print(f\"Test accuracy: {test_accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    \n    return (test_accuracy, precision, recall)\n\n# Component 4: Model Deployment\n@create_component_from_func\ndef deploy_model(\n    model_path: str,\n    model_name: str,\n    namespace: str = 'kubeflow-user-example-com'\n) -> str:\n    \"\"\"Deploy model using KServe\"\"\"\n    import yaml\n    import subprocess\n    import os\n    \n    # Create InferenceService manifest\n    inference_service = {\n        'apiVersion': 'serving.kserve.io/v1beta1',\n        'kind': 'InferenceService',\n        'metadata': {\n            'name': model_name,\n            'namespace': namespace\n        },\n        'spec': {\n            'predictor': {\n                'sklearn': {\n                    'storageUri': model_path,\n                    'resources': {\n                        'requests': {\n                            'cpu': '100m',\n                            'memory': '256Mi'\n                        },\n                        'limits': {\n                            'cpu': '1',\n                            'memory': '1Gi'\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \n    # Save manifest to file\n    manifest_file = f'{model_name}-inference-service.yaml'\n    with open(manifest_file, 'w') as f:\n        yaml.dump(inference_service, f)\n    \n    # Apply manifest\n    try:\n        result = subprocess.run(['kubectl', 'apply', '-f', manifest_file], \n                              capture_output=True, text=True, check=True)\n        print(f\"InferenceService created successfully: {result.stdout}\")\n        \n        # Get service URL\n        get_url_cmd = ['kubectl', 'get', 'inferenceservice', model_name, \n                      '-n', namespace, '-o', 'jsonpath={.status.url}']\n        url_result = subprocess.run(get_url_cmd, capture_output=True, text=True)\n        service_url = url_result.stdout.strip()\n        \n        print(f\"Model deployed at: {service_url}\")\n        return service_url\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error deploying model: {e.stderr}\")\n        raise\n\n# Define the pipeline\n@dsl.pipeline(\n    name='ML Training Pipeline',\n    description='Complete ML pipeline with preprocessing, training, evaluation, and deployment'\n)\ndef ml_training_pipeline(\n    input_data_path: str = '/data/dataset.csv',\n    model_type: str = 'random_forest',\n    n_estimators: int = 100,\n    max_depth: int = 10,\n    test_size: float = 0.2,\n    model_name: str = 'ml-model'\n):\n    \"\"\"Complete ML training pipeline\"\"\"\n    \n    # Step 1: Preprocess data\n    preprocess_task = preprocess_data(\n        input_data_path=input_data_path,\n        output_data_path='/tmp/processed_data',\n        test_size=test_size\n    )\n    \n    # Step 2: Train model\n    train_task = train_model(\n        train_data_path=preprocess_task.outputs['train_data_path'],\n        model_output_path='/tmp/model',\n        model_type=model_type,\n        n_estimators=n_estimators,\n        max_depth=max_depth\n    )\n    \n    # Step 3: Evaluate model\n    evaluate_task = evaluate_model(\n        model_path=train_task.outputs['model_path'],\n        test_data_path=preprocess_task.outputs['test_data_path'],\n        metrics_output_path='/tmp/metrics'\n    )\n    \n    # Step 4: Deploy model (conditional on good performance)\n    with dsl.Condition(evaluate_task.outputs['test_accuracy'] > 0.8):\n        deploy_task = deploy_model(\n            model_path=train_task.outputs['model_path'],\n            model_name=model_name\n        )\n\nif __name__ == '__main__':\n    # Compile pipeline\n    kfp.compiler.Compiler().compile(ml_training_pipeline, 'ml_training_pipeline.yaml')\n    print(\"Pipeline compiled successfully!\")\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-step-4-execute-and-monitor-pipelines",children:"\u25b6\ufe0f Step 4: Execute and Monitor Pipelines"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# pipeline_runner.py\nimport kfp\nfrom kfp import Client\n\n# Connect to Kubeflow Pipelines\nclient = Client(host='http://localhost:8080')  # Adjust host as needed\n\n# Upload and run pipeline\ndef run_ml_pipeline():\n    \"\"\"Upload and execute the ML pipeline\"\"\"\n    \n    # Create experiment\n    experiment_name = 'ml-training-experiment'\n    try:\n        experiment = client.create_experiment(name=experiment_name)\n    except:\n        experiment = client.get_experiment(experiment_name=experiment_name)\n    \n    # Upload pipeline\n    pipeline_name = 'ML Training Pipeline'\n    pipeline_package_path = 'ml_training_pipeline.yaml'\n    \n    try:\n        pipeline = client.upload_pipeline(\n            pipeline_package_path=pipeline_package_path,\n            pipeline_name=pipeline_name\n        )\n    except:\n        # Pipeline might already exist\n        pipelines = client.list_pipelines()\n        pipeline = next((p for p in pipelines.pipelines if p.name == pipeline_name), None)\n    \n    # Run pipeline\n    run_name = 'ml-training-run-001'\n    arguments = {\n        'input_data_path': '/data/wine_dataset.csv',\n        'model_type': 'random_forest',\n        'n_estimators': 200,\n        'max_depth': 15,\n        'test_size': 0.2,\n        'model_name': 'wine-classifier'\n    }\n    \n    run_result = client.run_pipeline(\n        experiment_id=experiment.id,\n        job_name=run_name,\n        pipeline_id=pipeline.id,\n        params=arguments\n    )\n    \n    print(f\"Pipeline run started: {run_result.id}\")\n    print(f\"Monitor at: http://localhost:8080/#/runs/details/{run_result.id}\")\n    \n    # Wait for completion\n    run_detail = client.wait_for_run_completion(run_result.id, timeout=3600)\n    print(f\"Pipeline completed with status: {run_detail.run.status}\")\n    \n    return run_result\n\n# Monitor pipeline execution\ndef monitor_pipeline_run(run_id: str):\n    \"\"\"Monitor pipeline execution\"\"\"\n    \n    run_detail = client.get_run(run_id)\n    print(f\"Run Status: {run_detail.run.status}\")\n    print(f\"Created: {run_detail.run.created_at}\")\n    print(f\"Finished: {run_detail.run.finished_at}\")\n    \n    # Get run metrics\n    if run_detail.run.status == 'Succeeded':\n        metrics = client.get_run_metrics(run_id)\n        for metric in metrics:\n            print(f\"Metric: {metric.name} = {metric.number_value}\")\n\nif __name__ == '__main__':\n    # Run the pipeline\n    run_result = run_ml_pipeline()\n    \n    # Monitor execution\n    monitor_pipeline_run(run_result.id)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-step-5-model-serving-with-kserve",children:"\ud83d\udcca Step 5: Model Serving with KServe"}),"\n",(0,s.jsx)(n.p,{children:"Create a model serving configuration:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# model-serving.yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: wine-classifier\n  namespace: kubeflow-user-example-com\nspec:\n  predictor:\n    sklearn:\n      storageUri: "gs://my-bucket/models/wine-classifier"\n      resources:\n        requests:\n          cpu: 100m\n          memory: 256Mi\n        limits:\n          cpu: 1\n          memory: 1Gi\n  transformer:\n    containers:\n    - name: transformer\n      image: my-registry/wine-transformer:latest\n      env:\n      - name: STORAGE_URI\n        value: "gs://my-bucket/models/wine-classifier"\n---\n# Autoscaling configuration\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: wine-classifier-predictor-default\n  namespace: kubeflow-user-example-com\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: wine-classifier-predictor-default\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n'})}),"\n",(0,s.jsx)(n.p,{children:"Test the deployed model:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# test_model_serving.py\nimport requests\nimport json\nimport numpy as np\n\ndef test_model_inference():\n    """Test the deployed model"""\n    \n    # Model endpoint (adjust URL as needed)\n    model_url = "http://wine-classifier.kubeflow-user-example-com.example.com/v1/models/wine-classifier:predict"\n    \n    # Sample data for prediction\n    sample_data = {\n        "instances": [\n            [13.20, 1.78, 2.14, 11.2, 100, 2.65, 2.76, 0.26, 1.28, 4.38, 1.05, 3.40, 1050],\n            [12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520]\n        ]\n    }\n    \n    # Make prediction request\n    headers = {\'Content-Type\': \'application/json\'}\n    response = requests.post(model_url, data=json.dumps(sample_data), headers=headers)\n    \n    if response.status_code == 200:\n        predictions = response.json()\n        print("Predictions:", predictions)\n        return predictions\n    else:\n        print(f"Error: {response.status_code} - {response.text}")\n        return None\n\ndef load_test_model():\n    """Perform load testing on the model"""\n    import concurrent.futures\n    import time\n    \n    def make_request():\n        return test_model_inference()\n    \n    # Concurrent requests\n    num_requests = 100\n    start_time = time.time()\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        futures = [executor.submit(make_request) for _ in range(num_requests)]\n        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n    \n    end_time = time.time()\n    successful_requests = sum(1 for r in results if r is not None)\n    \n    print(f"Load test completed:")\n    print(f"Total requests: {num_requests}")\n    print(f"Successful requests: {successful_requests}")\n    print(f"Success rate: {successful_requests/num_requests*100:.2f}%")\n    print(f"Total time: {end_time - start_time:.2f} seconds")\n    print(f"Requests per second: {num_requests/(end_time - start_time):.2f}")\n\nif __name__ == \'__main__\':\n    # Test single prediction\n    test_model_inference()\n    \n    # Perform load test\n    load_test_model()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"-what-youll-see",children:"\ud83d\udd0d What You'll See"}),"\n",(0,s.jsx)(n.h3,{id:"kubeflow-dashboard",children:"Kubeflow Dashboard"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pipelines"}),": Visual pipeline editor and execution history"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Experiments"}),": Experiment tracking and comparison"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Notebooks"}),": Managed Jupyter notebook servers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Models"}),": Model registry and serving management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Artifacts"}),": Pipeline artifacts and metadata"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"pipeline-execution-output",children:"Pipeline Execution Output"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Pipeline run started: 12345678-abcd-efgh-ijkl-123456789012\nMonitor at: http://localhost:8080/#/runs/details/12345678-abcd-efgh-ijkl-123456789012\n\nStep 1: Data Preprocessing\n\u2705 Preprocessed data saved to /tmp/processed_data\n\u2705 Number of features: 13\n\u2705 Training samples: 142, Test samples: 36\n\nStep 2: Model Training\n\u2705 Training random_forest model...\n\u2705 Model saved to /tmp/model/random_forest_model.pkl\n\u2705 Training accuracy: 0.9648\n\nStep 3: Model Evaluation\n\u2705 Evaluation completed!\n\u2705 Test accuracy: 0.9722\n\u2705 Precision: 0.9733\n\u2705 Recall: 0.9722\n\nStep 4: Model Deployment\n\u2705 InferenceService created successfully\n\u2705 Model deployed at: http://wine-classifier.kubeflow-user-example-com.example.com\n\nPipeline completed with status: Succeeded\n"})}),"\n",(0,s.jsx)(n.h3,{id:"model-serving-results",children:"Model Serving Results"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "predictions": [\n    [0.1, 0.2, 0.7],  // Class probabilities for sample 1\n    [0.8, 0.1, 0.1]   // Class probabilities for sample 2\n  ]\n}\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"pros--cons",children:"Pros & Cons"}),"\n",(0,s.jsx)(n.h3,{id:"-pros",children:"\u2705 Pros"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kubernetes Native"}),": Leverages Kubernetes for scalability and reliability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complete ML Platform"}),": End-to-end ML workflow management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Training"}),": Multi-GPU and multi-node training support"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Serving"}),": Production-ready model deployment with KServe"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-User"}),": Secure multi-tenant environment"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-cons",children:"\u274c Cons"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complexity"}),": Requires Kubernetes expertise and significant setup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Intensive"}),": High resource requirements for cluster"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning Curve"}),": Complex platform with many components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintenance"}),": Requires ongoing cluster and component maintenance"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsxs)(n.p,{children:["Kubeflow is the ",(0,s.jsx)(n.strong,{children:"enterprise-grade solution"})," for ",(0,s.jsx)(n.strong,{children:"scalable ML operations"})," on Kubernetes. Choose Kubeflow when you need:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large-scale ML workflows"})," with distributed training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enterprise ML platform"})," with multi-user support"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kubernetes-native"})," ML operations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Production-ready"})," model serving and monitoring"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The combination of pipeline orchestration, distributed training, and model serving makes Kubeflow ideal for organizations running ML at scale on Kubernetes."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You've Achieved:"}),"\n\u2705 Set up a complete Kubeflow ML platform",(0,s.jsx)(n.br,{}),"\n","\u2705 Created comprehensive ML pipelines with multiple components",(0,s.jsx)(n.br,{}),"\n","\u2705 Implemented distributed model training and evaluation",(0,s.jsx)(n.br,{}),"\n","\u2705 Deployed models for production inference with KServe",(0,s.jsx)(n.br,{}),"\n","\u2705 Established monitoring and load testing capabilities",(0,s.jsx)(n.br,{}),"\n","\u2705 Built scalable and reproducible ML workflows"]})]})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);