"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9900],{490:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"Observability/elk-stack","title":"ELK Stack (Elastic Stack)","description":"ELK Stack (Elasticsearch, Logstash, Kibana) is a powerful platform for searching, analyzing, and visualizing log data in real time with comprehensive observability features.","source":"@site/docs/Observability/elk-stack.md","sourceDirName":"Observability","slug":"/Observability/ELKStack","permalink":"/docs/Observability/ELKStack","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Observability/elk-stack.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"ELK Stack (Elastic Stack)","description":"ELK Stack (Elasticsearch, Logstash, Kibana) is a powerful platform for searching, analyzing, and visualizing log data in real time with comprehensive observability features.","slug":"/Observability/ELKStack","keywords":["ELK Stack","Elastic Stack","Elasticsearch","Logstash","Kibana","Beats","log management","log analysis","data visualization","search analytics","observability","centralized logging"]},"sidebar":"tutorialSidebar","previous":{"title":"New Relic","permalink":"/docs/Observability/NewRelic"},"next":{"title":"Jaeger","permalink":"/docs/Observability/Jaeger"}}');var t=s(4848),r=s(8453);const i={sidebar_position:5,title:"ELK Stack (Elastic Stack)",description:"ELK Stack (Elasticsearch, Logstash, Kibana) is a powerful platform for searching, analyzing, and visualizing log data in real time with comprehensive observability features.",slug:"/Observability/ELKStack",keywords:["ELK Stack","Elastic Stack","Elasticsearch","Logstash","Kibana","Beats","log management","log analysis","data visualization","search analytics","observability","centralized logging"]},o="\ud83d\udcca Comprehensive Log Management with ELK Stack",l={},d=[{value:"Key Features",id:"key-features",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"\ud83e\uddf0 Prerequisites",id:"-prerequisites",level:2},{value:"Set Up ELK Stack with Docker",id:"set-up-elk-stack-with-docker",level:2},{value:"Configuration Files",id:"configuration-files",level:2},{value:"Logstash Configuration",id:"logstash-configuration",level:3},{value:"Filebeat Configuration",id:"filebeat-configuration",level:3},{value:"Kibana Configuration",id:"kibana-configuration",level:3},{value:"Sample Applications and Log Generation",id:"sample-applications-and-log-generation",level:2},{value:"Python Application with Structured Logging",id:"python-application-with-structured-logging",level:3},{value:"Log Generator Script",id:"log-generator-script",level:3},{value:"Kibana Dashboards and Visualizations",id:"kibana-dashboards-and-visualizations",level:2},{value:"Index Patterns and Field Mappings",id:"index-patterns-and-field-mappings",level:3},{value:"Start ELK Stack",id:"start-elk-stack",level:2},{value:"Common Use Cases",id:"common-use-cases",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"-comprehensive-log-management-with-elk-stack",children:"\ud83d\udcca Comprehensive Log Management with ELK Stack"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ELK Stack"})," (now called ",(0,t.jsx)(n.strong,{children:"Elastic Stack"}),") is a collection of open-source products: ",(0,t.jsx)(n.strong,{children:"Elasticsearch"}),", ",(0,t.jsx)(n.strong,{children:"Logstash"}),", ",(0,t.jsx)(n.strong,{children:"Kibana"}),", and ",(0,t.jsx)(n.strong,{children:"Beats"}),". Together they provide a ",(0,t.jsx)(n.strong,{children:"powerful platform"})," for ",(0,t.jsx)(n.strong,{children:"searching"}),", ",(0,t.jsx)(n.strong,{children:"analyzing"}),", and ",(0,t.jsx)(n.strong,{children:"visualizing"})," log data in real time with ",(0,t.jsx)(n.strong,{children:"comprehensive observability"})," features."]}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Elasticsearch"}),": Distributed search and analytics engine"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Logstash"}),": Data processing pipeline for ingesting and transforming logs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kibana"}),": Data visualization and exploration platform"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Beats"}),": Lightweight data shippers for various data sources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Analytics"}),": Process and analyze data as it arrives"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Centralized Logging"}),": Aggregate logs from multiple applications and services"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security Analytics"}),": Detect threats and analyze security events"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Application Performance Monitoring"}),": Track application metrics and performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Business Intelligence"}),": Extract insights from operational data"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-prerequisites",children:"\ud83e\uddf0 Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Docker & Docker Compose"})," installed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"8GB+ RAM"})," recommended for Elasticsearch"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SSD storage"})," for better performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network access"})," for log collection from multiple sources"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"set-up-elk-stack-with-docker",children:"Set Up ELK Stack with Docker"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create a file named docker-compose.yml"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  # Elasticsearch\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    container_name: elasticsearch\n    restart: unless-stopped\n    environment:\n      - node.name=elasticsearch\n      - cluster.name=elk-cluster\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true\n      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"\n      - xpack.security.enabled=false\n      - xpack.security.enrollment.enabled=false\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - elasticsearch-data:/usr/share/elasticsearch/data\n    ports:\n      - "9200:9200"\n      - "9300:9300"\n    healthcheck:\n      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n\n  # Logstash\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    container_name: logstash\n    restart: unless-stopped\n    volumes:\n      - ./logstash/config:/usr/share/logstash/config:ro\n      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro\n    ports:\n      - "5044:5044"\n      - "5000:5000/tcp"\n      - "5000:5000/udp"\n      - "9600:9600"\n    environment:\n      - "LS_JAVA_OPTS=-Xmx512m -Xms512m"\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n\n  # Kibana\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    container_name: kibana\n    restart: unless-stopped\n    ports:\n      - "5601:5601"\n    environment:\n      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=kibana_password\n    volumes:\n      - ./kibana/config:/usr/share/kibana/config:ro\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n\n  # Filebeat for log shipping\n  filebeat:\n    image: docker.elastic.co/beats/filebeat:8.11.0\n    container_name: filebeat\n    restart: unless-stopped\n    user: root\n    volumes:\n      - ./filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./logs:/var/log/app:ro\n    environment:\n      - output.elasticsearch.hosts=["elasticsearch:9200"]\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n\n  # Sample application generating logs\n  sample-app:\n    image: nginx:alpine\n    container_name: sample-nginx\n    restart: unless-stopped\n    ports:\n      - "8080:80"\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./logs:/var/log/nginx\n    labels:\n      - "co.elastic.logs/enabled=true"\n      - "co.elastic.logs/module=nginx"\n\nvolumes:\n  elasticsearch-data:\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create necessary directories:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir -p logstash/config logstash/pipeline kibana/config filebeat/config nginx logs\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"configuration-files",children:"Configuration Files"}),"\n",(0,t.jsx)(n.h3,{id:"logstash-configuration",children:"Logstash Configuration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create logstash/config/logstash.yml:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'http.host: "0.0.0.0"\nxpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]\npath.config: /usr/share/logstash/pipeline\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create logstash/pipeline/logstash.conf:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ruby",children:'input {\n  beats {\n    port => 5044\n  }\n  \n  tcp {\n    port => 5000\n    codec => json_lines\n  }\n  \n  udp {\n    port => 5000\n    codec => json_lines\n  }\n  \n  http {\n    port => 8080\n  }\n}\n\nfilter {\n  # Parse nginx logs\n  if [fields][logtype] == "nginx" {\n    grok {\n      match => { \n        "message" => "%{NGINXACCESS}" \n      }\n    }\n    \n    date {\n      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]\n    }\n    \n    mutate {\n      convert => { "response" => "integer" }\n      convert => { "bytes" => "integer" }\n    }\n  }\n  \n  # Parse application logs\n  if [fields][logtype] == "application" {\n    json {\n      source => "message"\n    }\n    \n    date {\n      match => [ "timestamp", "ISO8601" ]\n    }\n    \n    if [level] {\n      mutate {\n        uppercase => [ "level" ]\n      }\n    }\n  }\n  \n  # Parse Docker container logs\n  if [container] {\n    mutate {\n      add_field => { "container_name" => "%{[container][name]}" }\n      add_field => { "container_image" => "%{[container][image][name]}" }\n    }\n  }\n  \n  # GeoIP enrichment for IP addresses\n  if [clientip] {\n    geoip {\n      source => "clientip"\n      target => "geoip"\n    }\n  }\n  \n  # User agent parsing\n  if [agent] {\n    useragent {\n      source => "agent"\n      target => "user_agent"\n    }\n  }\n  \n  # Add custom fields\n  mutate {\n    add_field => { "[@metadata][index]" => "logs-%{+YYYY.MM.dd}" }\n    add_field => { "processed_at" => "%{@timestamp}" }\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => ["elasticsearch:9200"]\n    index => "%{[@metadata][index]}"\n    template_name => "logs"\n    template_pattern => "logs-*"\n    template => {\n      "index_patterns" => ["logs-*"],\n      "settings" => {\n        "number_of_shards" => 1,\n        "number_of_replicas" => 0\n      },\n      "mappings" => {\n        "properties" => {\n          "@timestamp" => { "type" => "date" },\n          "level" => { "type" => "keyword" },\n          "message" => { "type" => "text" },\n          "host" => { "type" => "keyword" },\n          "container_name" => { "type" => "keyword" },\n          "geoip" => {\n            "properties" => {\n              "location" => { "type" => "geo_point" }\n            }\n          }\n        }\n      }\n    }\n  }\n  \n  # Debug output\n  stdout { \n    codec => rubydebug \n  }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"filebeat-configuration",children:"Filebeat Configuration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create filebeat/config/filebeat.yml:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"filebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /var/log/app/*.log\n  fields:\n    logtype: application\n  fields_under_root: true\n  multiline.pattern: '^\\d{4}-\\d{2}-\\d{2}'\n  multiline.negate: true\n  multiline.match: after\n\n- type: container\n  enabled: true\n  paths:\n    - '/var/lib/docker/containers/*/*.log'\n  processors:\n    - add_docker_metadata:\n        host: \"unix:///var/run/docker.sock\"\n\n- type: log\n  enabled: true\n  paths:\n    - /var/log/nginx/access.log\n  fields:\n    logtype: nginx\n  fields_under_root: true\n\nprocessors:\n  - add_host_metadata:\n      when.not.contains.tags: forwarded\n  - add_cloud_metadata: ~\n  - add_docker_metadata: ~\n\noutput.logstash:\n  hosts: [\"logstash:5044\"]\n\nlogging.level: info\nlogging.to_files: true\nlogging.files:\n  path: /var/log/filebeat\n  name: filebeat\n  keepfiles: 7\n  permissions: 0644\n"})}),"\n",(0,t.jsx)(n.h3,{id:"kibana-configuration",children:"Kibana Configuration"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create kibana/config/kibana.yml:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'server.name: kibana\nserver.host: "0.0.0.0"\nelasticsearch.hosts: [ "http://elasticsearch:9200" ]\nmonitoring.ui.container.elasticsearch.enabled: true\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"sample-applications-and-log-generation",children:"Sample Applications and Log Generation"}),"\n",(0,t.jsx)(n.h3,{id:"python-application-with-structured-logging",children:"Python Application with Structured Logging"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create sample-apps/python-app/app.py:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport json\nimport logging\nimport time\nimport random\nfrom datetime import datetime\nfrom flask import Flask, request, jsonify\nimport requests\n\n# Configure structured logging\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'logger': record.name,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add extra fields if present\n        if hasattr(record, 'user_id'):\n            log_entry['user_id'] = record.user_id\n        if hasattr(record, 'request_id'):\n            log_entry['request_id'] = record.request_id\n        if hasattr(record, 'duration'):\n            log_entry['duration'] = record.duration\n            \n        return json.dumps(log_entry)\n\n# Set up logging\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nhandler = logging.FileHandler('/var/log/app/application.log')\nhandler.setFormatter(JSONFormatter())\nlogger.addHandler(handler)\n\n# Console handler for debugging\nconsole_handler = logging.StreamHandler()\nconsole_handler.setFormatter(JSONFormatter())\nlogger.addHandler(console_handler)\n\napp = Flask(__name__)\n\n@app.before_request\ndef before_request():\n    request.start_time = time.time()\n    request.request_id = f\"req-{random.randint(100000, 999999)}\"\n\n@app.after_request\ndef after_request(response):\n    duration = time.time() - request.start_time\n    \n    logger.info(\"Request completed\", extra={\n        'request_id': request.request_id,\n        'method': request.method,\n        'url': request.url,\n        'status_code': response.status_code,\n        'duration': round(duration * 1000, 2),  # milliseconds\n        'user_agent': request.headers.get('User-Agent'),\n        'remote_addr': request.remote_addr\n    })\n    \n    return response\n\n@app.route('/')\ndef home():\n    logger.info(\"Home page accessed\", extra={\n        'request_id': request.request_id,\n        'endpoint': '/'\n    })\n    \n    return jsonify({\n        'message': 'Hello from Python Flask App!',\n        'timestamp': datetime.utcnow().isoformat(),\n        'request_id': request.request_id\n    })\n\n@app.route('/api/users/<int:user_id>')\ndef get_user(user_id):\n    logger.info(\"User data requested\", extra={\n        'request_id': request.request_id,\n        'user_id': user_id,\n        'endpoint': '/api/users'\n    })\n    \n    # Simulate database query\n    time.sleep(random.uniform(0.1, 0.5))\n    \n    if user_id == 404:\n        logger.warning(\"User not found\", extra={\n            'request_id': request.request_id,\n            'user_id': user_id\n        })\n        return jsonify({'error': 'User not found'}), 404\n    \n    user_data = {\n        'id': user_id,\n        'name': f'User {user_id}',\n        'email': f'user{user_id}@example.com'\n    }\n    \n    logger.info(\"User data retrieved successfully\", extra={\n        'request_id': request.request_id,\n        'user_id': user_id\n    })\n    \n    return jsonify(user_data)\n\n@app.route('/api/error')\ndef trigger_error():\n    logger.error(\"Intentional error triggered\", extra={\n        'request_id': request.request_id,\n        'endpoint': '/api/error'\n    })\n    \n    raise Exception(\"This is an intentional error for testing\")\n\n@app.route('/api/slow')\ndef slow_endpoint():\n    delay = random.uniform(2, 5)\n    \n    logger.warning(\"Slow endpoint accessed\", extra={\n        'request_id': request.request_id,\n        'delay': delay,\n        'endpoint': '/api/slow'\n    })\n    \n    time.sleep(delay)\n    \n    return jsonify({\n        'message': 'Slow response completed',\n        'delay': delay\n    })\n\n@app.errorhandler(Exception)\ndef handle_exception(e):\n    logger.error(\"Unhandled exception\", extra={\n        'request_id': getattr(request, 'request_id', 'unknown'),\n        'error': str(e),\n        'error_type': type(e).__name__\n    })\n    \n    return jsonify({'error': 'Internal server error'}), 500\n\nif __name__ == '__main__':\n    logger.info(\"Application starting\", extra={\n        'event': 'startup',\n        'port': 5000\n    })\n    \n    app.run(host='0.0.0.0', port=5000, debug=False)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"log-generator-script",children:"Log Generator Script"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create scripts/log-generator.py:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport json\nimport time\nimport random\nfrom datetime import datetime\nimport logging\n\n# Set up logging\nlogging.basicConfig(\n    filename='/var/log/app/generated.log',\n    level=logging.INFO,\n    format='%(message)s'\n)\n\nlogger = logging.getLogger(__name__)\n\ndef generate_log_entry(level, message, **kwargs):\n    \"\"\"Generate a structured log entry\"\"\"\n    log_entry = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'level': level,\n        'message': message,\n        'service': 'log-generator',\n        **kwargs\n    }\n    \n    logger.info(json.dumps(log_entry))\n\ndef simulate_user_activity():\n    \"\"\"Simulate user activity logs\"\"\"\n    users = ['alice', 'bob', 'charlie', 'diana', 'eve']\n    actions = ['login', 'logout', 'view_page', 'purchase', 'search']\n    \n    user = random.choice(users)\n    action = random.choice(actions)\n    \n    generate_log_entry(\n        'INFO',\n        f'User {action} event',\n        user_id=user,\n        action=action,\n        session_id=f'sess-{random.randint(1000, 9999)}',\n        ip_address=f'192.168.1.{random.randint(1, 254)}'\n    )\n\ndef simulate_system_events():\n    \"\"\"Simulate system events\"\"\"\n    events = [\n        ('INFO', 'System health check passed'),\n        ('WARNING', 'High memory usage detected'),\n        ('ERROR', 'Database connection failed'),\n        ('INFO', 'Backup completed successfully'),\n        ('WARNING', 'Disk space running low')\n    ]\n    \n    level, message = random.choice(events)\n    \n    generate_log_entry(\n        level,\n        message,\n        component='system',\n        cpu_usage=random.uniform(10, 90),\n        memory_usage=random.uniform(20, 95),\n        disk_usage=random.uniform(30, 85)\n    )\n\ndef simulate_api_requests():\n    \"\"\"Simulate API request logs\"\"\"\n    endpoints = ['/api/users', '/api/orders', '/api/products', '/api/health']\n    methods = ['GET', 'POST', 'PUT', 'DELETE']\n    status_codes = [200, 201, 400, 404, 500]\n    \n    endpoint = random.choice(endpoints)\n    method = random.choice(methods)\n    status = random.choice(status_codes)\n    duration = random.uniform(10, 2000)  # milliseconds\n    \n    level = 'ERROR' if status >= 500 else 'WARNING' if status >= 400 else 'INFO'\n    \n    generate_log_entry(\n        level,\n        f'{method} {endpoint}',\n        method=method,\n        endpoint=endpoint,\n        status_code=status,\n        duration=round(duration, 2),\n        request_id=f'req-{random.randint(100000, 999999)}'\n    )\n\ndef main():\n    \"\"\"Main log generation loop\"\"\"\n    print(\"Starting log generator...\")\n    \n    while True:\n        # Generate different types of logs\n        log_type = random.choice(['user', 'system', 'api'])\n        \n        if log_type == 'user':\n            simulate_user_activity()\n        elif log_type == 'system':\n            simulate_system_events()\n        else:\n            simulate_api_requests()\n        \n        # Random delay between log entries\n        time.sleep(random.uniform(0.5, 3.0))\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"kibana-dashboards-and-visualizations",children:"Kibana Dashboards and Visualizations"}),"\n",(0,t.jsx)(n.h3,{id:"index-patterns-and-field-mappings",children:"Index Patterns and Field Mappings"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Create kibana-setup.py:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport requests\nimport json\nimport time\n\nKIBANA_URL = "http://localhost:5601"\nELASTICSEARCH_URL = "http://localhost:9200"\n\ndef wait_for_kibana():\n    """Wait for Kibana to be ready"""\n    while True:\n        try:\n            response = requests.get(f"{KIBANA_URL}/api/status")\n            if response.status_code == 200:\n                print("Kibana is ready!")\n                break\n        except requests.exceptions.ConnectionError:\n            print("Waiting for Kibana...")\n            time.sleep(5)\n\ndef create_index_pattern():\n    """Create index pattern in Kibana"""\n    index_pattern = {\n        "attributes": {\n            "title": "logs-*",\n            "timeFieldName": "@timestamp"\n        }\n    }\n    \n    headers = {\n        "Content-Type": "application/json",\n        "kbn-xsrf": "true"\n    }\n    \n    response = requests.post(\n        f"{KIBANA_URL}/api/saved_objects/index-pattern/logs-pattern",\n        headers=headers,\n        json=index_pattern\n    )\n    \n    if response.status_code in [200, 409]:  # 409 means already exists\n        print("Index pattern created successfully!")\n    else:\n        print(f"Failed to create index pattern: {response.text}")\n\ndef create_dashboard():\n    """Create sample dashboard"""\n    dashboard = {\n        "attributes": {\n            "title": "Application Logs Dashboard",\n            "type": "dashboard",\n            "description": "Dashboard for monitoring application logs",\n            "panelsJSON": json.dumps([\n                {\n                    "version": "8.11.0",\n                    "type": "visualization",\n                    "gridData": {\n                        "x": 0, "y": 0, "w": 24, "h": 15\n                    },\n                    "panelIndex": "1",\n                    "embeddableConfig": {},\n                    "panelRefName": "panel_1"\n                }\n            ])\n        }\n    }\n    \n    headers = {\n        "Content-Type": "application/json",\n        "kbn-xsrf": "true"\n    }\n    \n    response = requests.post(\n        f"{KIBANA_URL}/api/saved_objects/dashboard/app-logs-dashboard",\n        headers=headers,\n        json=dashboard\n    )\n    \n    if response.status_code in [200, 409]:\n        print("Dashboard created successfully!")\n    else:\n        print(f"Failed to create dashboard: {response.text}")\n\nif __name__ == "__main__":\n    wait_for_kibana()\n    time.sleep(10)  # Additional wait for full initialization\n    create_index_pattern()\n    create_dashboard()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"start-elk-stack",children:"Start ELK Stack"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Start the complete stack:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"docker compose up -d\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Run log generator:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"docker exec -d sample-nginx python3 /scripts/log-generator.py\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Access services:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'echo "Elasticsearch: http://localhost:9200"\necho "Kibana: http://localhost:5601"\necho "Logstash: http://localhost:9600"\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Centralized Logging"}),": Collect logs from multiple applications and services"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Log Analysis"}),": Search, filter, and analyze log data for troubleshooting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Monitoring"}),": Monitor application and system metrics in real-time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security Analytics"}),": Detect security threats and anomalies in log data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Business Intelligence"}),": Extract business insights from application logs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compliance"}),": Maintain audit trails and compliance reporting"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"\u2705 ELK Stack is now configured for comprehensive log management and analysis!"})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var a=s(6540);const t={},r=a.createContext(t);function i(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);