"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5256],{3390:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>d,contentTitle:()=>i,default:()=>l,frontMatter:()=>o,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"Data-Processing/apache-spark","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. Learn how to set up Spark with Docker for big data processing and analytics.","source":"@site/docs/Data-Processing/apache-spark.md","sourceDirName":"Data-Processing","slug":"/Data-Processing/ApacheSpark","permalink":"/docs/Data-Processing/ApacheSpark","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Data-Processing/apache-spark.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. Learn how to set up Spark with Docker for big data processing and analytics.","slug":"/Data-Processing/ApacheSpark","keywords":["Apache Spark","big data processing","distributed computing","data analytics","PySpark","Spark SQL","machine learning","stream processing","data engineering","cluster computing"]},"sidebar":"tutorialSidebar","previous":{"title":"Apache Airflow","permalink":"/docs/Data-Processing/ApacheAirflow"},"next":{"title":"Security","permalink":"/docs/category/security"}}');var s=a(4848),t=a(8453);const o={sidebar_position:3,title:"Apache Spark",description:"Apache Spark is a unified analytics engine for large-scale data processing. Learn how to set up Spark with Docker for big data processing and analytics.",slug:"/Data-Processing/ApacheSpark",keywords:["Apache Spark","big data processing","distributed computing","data analytics","PySpark","Spark SQL","machine learning","stream processing","data engineering","cluster computing"]},i="\u26a1 Apache Spark - Unified Analytics Engine for Big Data",d={},p=[{value:"Set Up Spark with Docker",id:"set-up-spark-with-docker",level:2},{value:"Basic PySpark Applications",id:"basic-pyspark-applications",level:2},{value:"Data Processing Example",id:"data-processing-example",level:3},{value:"Machine Learning with Spark MLlib",id:"machine-learning-with-spark-mllib",level:3},{value:"Spark SQL and DataFrames",id:"spark-sql-and-dataframes",level:2},{value:"Advanced SQL Operations",id:"advanced-sql-operations",level:3},{value:"Running Spark Applications",id:"running-spark-applications",level:2},{value:"Submit Spark Jobs",id:"submit-spark-jobs",level:3},{value:"Interactive Spark Shell",id:"interactive-spark-shell",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"-apache-spark---unified-analytics-engine-for-big-data",children:"\u26a1 Apache Spark - Unified Analytics Engine for Big Data"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Apache Spark"})," is a ",(0,s.jsx)(e.strong,{children:"unified analytics engine"})," for large-scale data processing. It provides high-level APIs in ",(0,s.jsx)(e.strong,{children:"Java"}),", ",(0,s.jsx)(e.strong,{children:"Scala"}),", ",(0,s.jsx)(e.strong,{children:"Python"}),", and ",(0,s.jsx)(e.strong,{children:"R"}),", and supports ",(0,s.jsx)(e.strong,{children:"SQL"}),", ",(0,s.jsx)(e.strong,{children:"streaming"}),", ",(0,s.jsx)(e.strong,{children:"machine learning"}),", and ",(0,s.jsx)(e.strong,{children:"graph processing"}),"."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"set-up-spark-with-docker",children:"Set Up Spark with Docker"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Create a file named docker-compose.yml"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  # Spark Master\n  spark-master:\n    image: bitnami/spark:3.5\n    container_name: spark-master\n    restart: unless-stopped\n    environment:\n      - SPARK_MODE=master\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    ports:\n      - "8080:8080"  # Spark Master Web UI\n      - "7077:7077"  # Spark Master Port\n    volumes:\n      - ./spark-apps:/opt/bitnami/spark/apps\n      - ./spark-data:/opt/bitnami/spark/data\n      - ./spark-logs:/opt/bitnami/spark/logs\n\n  # Spark Worker 1\n  spark-worker-1:\n    image: bitnami/spark:3.5\n    container_name: spark-worker-1\n    restart: unless-stopped\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_MEMORY=2g\n      - SPARK_WORKER_CORES=2\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    ports:\n      - "8081:8081"  # Spark Worker Web UI\n    volumes:\n      - ./spark-apps:/opt/bitnami/spark/apps\n      - ./spark-data:/opt/bitnami/spark/data\n      - ./spark-logs:/opt/bitnami/spark/logs\n    depends_on:\n      - spark-master\n\n  # Spark Worker 2\n  spark-worker-2:\n    image: bitnami/spark:3.5\n    container_name: spark-worker-2\n    restart: unless-stopped\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_MEMORY=2g\n      - SPARK_WORKER_CORES=2\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n      - SPARK_USER=spark\n    ports:\n      - "8082:8082"  # Spark Worker Web UI\n    volumes:\n      - ./spark-apps:/opt/bitnami/spark/apps\n      - ./spark-data:/opt/bitnami/spark/data\n      - ./spark-logs:/opt/bitnami/spark/logs\n    depends_on:\n      - spark-master\n\n  # Jupyter Notebook with PySpark\n  jupyter:\n    image: jupyter/pyspark-notebook:latest\n    container_name: spark-jupyter\n    restart: unless-stopped\n    ports:\n      - "8888:8888"\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - SPARK_MASTER=spark://spark-master:7077\n    volumes:\n      - ./notebooks:/home/jovyan/work\n      - ./spark-data:/home/jovyan/data\n    depends_on:\n      - spark-master\n\n  # PostgreSQL for data storage\n  postgres:\n    image: postgres:15\n    container_name: spark-postgres\n    restart: unless-stopped\n    environment:\n      - POSTGRES_DB=sparkdb\n      - POSTGRES_USER=spark\n      - POSTGRES_PASSWORD=spark123\n    ports:\n      - "5432:5432"\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n\n  # MinIO for object storage\n  minio:\n    image: minio/minio:latest\n    container_name: spark-minio\n    restart: unless-stopped\n    ports:\n      - "9000:9000"\n      - "9001:9001"\n    environment:\n      - MINIO_ROOT_USER=minioadmin\n      - MINIO_ROOT_PASSWORD=minioadmin\n    volumes:\n      - minio-data:/data\n    command: server /data --console-address ":9001"\n\nvolumes:\n  postgres-data:\n  minio-data:\n'})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Create necessary directories:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"mkdir -p spark-apps spark-data spark-logs notebooks\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Start Spark cluster:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"docker compose up -d\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Access Spark UIs:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'echo "Spark Master UI: http://localhost:8080"\necho "Spark Worker 1 UI: http://localhost:8081"\necho "Spark Worker 2 UI: http://localhost:8082"\necho "Jupyter Notebook: http://localhost:8888"\necho "MinIO Console: http://localhost:9001"\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"basic-pyspark-applications",children:"Basic PySpark Applications"}),"\n",(0,s.jsx)(e.h3,{id:"data-processing-example",children:"Data Processing Example"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Create spark-apps/data_processing.py:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport sys\n\ndef create_spark_session(app_name="DataProcessing"):\n    """Create Spark session"""\n    return SparkSession.builder \\\n        .appName(app_name) \\\n        .config("spark.sql.adaptive.enabled", "true") \\\n        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\\n        .getOrCreate()\n\ndef process_sales_data(spark):\n    """Process sales data example"""\n    \n    # Create sample sales data\n    sales_data = [\n        (1, "2024-01-01", "Electronics", "Laptop", 999.99, 2),\n        (2, "2024-01-01", "Electronics", "Mouse", 29.99, 5),\n        (3, "2024-01-02", "Books", "Python Guide", 49.99, 3),\n        (4, "2024-01-02", "Electronics", "Keyboard", 79.99, 2),\n        (5, "2024-01-03", "Books", "Data Science", 59.99, 1),\n        (6, "2024-01-03", "Electronics", "Monitor", 299.99, 1),\n        (7, "2024-01-04", "Electronics", "Laptop", 999.99, 1),\n        (8, "2024-01-04", "Books", "Machine Learning", 69.99, 2)\n    ]\n    \n    schema = StructType([\n        StructField("order_id", IntegerType(), True),\n        StructField("order_date", StringType(), True),\n        StructField("category", StringType(), True),\n        StructField("product", StringType(), True),\n        StructField("price", DoubleType(), True),\n        StructField("quantity", IntegerType(), True)\n    ])\n    \n    df = spark.createDataFrame(sales_data, schema)\n    \n    # Convert string date to date type\n    df = df.withColumn("order_date", to_date(col("order_date"), "yyyy-MM-dd"))\n    \n    # Calculate total amount\n    df = df.withColumn("total_amount", col("price") * col("quantity"))\n    \n    print("=== Original Data ===")\n    df.show()\n    \n    # Aggregations\n    print("=== Sales by Category ===")\n    category_sales = df.groupBy("category") \\\n        .agg(\n            sum("total_amount").alias("total_sales"),\n            count("order_id").alias("order_count"),\n            avg("total_amount").alias("avg_order_value")\n        ) \\\n        .orderBy(desc("total_sales"))\n    \n    category_sales.show()\n    \n    # Daily sales trend\n    print("=== Daily Sales Trend ===")\n    daily_sales = df.groupBy("order_date") \\\n        .agg(\n            sum("total_amount").alias("daily_total"),\n            count("order_id").alias("daily_orders")\n        ) \\\n        .orderBy("order_date")\n    \n    daily_sales.show()\n    \n    # Top products\n    print("=== Top Products by Revenue ===")\n    product_revenue = df.groupBy("product") \\\n        .agg(\n            sum("total_amount").alias("product_revenue"),\n            sum("quantity").alias("total_quantity")\n        ) \\\n        .orderBy(desc("product_revenue"))\n    \n    product_revenue.show()\n    \n    # Save results\n    category_sales.write \\\n        .mode("overwrite") \\\n        .option("header", "true") \\\n        .csv("/opt/bitnami/spark/data/category_sales")\n    \n    print("Results saved to /opt/bitnami/spark/data/category_sales")\n    \n    return df\n\ndef process_streaming_data(spark):\n    """Example of structured streaming (simulated)"""\n    \n    # Create a streaming DataFrame (simulated with rate source)\n    streaming_df = spark \\\n        .readStream \\\n        .format("rate") \\\n        .option("rowsPerSecond", 10) \\\n        .load()\n    \n    # Process the streaming data\n    processed_df = streaming_df \\\n        .withColumn("event_time", current_timestamp()) \\\n        .withColumn("random_value", rand() * 100) \\\n        .withColumn("category", \n                   when(col("random_value") < 33, "A")\n                   .when(col("random_value") < 66, "B")\n                   .otherwise("C"))\n    \n    # Write to console (for demonstration)\n    query = processed_df.writeStream \\\n        .outputMode("append") \\\n        .format("console") \\\n        .option("truncate", "false") \\\n        .trigger(processingTime=\'10 seconds\') \\\n        .start()\n    \n    print("Streaming query started. Press Ctrl+C to stop.")\n    \n    # In a real application, you might write to Kafka, database, etc.\n    # query.awaitTermination()\n    \n    return query\n\ndef main():\n    spark = create_spark_session("SalesDataProcessing")\n    \n    try:\n        # Process batch data\n        df = process_sales_data(spark)\n        \n        # Uncomment to run streaming example\n        # streaming_query = process_streaming_data(spark)\n        \n        print("Data processing completed successfully!")\n        \n    except Exception as e:\n        print(f"Error during processing: {e}")\n        sys.exit(1)\n    \n    finally:\n        spark.stop()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"machine-learning-with-spark-mllib",children:"Machine Learning with Spark MLlib"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Create spark-apps/ml_pipeline.py:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\nfrom pyspark.ml.classification import RandomForestClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport sys\n\ndef create_spark_session():\n    """Create Spark session with ML libraries"""\n    return SparkSession.builder \\\n        .appName("MLPipeline") \\\n        .config("spark.sql.adaptive.enabled", "true") \\\n        .getOrCreate()\n\ndef create_sample_data(spark):\n    """Create sample dataset for classification"""\n    \n    # Generate sample customer data\n    data = []\n    import random\n    \n    for i in range(1000):\n        age = random.randint(18, 80)\n        income = random.randint(20000, 150000)\n        spending_score = random.randint(1, 100)\n        \n        # Create target variable based on some logic\n        if age < 30 and income > 50000 and spending_score > 60:\n            target = "High Value"\n        elif age > 50 and income > 80000:\n            target = "High Value"\n        elif spending_score < 30:\n            target = "Low Value"\n        else:\n            target = "Medium Value"\n        \n        data.append((i, age, income, spending_score, target))\n    \n    schema = StructType([\n        StructField("customer_id", IntegerType(), True),\n        StructField("age", IntegerType(), True),\n        StructField("income", IntegerType(), True),\n        StructField("spending_score", IntegerType(), True),\n        StructField("customer_segment", StringType(), True)\n    ])\n    \n    return spark.createDataFrame(data, schema)\n\ndef build_ml_pipeline():\n    """Build machine learning pipeline"""\n    \n    # Feature engineering\n    feature_cols = ["age", "income", "spending_score"]\n    vector_assembler = VectorAssembler(\n        inputCols=feature_cols,\n        outputCol="features_raw"\n    )\n    \n    # Feature scaling\n    scaler = StandardScaler(\n        inputCol="features_raw",\n        outputCol="features",\n        withStd=True,\n        withMean=True\n    )\n    \n    # Target encoding\n    label_indexer = StringIndexer(\n        inputCol="customer_segment",\n        outputCol="label"\n    )\n    \n    # Classifier\n    rf = RandomForestClassifier(\n        featuresCol="features",\n        labelCol="label",\n        numTrees=100,\n        maxDepth=10,\n        seed=42\n    )\n    \n    # Create pipeline\n    pipeline = Pipeline(stages=[\n        vector_assembler,\n        scaler,\n        label_indexer,\n        rf\n    ])\n    \n    return pipeline\n\ndef train_and_evaluate_model(spark, df):\n    """Train and evaluate the ML model"""\n    \n    print("=== Dataset Overview ===")\n    df.show(10)\n    df.describe().show()\n    \n    print("=== Target Distribution ===")\n    df.groupBy("customer_segment").count().show()\n    \n    # Split data\n    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n    \n    print(f"Training samples: {train_df.count()}")\n    print(f"Test samples: {test_df.count()}")\n    \n    # Build and train pipeline\n    pipeline = build_ml_pipeline()\n    model = pipeline.fit(train_df)\n    \n    # Make predictions\n    predictions = model.transform(test_df)\n    \n    print("=== Sample Predictions ===")\n    predictions.select(\n        "customer_id", "age", "income", "spending_score",\n        "customer_segment", "prediction", "probability"\n    ).show(10, truncate=False)\n    \n    # Evaluate model\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol="label",\n        predictionCol="prediction",\n        metricName="accuracy"\n    )\n    \n    accuracy = evaluator.evaluate(predictions)\n    print(f"Model Accuracy: {accuracy:.4f}")\n    \n    # Additional metrics\n    evaluator_f1 = MulticlassClassificationEvaluator(\n        labelCol="label",\n        predictionCol="prediction",\n        metricName="f1"\n    )\n    \n    f1_score = evaluator_f1.evaluate(predictions)\n    print(f"F1 Score: {f1_score:.4f}")\n    \n    # Feature importance (for Random Forest)\n    rf_model = model.stages[-1]  # Last stage is the RF classifier\n    feature_importance = rf_model.featureImportances.toArray()\n    feature_names = ["age", "income", "spending_score"]\n    \n    print("=== Feature Importance ===")\n    for name, importance in zip(feature_names, feature_importance):\n        print(f"{name}: {importance:.4f}")\n    \n    return model, predictions\n\ndef hyperparameter_tuning(spark, df):\n    """Perform hyperparameter tuning with cross-validation"""\n    \n    print("=== Hyperparameter Tuning ===")\n    \n    # Split data\n    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n    \n    # Build base pipeline (without the final classifier)\n    feature_cols = ["age", "income", "spending_score"]\n    vector_assembler = VectorAssembler(\n        inputCols=feature_cols,\n        outputCol="features_raw"\n    )\n    \n    scaler = StandardScaler(\n        inputCol="features_raw",\n        outputCol="features",\n        withStd=True,\n        withMean=True\n    )\n    \n    label_indexer = StringIndexer(\n        inputCol="customer_segment",\n        outputCol="label"\n    )\n    \n    # Classifier for tuning\n    lr = LogisticRegression(\n        featuresCol="features",\n        labelCol="label"\n    )\n    \n    # Pipeline\n    pipeline = Pipeline(stages=[\n        vector_assembler,\n        scaler,\n        label_indexer,\n        lr\n    ])\n    \n    # Parameter grid\n    param_grid = ParamGridBuilder() \\\n        .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n        .build()\n    \n    # Cross validator\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol="label",\n        predictionCol="prediction",\n        metricName="accuracy"\n    )\n    \n    cv = CrossValidator(\n        estimator=pipeline,\n        estimatorParamMaps=param_grid,\n        evaluator=evaluator,\n        numFolds=3,\n        seed=42\n    )\n    \n    # Train with cross-validation\n    cv_model = cv.fit(train_df)\n    \n    # Best model predictions\n    best_predictions = cv_model.transform(test_df)\n    best_accuracy = evaluator.evaluate(best_predictions)\n    \n    print(f"Best Model Accuracy: {best_accuracy:.4f}")\n    \n    # Best parameters\n    best_model = cv_model.bestModel\n    best_lr = best_model.stages[-1]\n    print(f"Best RegParam: {best_lr.getRegParam()}")\n    print(f"Best ElasticNetParam: {best_lr.getElasticNetParam()}")\n    \n    return cv_model\n\ndef save_model_and_predictions(model, predictions, spark):\n    """Save model and predictions"""\n    \n    # Save model\n    model.write().overwrite().save("/opt/bitnami/spark/data/ml_model")\n    print("Model saved to /opt/bitnami/spark/data/ml_model")\n    \n    # Save predictions\n    predictions.select(\n        "customer_id", "age", "income", "spending_score",\n        "customer_segment", "prediction"\n    ).write \\\n        .mode("overwrite") \\\n        .option("header", "true") \\\n        .csv("/opt/bitnami/spark/data/predictions")\n    \n    print("Predictions saved to /opt/bitnami/spark/data/predictions")\n\ndef main():\n    spark = create_spark_session()\n    \n    try:\n        # Create sample data\n        df = create_sample_data(spark)\n        \n        # Train and evaluate model\n        model, predictions = train_and_evaluate_model(spark, df)\n        \n        # Hyperparameter tuning example\n        cv_model = hyperparameter_tuning(spark, df)\n        \n        # Save results\n        save_model_and_predictions(model, predictions, spark)\n        \n        print("ML pipeline completed successfully!")\n        \n    except Exception as e:\n        print(f"Error during ML pipeline: {e}")\n        sys.exit(1)\n    \n    finally:\n        spark.stop()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"spark-sql-and-dataframes",children:"Spark SQL and DataFrames"}),"\n",(0,s.jsx)(e.h3,{id:"advanced-sql-operations",children:"Advanced SQL Operations"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Create spark-apps/spark_sql_demo.py:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport sys\n\ndef create_spark_session():\n    """Create Spark session"""\n    return SparkSession.builder \\\n        .appName("SparkSQLDemo") \\\n        .config("spark.sql.adaptive.enabled", "true") \\\n        .getOrCreate()\n\ndef create_sample_datasets(spark):\n    """Create sample datasets for demonstration"""\n    \n    # Orders dataset\n    orders_data = [\n        (1, 101, "2024-01-01", 1500.00, "completed"),\n        (2, 102, "2024-01-01", 750.00, "completed"),\n        (3, 103, "2024-01-02", 2200.00, "pending"),\n        (4, 101, "2024-01-02", 890.00, "completed"),\n        (5, 104, "2024-01-03", 1200.00, "cancelled"),\n        (6, 102, "2024-01-03", 650.00, "completed"),\n        (7, 105, "2024-01-04", 3200.00, "completed"),\n        (8, 103, "2024-01-04", 1800.00, "completed")\n    ]\n    \n    orders_schema = StructType([\n        StructField("order_id", IntegerType(), True),\n        StructField("customer_id", IntegerType(), True),\n        StructField("order_date", StringType(), True),\n        StructField("amount", DoubleType(), True),\n        StructField("status", StringType(), True)\n    ])\n    \n    orders_df = spark.createDataFrame(orders_data, orders_schema)\n    orders_df = orders_df.withColumn("order_date", to_date(col("order_date"), "yyyy-MM-dd"))\n    \n    # Customers dataset\n    customers_data = [\n        (101, "Alice Johnson", "alice@email.com", "Premium"),\n        (102, "Bob Smith", "bob@email.com", "Standard"),\n        (103, "Charlie Brown", "charlie@email.com", "Premium"),\n        (104, "Diana Prince", "diana@email.com", "Standard"),\n        (105, "Eve Wilson", "eve@email.com", "Premium")\n    ]\n    \n    customers_schema = StructType([\n        StructField("customer_id", IntegerType(), True),\n        StructField("name", StringType(), True),\n        StructField("email", StringType(), True),\n        StructField("tier", StringType(), True)\n    ])\n    \n    customers_df = spark.createDataFrame(customers_data, customers_schema)\n    \n    # Register as temporary views for SQL queries\n    orders_df.createOrReplaceTempView("orders")\n    customers_df.createOrReplaceTempView("customers")\n    \n    return orders_df, customers_df\n\ndef demonstrate_basic_operations(spark, orders_df, customers_df):\n    """Demonstrate basic DataFrame operations"""\n    \n    print("=== Basic DataFrame Operations ===")\n    \n    # Show data\n    print("Orders:")\n    orders_df.show()\n    \n    print("Customers:")\n    customers_df.show()\n    \n    # Basic aggregations\n    print("=== Order Statistics ===")\n    orders_df.agg(\n        count("order_id").alias("total_orders"),\n        sum("amount").alias("total_revenue"),\n        avg("amount").alias("avg_order_value"),\n        max("amount").alias("max_order"),\n        min("amount").alias("min_order")\n    ).show()\n    \n    # Filtering and grouping\n    print("=== Orders by Status ===")\n    orders_df.groupBy("status") \\\n        .agg(\n            count("order_id").alias("order_count"),\n            sum("amount").alias("total_amount")\n        ) \\\n        .orderBy(desc("total_amount")) \\\n        .show()\n\ndef demonstrate_joins(spark):\n    """Demonstrate different types of joins"""\n    \n    print("=== Join Operations ===")\n    \n    # Inner join\n    print("Customer Orders (Inner Join):")\n    result = spark.sql("""\n        SELECT c.name, c.tier, o.order_date, o.amount, o.status\n        FROM customers c\n        INNER JOIN orders o ON c.customer_id = o.customer_id\n        ORDER BY o.order_date, c.name\n    """)\n    result.show()\n    \n    # Customer summary with aggregation\n    print("Customer Order Summary:")\n    customer_summary = spark.sql("""\n        SELECT \n            c.name,\n            c.tier,\n            COUNT(o.order_id) as total_orders,\n            COALESCE(SUM(o.amount), 0) as total_spent,\n            COALESCE(AVG(o.amount), 0) as avg_order_value\n        FROM customers c\n        LEFT JOIN orders o ON c.customer_id = o.customer_id \n            AND o.status = \'completed\'\n        GROUP BY c.customer_id, c.name, c.tier\n        ORDER BY total_spent DESC\n    """)\n    customer_summary.show()\n\ndef demonstrate_window_functions(spark):\n    """Demonstrate window functions"""\n    \n    print("=== Window Functions ===")\n    \n    # Running totals and rankings\n    result = spark.sql("""\n        SELECT \n            customer_id,\n            order_date,\n            amount,\n            SUM(amount) OVER (\n                PARTITION BY customer_id \n                ORDER BY order_date \n                ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n            ) as running_total,\n            ROW_NUMBER() OVER (\n                PARTITION BY customer_id \n                ORDER BY order_date\n            ) as order_sequence,\n            RANK() OVER (\n                ORDER BY amount DESC\n            ) as amount_rank\n        FROM orders\n        WHERE status = \'completed\'\n        ORDER BY customer_id, order_date\n    """)\n    result.show()\n    \n    # Moving averages\n    print("Moving Averages:")\n    moving_avg = spark.sql("""\n        SELECT \n            order_date,\n            amount,\n            AVG(amount) OVER (\n                ORDER BY order_date \n                ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n            ) as moving_avg_3day\n        FROM orders\n        WHERE status = \'completed\'\n        ORDER BY order_date\n    """)\n    moving_avg.show()\n\ndef demonstrate_advanced_analytics(spark):\n    """Demonstrate advanced analytics functions"""\n    \n    print("=== Advanced Analytics ===")\n    \n    # Percentiles and statistical functions\n    result = spark.sql("""\n        SELECT \n            tier,\n            COUNT(*) as customer_count,\n            AVG(total_spent) as avg_spent,\n            PERCENTILE_APPROX(total_spent, 0.5) as median_spent,\n            PERCENTILE_APPROX(total_spent, 0.9) as p90_spent,\n            STDDEV(total_spent) as stddev_spent\n        FROM (\n            SELECT \n                c.tier,\n                COALESCE(SUM(o.amount), 0) as total_spent\n            FROM customers c\n            LEFT JOIN orders o ON c.customer_id = o.customer_id \n                AND o.status = \'completed\'\n            GROUP BY c.customer_id, c.tier\n        ) customer_spending\n        GROUP BY tier\n    """)\n    result.show()\n    \n    # Cohort analysis (simplified)\n    print("Customer Cohort Analysis:")\n    cohort_analysis = spark.sql("""\n        WITH first_orders AS (\n            SELECT \n                customer_id,\n                MIN(order_date) as first_order_date\n            FROM orders\n            WHERE status = \'completed\'\n            GROUP BY customer_id\n        ),\n        customer_months AS (\n            SELECT \n                o.customer_id,\n                f.first_order_date,\n                o.order_date,\n                DATEDIFF(o.order_date, f.first_order_date) as days_since_first\n            FROM orders o\n            JOIN first_orders f ON o.customer_id = f.customer_id\n            WHERE o.status = \'completed\'\n        )\n        SELECT \n            first_order_date,\n            CASE \n                WHEN days_since_first = 0 THEN \'Month 0\'\n                WHEN days_since_first <= 30 THEN \'Month 1\'\n                WHEN days_since_first <= 60 THEN \'Month 2\'\n                ELSE \'Month 3+\'\n            END as cohort_month,\n            COUNT(DISTINCT customer_id) as customers\n        FROM customer_months\n        GROUP BY first_order_date, \n                 CASE \n                     WHEN days_since_first = 0 THEN \'Month 0\'\n                     WHEN days_since_first <= 30 THEN \'Month 1\'\n                     WHEN days_since_first <= 60 THEN \'Month 2\'\n                     ELSE \'Month 3+\'\n                 END\n        ORDER BY first_order_date, cohort_month\n    """)\n    cohort_analysis.show()\n\ndef demonstrate_data_quality_checks(spark):\n    """Demonstrate data quality checks"""\n    \n    print("=== Data Quality Checks ===")\n    \n    # Null checks\n    null_checks = spark.sql("""\n        SELECT \n            \'orders\' as table_name,\n            SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) as null_order_id,\n            SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customer_id,\n            SUM(CASE WHEN amount IS NULL THEN 1 ELSE 0 END) as null_amount,\n            SUM(CASE WHEN amount <= 0 THEN 1 ELSE 0 END) as invalid_amount\n        FROM orders\n        \n        UNION ALL\n        \n        SELECT \n            \'customers\' as table_name,\n            SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customer_id,\n            SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END) as null_name,\n            SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_email,\n            SUM(CASE WHEN email NOT LIKE \'%@%\' THEN 1 ELSE 0 END) as invalid_email\n        FROM customers\n    """)\n    null_checks.show()\n    \n    # Duplicate checks\n    print("Duplicate Checks:")\n    duplicate_checks = spark.sql("""\n        SELECT \n            \'orders\' as table_name,\n            COUNT(*) as total_records,\n            COUNT(DISTINCT order_id) as unique_order_ids,\n            COUNT(*) - COUNT(DISTINCT order_id) as duplicate_order_ids\n        FROM orders\n        \n        UNION ALL\n        \n        SELECT \n            \'customers\' as table_name,\n            COUNT(*) as total_records,\n            COUNT(DISTINCT customer_id) as unique_customer_ids,\n            COUNT(*) - COUNT(DISTINCT customer_id) as duplicate_customer_ids\n        FROM customers\n    """)\n    duplicate_checks.show()\n\ndef save_results(spark):\n    """Save analysis results"""\n    \n    # Customer summary for reporting\n    customer_report = spark.sql("""\n        SELECT \n            c.customer_id,\n            c.name,\n            c.email,\n            c.tier,\n            COUNT(o.order_id) as total_orders,\n            COALESCE(SUM(CASE WHEN o.status = \'completed\' THEN o.amount END), 0) as total_revenue,\n            COALESCE(AVG(CASE WHEN o.status = \'completed\' THEN o.amount END), 0) as avg_order_value,\n            MAX(o.order_date) as last_order_date\n        FROM customers c\n        LEFT JOIN orders o ON c.customer_id = o.customer_id\n        GROUP BY c.customer_id, c.name, c.email, c.tier\n        ORDER BY total_revenue DESC\n    """)\n    \n    # Save to CSV\n    customer_report.write \\\n        .mode("overwrite") \\\n        .option("header", "true") \\\n        .csv("/opt/bitnami/spark/data/customer_report")\n    \n    print("Customer report saved to /opt/bitnami/spark/data/customer_report")\n\ndef main():\n    spark = create_spark_session()\n    \n    try:\n        # Create sample data\n        orders_df, customers_df = create_sample_datasets(spark)\n        \n        # Demonstrate various operations\n        demonstrate_basic_operations(spark, orders_df, customers_df)\n        demonstrate_joins(spark)\n        demonstrate_window_functions(spark)\n        demonstrate_advanced_analytics(spark)\n        demonstrate_data_quality_checks(spark)\n        \n        # Save results\n        save_results(spark)\n        \n        print("Spark SQL demonstration completed successfully!")\n        \n    except Exception as e:\n        print(f"Error during Spark SQL demo: {e}")\n        sys.exit(1)\n    \n    finally:\n        spark.stop()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"running-spark-applications",children:"Running Spark Applications"}),"\n",(0,s.jsx)(e.h3,{id:"submit-spark-jobs",children:"Submit Spark Jobs"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Run the applications:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Submit data processing job\ndocker exec spark-master spark-submit \\\n  --master spark://spark-master:7077 \\\n  --deploy-mode client \\\n  --executor-memory 1g \\\n  --executor-cores 1 \\\n  /opt/bitnami/spark/apps/data_processing.py\n\n# Submit ML pipeline\ndocker exec spark-master spark-submit \\\n  --master spark://spark-master:7077 \\\n  --deploy-mode client \\\n  --executor-memory 2g \\\n  --executor-cores 2 \\\n  --packages org.apache.spark:spark-mllib_2.12:3.5.0 \\\n  /opt/bitnami/spark/apps/ml_pipeline.py\n\n# Submit Spark SQL demo\ndocker exec spark-master spark-submit \\\n  --master spark://spark-master:7077 \\\n  --deploy-mode client \\\n  /opt/bitnami/spark/apps/spark_sql_demo.py\n"})}),"\n",(0,s.jsx)(e.h3,{id:"interactive-spark-shell",children:"Interactive Spark Shell"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Start PySpark shell:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"docker exec -it spark-master pyspark \\\n  --master spark://spark-master:7077 \\\n  --executor-memory 1g\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.code,{children:"Start Scala Spark shell:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"docker exec -it spark-master spark-shell \\\n  --master spark://spark-master:7077 \\\n  --executor-memory 1g\n"})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Big Data Processing"}),": Large-scale data transformation and analysis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ETL Pipelines"}),": Extract, transform, and load operations on massive datasets"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Machine Learning"}),": Distributed machine learning model training and inference"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Real-time Analytics"}),": Stream processing for real-time insights"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Data Lake Analytics"}),": Processing data stored in distributed file systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Graph Processing"}),": Large-scale graph analytics and algorithms"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"\u2705 Apache Spark is now configured for distributed big data processing!"})]})}function l(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>i});var r=a(6540);const s={},t=r.createContext(s);function o(n){const e=r.useContext(t);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);