"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6829],{6123:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Data-Processing/apache-airflow","title":"Apache Airflow","description":"Apache Airflow is an open-source platform for developing, scheduling, and monitoring workflows. Learn how to set up Airflow with Docker for data pipeline orchestration.","source":"@site/docs/Data-Processing/apache-airflow.md","sourceDirName":"Data-Processing","slug":"/Data-Processing/ApacheAirflow","permalink":"/docs/docs/Data-Processing/ApacheAirflow","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Data-Processing/apache-airflow.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Apache Airflow","description":"Apache Airflow is an open-source platform for developing, scheduling, and monitoring workflows. Learn how to set up Airflow with Docker for data pipeline orchestration.","slug":"/Data-Processing/ApacheAirflow","keywords":["Apache Airflow","workflow orchestration","data pipelines","ETL","task scheduling","DAG","data engineering","pipeline automation","workflow management","batch processing"]},"sidebar":"tutorialSidebar","previous":{"title":"Apache Kafka","permalink":"/docs/docs/Data-Processing/ApacheKafka"},"next":{"title":"Apache Spark","permalink":"/docs/docs/Data-Processing/ApacheSpark"}}');var r=a(4848),s=a(8453);const o={sidebar_position:2,title:"Apache Airflow",description:"Apache Airflow is an open-source platform for developing, scheduling, and monitoring workflows. Learn how to set up Airflow with Docker for data pipeline orchestration.",slug:"/Data-Processing/ApacheAirflow",keywords:["Apache Airflow","workflow orchestration","data pipelines","ETL","task scheduling","DAG","data engineering","pipeline automation","workflow management","batch processing"]},i="\ud83c\udf2c\ufe0f Apache Airflow - Workflow Orchestration Platform",l={},d=[{value:"Set Up Airflow with Docker",id:"set-up-airflow-with-docker",level:2},{value:"Basic DAG Examples",id:"basic-dag-examples",level:2},{value:"Simple ETL Pipeline",id:"simple-etl-pipeline",level:3},{value:"Data Processing with Docker",id:"data-processing-with-docker",level:3},{value:"Machine Learning Pipeline",id:"machine-learning-pipeline",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Dynamic DAGs",id:"dynamic-dags",level:3},{value:"Custom Operators",id:"custom-operators",level:3},{value:"Monitoring and Alerting",id:"monitoring-and-alerting",level:2},{value:"Custom Monitoring DAG",id:"custom-monitoring-dag",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"\ufe0f-apache-airflow---workflow-orchestration-platform",children:"\ud83c\udf2c\ufe0f Apache Airflow - Workflow Orchestration Platform"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Apache Airflow"})," is an open-source platform for ",(0,r.jsx)(n.strong,{children:"developing"}),", ",(0,r.jsx)(n.strong,{children:"scheduling"}),", and ",(0,r.jsx)(n.strong,{children:"monitoring workflows"}),". It allows you to programmatically author, schedule, and monitor ",(0,r.jsx)(n.strong,{children:"data pipelines"})," using ",(0,r.jsx)(n.strong,{children:"Directed Acyclic Graphs (DAGs)"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"set-up-airflow-with-docker",children:"Set Up Airflow with Docker"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create a file named docker-compose.yml"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  # PostgreSQL database for Airflow metadata\n  postgres:\n    image: postgres:15\n    container_name: airflow-postgres\n    restart: unless-stopped\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    ports:\n      - "5432:5432"\n    healthcheck:\n      test: ["CMD", "pg_isready", "-U", "airflow"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n\n  # Redis for Celery executor\n  redis:\n    image: redis:7-alpine\n    container_name: airflow-redis\n    restart: unless-stopped\n    expose:\n      - 6379\n    volumes:\n      - redis-data:/data\n    healthcheck:\n      test: ["CMD", "redis-cli", "ping"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n\n  # Airflow webserver\n  airflow-webserver:\n    image: apache/airflow:2.7.3\n    container_name: airflow-webserver\n    restart: unless-stopped\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment: &airflow-common-env\n      AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n      AIRFLOW__CORE__FERNET_KEY: \'\'\n      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \'true\'\n      AIRFLOW__CORE__LOAD_EXAMPLES: \'false\'\n      AIRFLOW__API__AUTH_BACKENDS: \'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\'\n      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \'true\'\n      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: \'true\'\n      _PIP_ADDITIONAL_REQUIREMENTS: \'apache-airflow-providers-postgres apache-airflow-providers-redis apache-airflow-providers-docker apache-airflow-providers-kubernetes\'\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./logs:/opt/airflow/logs\n      - ./plugins:/opt/airflow/plugins\n      - ./config:/opt/airflow/config\n      - /var/run/docker.sock:/var/run/docker.sock\n    ports:\n      - "8080:8080"\n    command: webserver\n    healthcheck:\n      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  # Airflow scheduler\n  airflow-scheduler:\n    image: apache/airflow:2.7.3\n    container_name: airflow-scheduler\n    restart: unless-stopped\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      <<: *airflow-common-env\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./logs:/opt/airflow/logs\n      - ./plugins:/opt/airflow/plugins\n      - ./config:/opt/airflow/config\n      - /var/run/docker.sock:/var/run/docker.sock\n    command: scheduler\n    healthcheck:\n      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  # Airflow worker\n  airflow-worker:\n    image: apache/airflow:2.7.3\n    container_name: airflow-worker\n    restart: unless-stopped\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      <<: *airflow-common-env\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./logs:/opt/airflow/logs\n      - ./plugins:/opt/airflow/plugins\n      - ./config:/opt/airflow/config\n      - /var/run/docker.sock:/var/run/docker.sock\n    command: celery worker\n    healthcheck:\n      test: ["CMD-SHELL", \'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"\']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  # Airflow triggerer\n  airflow-triggerer:\n    image: apache/airflow:2.7.3\n    container_name: airflow-triggerer\n    restart: unless-stopped\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      <<: *airflow-common-env\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./logs:/opt/airflow/logs\n      - ./plugins:/opt/airflow/plugins\n      - ./config:/opt/airflow/config\n    command: triggerer\n    healthcheck:\n      test: ["CMD-SHELL", \'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"\']\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  # Flower for monitoring Celery\n  flower:\n    image: apache/airflow:2.7.3\n    container_name: airflow-flower\n    restart: unless-stopped\n    depends_on:\n      redis:\n        condition: service_healthy\n    environment:\n      <<: *airflow-common-env\n    ports:\n      - "5555:5555"\n    command: celery flower\n\n  # Airflow CLI for initialization\n  airflow-init:\n    image: apache/airflow:2.7.3\n    container_name: airflow-init\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      <<: *airflow-common-env\n      _AIRFLOW_DB_UPGRADE: \'true\'\n      _AIRFLOW_WWW_USER_CREATE: \'true\'\n      _AIRFLOW_WWW_USER_USERNAME: admin\n      _AIRFLOW_WWW_USER_PASSWORD: admin\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./logs:/opt/airflow/logs\n      - ./plugins:/opt/airflow/plugins\n      - ./config:/opt/airflow/config\n    command: version\n\nvolumes:\n  postgres-data:\n  redis-data:\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create necessary directories:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"mkdir -p dags logs plugins config\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Start Airflow:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker compose up airflow-init\ndocker compose up -d\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Access Airflow UI:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'echo "Airflow UI: http://localhost:8080"\necho "Username: admin"\necho "Password: admin"\necho "Flower UI: http://localhost:5555"\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"basic-dag-examples",children:"Basic DAG Examples"}),"\n",(0,r.jsx)(n.h3,{id:"simple-etl-pipeline",children:"Simple ETL Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create dags/simple_etl_dag.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nimport pandas as pd\nimport requests\n\n# Default arguments\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5),\n    'catchup': False\n}\n\n# Define DAG\ndag = DAG(\n    'simple_etl_pipeline',\n    default_args=default_args,\n    description='A simple ETL pipeline',\n    schedule_interval=timedelta(hours=1),\n    max_active_runs=1,\n    tags=['etl', 'data-pipeline']\n)\n\ndef extract_data(**context):\n    \"\"\"Extract data from API\"\"\"\n    print(\"Extracting data from API...\")\n    \n    # Example API call\n    response = requests.get('https://jsonplaceholder.typicode.com/users')\n    data = response.json()\n    \n    # Save to temporary location\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/extracted_data.csv', index=False)\n    \n    print(f\"Extracted {len(data)} records\")\n    return len(data)\n\ndef transform_data(**context):\n    \"\"\"Transform the extracted data\"\"\"\n    print(\"Transforming data...\")\n    \n    # Read extracted data\n    df = pd.read_csv('/tmp/extracted_data.csv')\n    \n    # Data transformations\n    df['full_name'] = df['name'].str.upper()\n    df['domain'] = df['email'].str.split('@').str[1]\n    df['created_at'] = datetime.now()\n    \n    # Select relevant columns\n    transformed_df = df[['id', 'full_name', 'email', 'domain', 'phone', 'created_at']]\n    \n    # Save transformed data\n    transformed_df.to_csv('/tmp/transformed_data.csv', index=False)\n    \n    print(f\"Transformed {len(transformed_df)} records\")\n    return len(transformed_df)\n\ndef load_data(**context):\n    \"\"\"Load data into database\"\"\"\n    print(\"Loading data into database...\")\n    \n    # Read transformed data\n    df = pd.read_csv('/tmp/transformed_data.csv')\n    \n    # Get database connection\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    \n    # Insert data\n    for _, row in df.iterrows():\n        postgres_hook.run(\"\"\"\n            INSERT INTO users (id, full_name, email, domain, phone, created_at)\n            VALUES (%s, %s, %s, %s, %s, %s)\n            ON CONFLICT (id) DO UPDATE SET\n                full_name = EXCLUDED.full_name,\n                email = EXCLUDED.email,\n                domain = EXCLUDED.domain,\n                phone = EXCLUDED.phone,\n                created_at = EXCLUDED.created_at\n        \"\"\", parameters=(\n            row['id'], row['full_name'], row['email'], \n            row['domain'], row['phone'], row['created_at']\n        ))\n    \n    print(f\"Loaded {len(df)} records into database\")\n    return len(df)\n\ndef cleanup_temp_files(**context):\n    \"\"\"Clean up temporary files\"\"\"\n    import os\n    \n    temp_files = ['/tmp/extracted_data.csv', '/tmp/transformed_data.csv']\n    \n    for file_path in temp_files:\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            print(f\"Removed {file_path}\")\n\n# Create table task\ncreate_table = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='postgres_default',\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id INTEGER PRIMARY KEY,\n            full_name VARCHAR(255),\n            email VARCHAR(255),\n            domain VARCHAR(255),\n            phone VARCHAR(255),\n            created_at TIMESTAMP\n        );\n    \"\"\",\n    dag=dag\n)\n\n# Extract task\nextract_task = PythonOperator(\n    task_id='extract_data',\n    python_callable=extract_data,\n    dag=dag\n)\n\n# Transform task\ntransform_task = PythonOperator(\n    task_id='transform_data',\n    python_callable=transform_data,\n    dag=dag\n)\n\n# Load task\nload_task = PythonOperator(\n    task_id='load_data',\n    python_callable=load_data,\n    dag=dag\n)\n\n# Data quality check\nquality_check = PostgresOperator(\n    task_id='data_quality_check',\n    postgres_conn_id='postgres_default',\n    sql=\"\"\"\n        SELECT COUNT(*) as record_count,\n               COUNT(DISTINCT email) as unique_emails,\n               COUNT(CASE WHEN email LIKE '%@%' THEN 1 END) as valid_emails\n        FROM users\n        WHERE created_at >= NOW() - INTERVAL '1 hour';\n    \"\"\",\n    dag=dag\n)\n\n# Cleanup task\ncleanup_task = PythonOperator(\n    task_id='cleanup_temp_files',\n    python_callable=cleanup_temp_files,\n    dag=dag,\n    trigger_rule='all_done'  # Run regardless of upstream task status\n)\n\n# Define task dependencies\ncreate_table >> extract_task >> transform_task >> load_task >> quality_check >> cleanup_task\n"})}),"\n",(0,r.jsx)(n.h3,{id:"data-processing-with-docker",children:"Data Processing with Docker"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create dags/docker_processing_dag.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'docker_data_processing',\n    default_args=default_args,\n    description='Data processing using Docker containers',\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['docker', 'data-processing']\n)\n\n# Python data processing in Docker\npython_processing = DockerOperator(\n    task_id='python_data_processing',\n    image='python:3.9-slim',\n    command=[\n        'bash', '-c',\n        '''\n        pip install pandas requests &&\n        python -c \"\nimport pandas as pd\nimport requests\nimport json\n\n# Fetch data\nresponse = requests.get('https://api.github.com/repos/apache/airflow/commits')\ncommits = response.json()\n\n# Process data\ndf = pd.DataFrame([{\n    'sha': commit['sha'][:7],\n    'author': commit['commit']['author']['name'],\n    'message': commit['commit']['message'][:50],\n    'date': commit['commit']['author']['date']\n} for commit in commits])\n\n# Save results\ndf.to_csv('/tmp/processed_commits.csv', index=False)\nprint(f'Processed {len(df)} commits')\n        \"\n        '''\n    ],\n    volumes=['/tmp:/tmp'],\n    dag=dag\n)\n\n# Spark processing in Docker\nspark_processing = DockerOperator(\n    task_id='spark_data_processing',\n    image='bitnami/spark:3.4',\n    command=[\n        'spark-submit',\n        '--master', 'local[*]',\n        '--py-files', '/tmp/spark_job.py',\n        '/tmp/spark_job.py'\n    ],\n    volumes=['/tmp:/tmp'],\n    dag=dag\n)\n\n# Create Spark job file\ncreate_spark_job = BashOperator(\n    task_id='create_spark_job',\n    bash_command='''\ncat > /tmp/spark_job.py << 'EOF'\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n\n# Create sample data\ndata = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\ndf = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n\n# Process data\nresult = df.withColumn(\"age_group\", \n    when(col(\"age\") < 30, \"Young\")\n    .when(col(\"age\") < 40, \"Middle\")\n    .otherwise(\"Senior\")\n)\n\n# Save results\nresult.write.mode(\"overwrite\").csv(\"/tmp/spark_output\")\n\nprint(\"Spark processing completed\")\nspark.stop()\nEOF\n    ''',\n    dag=dag\n)\n\n# Task dependencies\ncreate_spark_job >> [python_processing, spark_processing]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"machine-learning-pipeline",children:"Machine Learning Pipeline"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create dags/ml_pipeline_dag.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport joblib\nimport os\n\ndefault_args = {\n    'owner': 'ml-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'ml_training_pipeline',\n    default_args=default_args,\n    description='Machine Learning training pipeline',\n    schedule_interval='@weekly',\n    catchup=False,\n    tags=['ml', 'training']\n)\n\ndef extract_training_data(**context):\n    \"\"\"Extract data for model training\"\"\"\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    \n    # Extract features and target\n    sql = \"\"\"\n        SELECT feature1, feature2, feature3, feature4, target\n        FROM ml_training_data\n        WHERE created_at >= NOW() - INTERVAL '30 days'\n    \"\"\"\n    \n    df = postgres_hook.get_pandas_df(sql)\n    \n    # Save to temporary location\n    df.to_csv('/tmp/training_data.csv', index=False)\n    \n    print(f\"Extracted {len(df)} training samples\")\n    return len(df)\n\ndef preprocess_data(**context):\n    \"\"\"Preprocess the training data\"\"\"\n    df = pd.read_csv('/tmp/training_data.csv')\n    \n    # Handle missing values\n    df = df.fillna(df.mean())\n    \n    # Feature engineering\n    df['feature_interaction'] = df['feature1'] * df['feature2']\n    df['feature_ratio'] = df['feature3'] / (df['feature4'] + 1)\n    \n    # Save preprocessed data\n    df.to_csv('/tmp/preprocessed_data.csv', index=False)\n    \n    print(f\"Preprocessed {len(df)} samples\")\n    return len(df)\n\ndef train_model(**context):\n    \"\"\"Train the machine learning model\"\"\"\n    df = pd.read_csv('/tmp/preprocessed_data.csv')\n    \n    # Prepare features and target\n    feature_columns = ['feature1', 'feature2', 'feature3', 'feature4', \n                      'feature_interaction', 'feature_ratio']\n    X = df[feature_columns]\n    y = df['target']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Train model\n    model = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    model.fit(X_train, y_train)\n    \n    # Evaluate model\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    print(f\"Model accuracy: {accuracy:.4f}\")\n    \n    # Save model\n    os.makedirs('/tmp/models', exist_ok=True)\n    model_path = f'/tmp/models/model_{context[\"ds\"]}.joblib'\n    joblib.dump(model, model_path)\n    \n    # Save metrics\n    metrics = {\n        'accuracy': accuracy,\n        'training_samples': len(X_train),\n        'test_samples': len(X_test),\n        'model_path': model_path\n    }\n    \n    return metrics\n\ndef validate_model(**context):\n    \"\"\"Validate the trained model\"\"\"\n    ti = context['ti']\n    metrics = ti.xcom_pull(task_ids='train_model')\n    \n    # Model validation criteria\n    min_accuracy = 0.8\n    min_samples = 1000\n    \n    if metrics['accuracy'] < min_accuracy:\n        raise ValueError(f\"Model accuracy {metrics['accuracy']:.4f} below threshold {min_accuracy}\")\n    \n    if metrics['training_samples'] < min_samples:\n        raise ValueError(f\"Training samples {metrics['training_samples']} below threshold {min_samples}\")\n    \n    print(\"Model validation passed\")\n    return True\n\ndef deploy_model(**context):\n    \"\"\"Deploy the validated model\"\"\"\n    ti = context['ti']\n    metrics = ti.xcom_pull(task_ids='train_model')\n    \n    model_path = metrics['model_path']\n    \n    # In production, this would deploy to a model serving platform\n    # For now, we'll just copy to a \"production\" location\n    production_path = '/tmp/models/production_model.joblib'\n    \n    import shutil\n    shutil.copy2(model_path, production_path)\n    \n    print(f\"Model deployed to {production_path}\")\n    \n    # Log deployment\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    postgres_hook.run(\"\"\"\n        INSERT INTO model_deployments (model_path, accuracy, deployed_at)\n        VALUES (%s, %s, %s)\n    \"\"\", parameters=(production_path, metrics['accuracy'], datetime.now()))\n    \n    return production_path\n\n# Define tasks\nextract_task = PythonOperator(\n    task_id='extract_training_data',\n    python_callable=extract_training_data,\n    dag=dag\n)\n\npreprocess_task = PythonOperator(\n    task_id='preprocess_data',\n    python_callable=preprocess_data,\n    dag=dag\n)\n\ntrain_task = PythonOperator(\n    task_id='train_model',\n    python_callable=train_model,\n    dag=dag\n)\n\nvalidate_task = PythonOperator(\n    task_id='validate_model',\n    python_callable=validate_model,\n    dag=dag\n)\n\ndeploy_task = PythonOperator(\n    task_id='deploy_model',\n    python_callable=deploy_model,\n    dag=dag\n)\n\n# Task dependencies\nextract_task >> preprocess_task >> train_task >> validate_task >> deploy_task\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,r.jsx)(n.h3,{id:"dynamic-dags",children:"Dynamic DAGs"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create dags/dynamic_dag_generator.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\n\n# Configuration for different data sources\nDATA_SOURCES = {\n    'users': {\n        'table': 'users',\n        'api_endpoint': 'https://api.example.com/users',\n        'schedule': '@hourly'\n    },\n    'orders': {\n        'table': 'orders',\n        'api_endpoint': 'https://api.example.com/orders',\n        'schedule': '@daily'\n    },\n    'products': {\n        'table': 'products',\n        'api_endpoint': 'https://api.example.com/products',\n        'schedule': '@weekly'\n    }\n}\n\ndef create_dag(source_name, config):\n    \"\"\"Create a DAG for a specific data source\"\"\"\n    \n    default_args = {\n        'owner': 'data-team',\n        'depends_on_past': False,\n        'start_date': datetime(2024, 1, 1),\n        'retries': 2,\n        'retry_delay': timedelta(minutes=5)\n    }\n    \n    dag = DAG(\n        f'etl_{source_name}',\n        default_args=default_args,\n        description=f'ETL pipeline for {source_name}',\n        schedule_interval=config['schedule'],\n        catchup=False,\n        tags=['etl', 'dynamic', source_name]\n    )\n    \n    def extract_data(**context):\n        import requests\n        import pandas as pd\n        \n        response = requests.get(config['api_endpoint'])\n        data = response.json()\n        \n        df = pd.DataFrame(data)\n        df.to_csv(f'/tmp/{source_name}_data.csv', index=False)\n        \n        return len(data)\n    \n    def load_data(**context):\n        import pandas as pd\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        \n        df = pd.read_csv(f'/tmp/{source_name}_data.csv')\n        postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n        \n        # Truncate and load\n        postgres_hook.run(f'TRUNCATE TABLE {config[\"table\"]}')\n        \n        for _, row in df.iterrows():\n            columns = ', '.join(row.index)\n            values = ', '.join(['%s'] * len(row))\n            sql = f'INSERT INTO {config[\"table\"]} ({columns}) VALUES ({values})'\n            postgres_hook.run(sql, parameters=tuple(row.values))\n        \n        return len(df)\n    \n    # Create tasks\n    extract_task = PythonOperator(\n        task_id=f'extract_{source_name}',\n        python_callable=extract_data,\n        dag=dag\n    )\n    \n    load_task = PythonOperator(\n        task_id=f'load_{source_name}',\n        python_callable=load_data,\n        dag=dag\n    )\n    \n    cleanup_task = BashOperator(\n        task_id=f'cleanup_{source_name}',\n        bash_command=f'rm -f /tmp/{source_name}_data.csv',\n        dag=dag\n    )\n    \n    # Set dependencies\n    extract_task >> load_task >> cleanup_task\n    \n    return dag\n\n# Generate DAGs for each data source\nfor source_name, config in DATA_SOURCES.items():\n    globals()[f'etl_{source_name}_dag'] = create_dag(source_name, config)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"custom-operators",children:"Custom Operators"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create plugins/operators/custom_operators.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nimport requests\nimport pandas as pd\n\nclass APIToPostgresOperator(BaseOperator):\n    """Custom operator to extract data from API and load to PostgreSQL"""\n    \n    template_fields = [\'api_endpoint\', \'table_name\']\n    \n    @apply_defaults\n    def __init__(\n        self,\n        api_endpoint,\n        table_name,\n        postgres_conn_id=\'postgres_default\',\n        method=\'GET\',\n        headers=None,\n        params=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.api_endpoint = api_endpoint\n        self.table_name = table_name\n        self.postgres_conn_id = postgres_conn_id\n        self.method = method\n        self.headers = headers or {}\n        self.params = params or {}\n    \n    def execute(self, context):\n        # Extract data from API\n        self.log.info(f"Extracting data from {self.api_endpoint}")\n        \n        response = requests.request(\n            method=self.method,\n            url=self.api_endpoint,\n            headers=self.headers,\n            params=self.params\n        )\n        response.raise_for_status()\n        \n        data = response.json()\n        df = pd.DataFrame(data)\n        \n        self.log.info(f"Extracted {len(df)} records")\n        \n        # Load data to PostgreSQL\n        postgres_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        \n        # Create table if not exists (basic implementation)\n        if not df.empty:\n            columns = []\n            for col in df.columns:\n                if df[col].dtype == \'object\':\n                    columns.append(f"{col} TEXT")\n                elif df[col].dtype in [\'int64\', \'int32\']:\n                    columns.append(f"{col} INTEGER")\n                elif df[col].dtype in [\'float64\', \'float32\']:\n                    columns.append(f"{col} FLOAT")\n                else:\n                    columns.append(f"{col} TEXT")\n            \n            create_table_sql = f"""\n                CREATE TABLE IF NOT EXISTS {self.table_name} (\n                    {\', \'.join(columns)}\n                )\n            """\n            postgres_hook.run(create_table_sql)\n            \n            # Insert data\n            for _, row in df.iterrows():\n                columns_str = \', \'.join(row.index)\n                values_str = \', \'.join([\'%s\'] * len(row))\n                insert_sql = f"INSERT INTO {self.table_name} ({columns_str}) VALUES ({values_str})"\n                postgres_hook.run(insert_sql, parameters=tuple(row.values))\n        \n        self.log.info(f"Loaded {len(df)} records to {self.table_name}")\n        return len(df)\n\nclass DataQualityOperator(BaseOperator):\n    """Custom operator for data quality checks"""\n    \n    template_fields = [\'sql_checks\']\n    \n    @apply_defaults\n    def __init__(\n        self,\n        sql_checks,\n        postgres_conn_id=\'postgres_default\',\n        *args,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.sql_checks = sql_checks\n        self.postgres_conn_id = postgres_conn_id\n    \n    def execute(self, context):\n        postgres_hook = PostgresHook(postgres_conn_id=self.postgres_conn_id)\n        \n        failed_checks = []\n        \n        for check_name, check_sql in self.sql_checks.items():\n            self.log.info(f"Running data quality check: {check_name}")\n            \n            result = postgres_hook.get_first(check_sql)\n            \n            if result and result[0] == 0:\n                failed_checks.append(check_name)\n                self.log.error(f"Data quality check failed: {check_name}")\n            else:\n                self.log.info(f"Data quality check passed: {check_name}")\n        \n        if failed_checks:\n            raise ValueError(f"Data quality checks failed: {\', \'.join(failed_checks)}")\n        \n        self.log.info("All data quality checks passed")\n        return True\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"monitoring-and-alerting",children:"Monitoring and Alerting"}),"\n",(0,r.jsx)(n.h3,{id:"custom-monitoring-dag",children:"Custom Monitoring DAG"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Create dags/monitoring_dag.py:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.email.operators.email import EmailOperator\nimport requests\n\ndefault_args = {\n    'owner': 'ops-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'system_monitoring',\n    default_args=default_args,\n    description='System monitoring and alerting',\n    schedule_interval=timedelta(minutes=15),\n    catchup=False,\n    tags=['monitoring', 'alerts']\n)\n\ndef check_dag_failures(**context):\n    \"\"\"Check for failed DAG runs in the last hour\"\"\"\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    \n    sql = \"\"\"\n        SELECT dag_id, COUNT(*) as failure_count\n        FROM dag_run\n        WHERE state = 'failed'\n        AND start_date >= NOW() - INTERVAL '1 hour'\n        GROUP BY dag_id\n        HAVING COUNT(*) > 0\n    \"\"\"\n    \n    failures = postgres_hook.get_records(sql)\n    \n    if failures:\n        failure_msg = \"Failed DAGs in the last hour:\\n\"\n        for dag_id, count in failures:\n            failure_msg += f\"- {dag_id}: {count} failures\\n\"\n        \n        context['task_instance'].xcom_push(key='dag_failures', value=failure_msg)\n        return True\n    \n    return False\n\ndef check_task_duration(**context):\n    \"\"\"Check for tasks running longer than expected\"\"\"\n    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')\n    \n    sql = \"\"\"\n        SELECT dag_id, task_id, \n               EXTRACT(EPOCH FROM (NOW() - start_date))/60 as duration_minutes\n        FROM task_instance\n        WHERE state = 'running'\n        AND start_date < NOW() - INTERVAL '30 minutes'\n    \"\"\"\n    \n    long_running = postgres_hook.get_records(sql)\n    \n    if long_running:\n        duration_msg = \"Long-running tasks:\\n\"\n        for dag_id, task_id, duration in long_running:\n            duration_msg += f\"- {dag_id}.{task_id}: {duration:.1f} minutes\\n\"\n        \n        context['task_instance'].xcom_push(key='long_running_tasks', value=duration_msg)\n        return True\n    \n    return False\n\ndef check_system_resources(**context):\n    \"\"\"Check system resource usage\"\"\"\n    # This would typically check actual system metrics\n    # For demo purposes, we'll simulate some checks\n    \n    alerts = []\n    \n    # Simulate disk usage check\n    disk_usage = 85  # Simulated percentage\n    if disk_usage > 80:\n        alerts.append(f\"High disk usage: {disk_usage}%\")\n    \n    # Simulate memory usage check\n    memory_usage = 75  # Simulated percentage\n    if memory_usage > 80:\n        alerts.append(f\"High memory usage: {memory_usage}%\")\n    \n    if alerts:\n        resource_msg = \"System resource alerts:\\n\" + \"\\n\".join(f\"- {alert}\" for alert in alerts)\n        context['task_instance'].xcom_push(key='resource_alerts', value=resource_msg)\n        return True\n    \n    return False\n\ndef send_alert_if_needed(**context):\n    \"\"\"Send alert email if any issues were found\"\"\"\n    ti = context['task_instance']\n    \n    dag_failures = ti.xcom_pull(task_ids='check_dag_failures', key='dag_failures')\n    long_running = ti.xcom_pull(task_ids='check_task_duration', key='long_running_tasks')\n    resource_alerts = ti.xcom_pull(task_ids='check_system_resources', key='resource_alerts')\n    \n    if any([dag_failures, long_running, resource_alerts]):\n        alert_message = \"Airflow System Alert\\n\\n\"\n        \n        if dag_failures:\n            alert_message += dag_failures + \"\\n\"\n        \n        if long_running:\n            alert_message += long_running + \"\\n\"\n        \n        if resource_alerts:\n            alert_message += resource_alerts + \"\\n\"\n        \n        # In a real implementation, you would send this via email or Slack\n        print(\"ALERT:\", alert_message)\n        return alert_message\n    \n    return \"No alerts\"\n\n# Define tasks\ncheck_dag_failures_task = PythonOperator(\n    task_id='check_dag_failures',\n    python_callable=check_dag_failures,\n    dag=dag\n)\n\ncheck_task_duration_task = PythonOperator(\n    task_id='check_task_duration',\n    python_callable=check_task_duration,\n    dag=dag\n)\n\ncheck_system_resources_task = PythonOperator(\n    task_id='check_system_resources',\n    python_callable=check_system_resources,\n    dag=dag\n)\n\nsend_alert_task = PythonOperator(\n    task_id='send_alert_if_needed',\n    python_callable=send_alert_if_needed,\n    dag=dag\n)\n\n# Task dependencies\n[check_dag_failures_task, check_task_duration_task, check_system_resources_task] >> send_alert_task\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ETL Pipelines"}),": Extract, transform, and load data from various sources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Warehouse Management"}),": Scheduled data processing and aggregation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Machine Learning Workflows"}),": Model training, validation, and deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Quality Monitoring"}),": Automated data validation and alerting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Report Generation"}),": Scheduled report creation and distribution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System Maintenance"}),": Automated cleanup and maintenance tasks"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Apache Airflow is now configured for comprehensive workflow orchestration!"})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>i});var t=a(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);