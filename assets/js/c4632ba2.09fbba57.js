"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2166],{6401:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"Build-ML-Tools/vllm","title":"\u26a1 vLLM - High-Performance LLM Inference Engine","description":"vLLM is a fast and easy-to-use library for LLM inference and serving. It provides high-throughput serving with various decoding algorithms, continuous batching, and optimized CUDA kernels for maximum performance.","source":"@site/docs/Build-ML-Tools/vllm.md","sourceDirName":"Build-ML-Tools","slug":"/Build-ML-Tools/vllm","permalink":"/docs/docs/Build-ML-Tools/vllm","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Build-ML-Tools/vllm.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\ud83d\ude80 Google Vertex AI - Unified ML Platform","permalink":"/docs/docs/Build-ML-Tools/vertex-ai"},"next":{"title":"Observability & Monitoring","permalink":"/docs/docs/category/observability--monitoring"}}');var l=t(4848),o=t(8453);const i={},r="\u26a1 vLLM - High-Performance LLM Inference Engine",s={},m=[{value:"\ud83d\udccb Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udee0\ufe0f Installation",id:"\ufe0f-installation",level:2},{value:"Basic Installation",id:"basic-installation",level:3},{value:"Docker Installation",id:"docker-installation",level:3},{value:"\ud83d\ude80 Quick Start",id:"-quick-start",level:2},{value:"Basic Text Generation",id:"basic-text-generation",level:3},{value:"OpenAI-Compatible API Server",id:"openai-compatible-api-server",level:3},{value:"\ud83c\udfd7\ufe0f Advanced Configuration",id:"\ufe0f-advanced-configuration",level:2},{value:"Model Loading Options",id:"model-loading-options",level:3},{value:"Multi-GPU Setup",id:"multi-gpu-setup",level:3},{value:"\ud83d\udd27 Production Deployment",id:"-production-deployment",level:2},{value:"Docker Compose Setup",id:"docker-compose-setup",level:3},{value:"Nginx Load Balancer",id:"nginx-load-balancer",level:3},{value:"Kubernetes Deployment",id:"kubernetes-deployment",level:3},{value:"\ud83d\udd04 Batch Processing",id:"-batch-processing",level:2},{value:"Offline Batch Inference",id:"offline-batch-inference",level:3},{value:"Streaming Responses",id:"streaming-responses",level:3},{value:"\ud83c\udfaf Model-Specific Configurations",id:"-model-specific-configurations",level:2},{value:"Llama 2 Setup",id:"llama-2-setup",level:3},{value:"Code Generation Models",id:"code-generation-models",level:3},{value:"\ud83d\udcca Performance Monitoring",id:"-performance-monitoring",level:2},{value:"Metrics Collection",id:"metrics-collection",level:3},{value:"Health Check Endpoint",id:"health-check-endpoint",level:3},{value:"\ud83d\udd12 Security &amp; Authentication",id:"-security--authentication",level:2},{value:"API Key Authentication",id:"api-key-authentication",level:3},{value:"Rate Limiting",id:"rate-limiting",level:3},{value:"\ud83d\udd0d Troubleshooting",id:"-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Mode",id:"debug-mode",level:3},{value:"\ud83d\udcc8 Performance Optimization",id:"-performance-optimization",level:2},{value:"Optimal Batch Sizes",id:"optimal-batch-sizes",level:3},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"-vllm---high-performance-llm-inference-engine",children:"\u26a1 vLLM - High-Performance LLM Inference Engine"})}),"\n",(0,l.jsx)(n.p,{children:"vLLM is a fast and easy-to-use library for LLM inference and serving. It provides high-throughput serving with various decoding algorithms, continuous batching, and optimized CUDA kernels for maximum performance."}),"\n",(0,l.jsx)(n.h2,{id:"-prerequisites",children:"\ud83d\udccb Prerequisites"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Python 3.8+"}),"\n",(0,l.jsx)(n.li,{children:"CUDA 11.8+ (for GPU acceleration)"}),"\n",(0,l.jsx)(n.li,{children:"PyTorch 2.0+"}),"\n",(0,l.jsx)(n.li,{children:"At least 16GB GPU memory for most models"}),"\n",(0,l.jsx)(n.li,{children:"Linux or WSL2 (recommended)"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"\ufe0f-installation",children:"\ud83d\udee0\ufe0f Installation"}),"\n",(0,l.jsx)(n.h3,{id:"basic-installation",children:"Basic Installation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# Install from PyPI\npip install vllm\n\n# Install with specific CUDA version\npip install vllm --extra-index-url https://download.pytorch.org/whl/cu118\n\n# Install from source (latest features)\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n"})}),"\n",(0,l.jsx)(n.h3,{id:"docker-installation",children:"Docker Installation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"# Pull official Docker image\ndocker pull vllm/vllm-openai:latest\n\n# Run with GPU support\ndocker run --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model microsoft/DialoGPT-medium\n"})}),"\n",(0,l.jsx)(n.h2,{id:"-quick-start",children:"\ud83d\ude80 Quick Start"}),"\n",(0,l.jsx)(n.h3,{id:"basic-text-generation",children:"Basic Text Generation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# Initialize the model\nllm = LLM(model="microsoft/DialoGPT-medium")\n\n# Define sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.8,\n    top_p=0.95,\n    max_tokens=100\n)\n\n# Generate text\nprompts = [\n    "The future of AI is",\n    "In a world where technology",\n    "The most important skill for developers"\n]\n\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")\n'})}),"\n",(0,l.jsx)(n.h3,{id:"openai-compatible-api-server",children:"OpenAI-Compatible API Server"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'# Start API server\npython -m vllm.entrypoints.openai.api_server \\\n    --model microsoft/DialoGPT-medium \\\n    --port 8000 \\\n    --host 0.0.0.0\n\n# Test with curl\ncurl http://localhost:8000/v1/completions \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n        "model": "microsoft/DialoGPT-medium",\n        "prompt": "San Francisco is a",\n        "max_tokens": 50,\n        "temperature": 0.7\n    }\'\n'})}),"\n",(0,l.jsx)(n.h2,{id:"\ufe0f-advanced-configuration",children:"\ud83c\udfd7\ufe0f Advanced Configuration"}),"\n",(0,l.jsx)(n.h3,{id:"model-loading-options",children:"Model Loading Options"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# Load with specific configurations\nllm = LLM(\n    model="meta-llama/Llama-2-7b-chat-hf",\n    tensor_parallel_size=2,  # Use 2 GPUs\n    dtype="float16",         # Use half precision\n    max_model_len=4096,      # Maximum sequence length\n    gpu_memory_utilization=0.9,  # Use 90% of GPU memory\n    trust_remote_code=True,  # Allow custom model code\n    download_dir="/path/to/models",  # Custom model cache\n)\n\n# Advanced sampling parameters\nsampling_params = SamplingParams(\n    temperature=0.8,\n    top_p=0.95,\n    top_k=50,\n    repetition_penalty=1.1,\n    max_tokens=512,\n    stop=["</s>", "\\n\\n"],\n    presence_penalty=0.1,\n    frequency_penalty=0.1,\n)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"multi-gpu-setup",children:"Multi-GPU Setup"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Tensor parallelism across multiple GPUs\nllm = LLM(\n    model="meta-llama/Llama-2-13b-chat-hf",\n    tensor_parallel_size=4,  # Use 4 GPUs\n    pipeline_parallel_size=1,\n    max_model_len=2048,\n    dtype="bfloat16",\n)\n\n# Pipeline parallelism for very large models\nllm = LLM(\n    model="meta-llama/Llama-2-70b-chat-hf",\n    tensor_parallel_size=4,\n    pipeline_parallel_size=2,  # 8 GPUs total (4x2)\n    max_model_len=2048,\n)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-production-deployment",children:"\ud83d\udd27 Production Deployment"}),"\n",(0,l.jsx)(n.h3,{id:"docker-compose-setup",children:"Docker Compose Setup"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:'# docker-compose.yml\nversion: \'3.8\'\n\nservices:\n  vllm-server:\n    image: vllm/vllm-openai:latest\n    ports:\n      - "8000:8000"\n    volumes:\n      - ~/.cache/huggingface:/root/.cache/huggingface\n      - ./models:/models\n    environment:\n      - CUDA_VISIBLE_DEVICES=0,1\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 2\n              capabilities: [gpu]\n    command: >\n      --model meta-llama/Llama-2-7b-chat-hf\n      --host 0.0.0.0\n      --port 8000\n      --tensor-parallel-size 2\n      --max-model-len 4096\n      --gpu-memory-utilization 0.9\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - "80:80"\n      - "443:443"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./ssl:/etc/nginx/ssl\n    depends_on:\n      - vllm-server\n'})}),"\n",(0,l.jsx)(n.h3,{id:"nginx-load-balancer",children:"Nginx Load Balancer"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-nginx",children:"# nginx.conf\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream vllm_backend {\n        server vllm-server:8000;\n        # Add more servers for load balancing\n        # server vllm-server-2:8000;\n        # server vllm-server-3:8000;\n    }\n\n    server {\n        listen 80;\n        server_name your-domain.com;\n\n        location / {\n            proxy_pass http://vllm_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            \n            # Increase timeouts for long generations\n            proxy_connect_timeout 60s;\n            proxy_send_timeout 300s;\n            proxy_read_timeout 300s;\n        }\n\n        location /health {\n            proxy_pass http://vllm_backend/health;\n            access_log off;\n        }\n    }\n}\n"})}),"\n",(0,l.jsx)(n.h3,{id:"kubernetes-deployment",children:"Kubernetes Deployment"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:'# vllm-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-server\n  labels:\n    app: vllm-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vllm-server\n  template:\n    metadata:\n      labels:\n        app: vllm-server\n    spec:\n      containers:\n      - name: vllm-server\n        image: vllm/vllm-openai:latest\n        ports:\n        - containerPort: 8000\n        args:\n          - --model\n          - meta-llama/Llama-2-7b-chat-hf\n          - --host\n          - 0.0.0.0\n          - --port\n          - "8000"\n          - --tensor-parallel-size\n          - "1"\n          - --max-model-len\n          - "4096"\n        resources:\n          requests:\n            nvidia.com/gpu: 1\n            memory: "16Gi"\n            cpu: "4"\n          limits:\n            nvidia.com/gpu: 1\n            memory: "32Gi"\n            cpu: "8"\n        volumeMounts:\n        - name: model-cache\n          mountPath: /root/.cache/huggingface\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n      volumes:\n      - name: model-cache\n        persistentVolumeClaim:\n          claimName: model-cache-pvc\n      nodeSelector:\n        accelerator: nvidia-tesla-v100\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-service\nspec:\n  selector:\n    app: vllm-server\n  ports:\n    - protocol: TCP\n      port: 8000\n      targetPort: 8000\n  type: LoadBalancer\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-batch-processing",children:"\ud83d\udd04 Batch Processing"}),"\n",(0,l.jsx)(n.h3,{id:"offline-batch-inference",children:"Offline Batch Inference"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"from vllm import LLM, SamplingParams\nimport json\n\ndef batch_inference(input_file, output_file, model_name):\n    # Load model\n    llm = LLM(\n        model=model_name,\n        tensor_parallel_size=2,\n        max_model_len=2048,\n    )\n    \n    # Load prompts\n    with open(input_file, 'r') as f:\n        prompts = [json.loads(line)['prompt'] for line in f]\n    \n    # Sampling parameters\n    sampling_params = SamplingParams(\n        temperature=0.7,\n        top_p=0.9,\n        max_tokens=256,\n        stop=[\"</s>\"]\n    )\n    \n    # Generate in batches\n    batch_size = 32\n    results = []\n    \n    for i in range(0, len(prompts), batch_size):\n        batch_prompts = prompts[i:i+batch_size]\n        outputs = llm.generate(batch_prompts, sampling_params)\n        \n        for output in outputs:\n            results.append({\n                'prompt': output.prompt,\n                'generated_text': output.outputs[0].text,\n                'tokens': len(output.outputs[0].token_ids),\n            })\n    \n    # Save results\n    with open(output_file, 'w') as f:\n        for result in results:\n            f.write(json.dumps(result) + '\\n')\n\n# Usage\nbatch_inference('prompts.jsonl', 'outputs.jsonl', 'meta-llama/Llama-2-7b-chat-hf')\n"})}),"\n",(0,l.jsx)(n.h3,{id:"streaming-responses",children:"Streaming Responses"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\nllm = LLM(model="microsoft/DialoGPT-medium")\n\ndef stream_generate(prompt):\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        max_tokens=200,\n        stream=True\n    )\n    \n    for output in llm.generate([prompt], sampling_params):\n        for token_output in output.outputs:\n            yield token_output.text\n\n# Usage\nfor chunk in stream_generate("The future of AI is"):\n    print(chunk, end=\'\', flush=True)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-model-specific-configurations",children:"\ud83c\udfaf Model-Specific Configurations"}),"\n",(0,l.jsx)(n.h3,{id:"llama-2-setup",children:"Llama 2 Setup"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from vllm import LLM, SamplingParams\n\n# Llama 2 7B\nllm = LLM(\n    model="meta-llama/Llama-2-7b-chat-hf",\n    tensor_parallel_size=1,\n    max_model_len=4096,\n    dtype="float16",\n)\n\n# Chat template for Llama 2\ndef format_llama2_prompt(system_message, user_message):\n    return f"""<s>[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{user_message} [/INST]"""\n\nsystem_msg = "You are a helpful assistant."\nuser_msg = "Explain quantum computing in simple terms."\nprompt = format_llama2_prompt(system_msg, user_msg)\n\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.9,\n    max_tokens=512,\n    stop=["</s>"]\n)\n\noutputs = llm.generate([prompt], sampling_params)\nprint(outputs[0].outputs[0].text)\n'})}),"\n",(0,l.jsx)(n.h3,{id:"code-generation-models",children:"Code Generation Models"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# CodeLlama setup\nllm = LLM(\n    model="codellama/CodeLlama-7b-Python-hf",\n    tensor_parallel_size=1,\n    max_model_len=2048,\n    trust_remote_code=True,\n)\n\n# Code generation prompt\ncode_prompt = """# Write a Python function to calculate fibonacci numbers\ndef fibonacci(n):"""\n\nsampling_params = SamplingParams(\n    temperature=0.1,  # Lower temperature for code\n    top_p=0.95,\n    max_tokens=256,\n    stop=["\\n\\n", "def ", "class "]\n)\n\noutputs = llm.generate([code_prompt], sampling_params)\nprint(code_prompt + outputs[0].outputs[0].text)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-performance-monitoring",children:"\ud83d\udcca Performance Monitoring"}),"\n",(0,l.jsx)(n.h3,{id:"metrics-collection",children:"Metrics Collection"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:"import time\nimport psutil\nimport GPUtil\nfrom vllm import LLM, SamplingParams\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = []\n    \n    def measure_inference(self, llm, prompts, sampling_params):\n        start_time = time.time()\n        start_gpu = GPUtil.getGPUs()[0].memoryUsed\n        \n        outputs = llm.generate(prompts, sampling_params)\n        \n        end_time = time.time()\n        end_gpu = GPUtil.getGPUs()[0].memoryUsed\n        \n        total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n        \n        metrics = {\n            'duration': end_time - start_time,\n            'tokens_per_second': total_tokens / (end_time - start_time),\n            'gpu_memory_used': end_gpu - start_gpu,\n            'batch_size': len(prompts),\n            'total_tokens': total_tokens,\n        }\n        \n        self.metrics.append(metrics)\n        return outputs, metrics\n\n# Usage\nmonitor = PerformanceMonitor()\nllm = LLM(model=\"microsoft/DialoGPT-medium\")\nsampling_params = SamplingParams(temperature=0.8, max_tokens=100)\n\nprompts = [\"Hello world\"] * 10\noutputs, metrics = monitor.measure_inference(llm, prompts, sampling_params)\nprint(f\"Tokens/sec: {metrics['tokens_per_second']:.2f}\")\n"})}),"\n",(0,l.jsx)(n.h3,{id:"health-check-endpoint",children:"Health Check Endpoint"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom vllm import LLM, SamplingParams\nimport uvicorn\n\napp = FastAPI()\nllm = LLM(model="microsoft/DialoGPT-medium")\n\n@app.get("/health")\nasync def health_check():\n    try:\n        # Quick inference test\n        test_output = llm.generate(\n            ["Test"], \n            SamplingParams(max_tokens=1, temperature=0)\n        )\n        return {"status": "healthy", "model_loaded": True}\n    except Exception as e:\n        return {"status": "unhealthy", "error": str(e)}\n\n@app.get("/metrics")\nasync def get_metrics():\n    import GPUtil\n    gpu = GPUtil.getGPUs()[0]\n    return {\n        "gpu_utilization": gpu.load * 100,\n        "gpu_memory_used": gpu.memoryUsed,\n        "gpu_memory_total": gpu.memoryTotal,\n        "gpu_temperature": gpu.temperature,\n    }\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8001)\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-security--authentication",children:"\ud83d\udd12 Security & Authentication"}),"\n",(0,l.jsx)(n.h3,{id:"api-key-authentication",children:"API Key Authentication"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException, Depends, Header\nfrom vllm import LLM, SamplingParams\nimport os\n\napp = FastAPI()\nllm = LLM(model="microsoft/DialoGPT-medium")\n\nAPI_KEYS = set(os.getenv("API_KEYS", "").split(","))\n\nasync def verify_api_key(x_api_key: str = Header()):\n    if x_api_key not in API_KEYS:\n        raise HTTPException(status_code=401, detail="Invalid API key")\n    return x_api_key\n\n@app.post("/generate")\nasync def generate_text(\n    request: dict,\n    api_key: str = Depends(verify_api_key)\n):\n    prompt = request.get("prompt", "")\n    max_tokens = min(request.get("max_tokens", 100), 512)  # Limit max tokens\n    \n    sampling_params = SamplingParams(\n        temperature=request.get("temperature", 0.8),\n        max_tokens=max_tokens,\n    )\n    \n    outputs = llm.generate([prompt], sampling_params)\n    return {"generated_text": outputs[0].outputs[0].text}\n'})}),"\n",(0,l.jsx)(n.h3,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n@app.post("/generate")\n@limiter.limit("10/minute")  # 10 requests per minute\nasync def generate_text(request: Request, data: dict):\n    # Generation logic here\n    pass\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-troubleshooting",children:"\ud83d\udd0d Troubleshooting"}),"\n",(0,l.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Out of Memory Errors"})}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Reduce GPU memory usage\nllm = LLM(\n    model="your-model",\n    gpu_memory_utilization=0.8,  # Use 80% instead of 90%\n    max_model_len=2048,          # Reduce sequence length\n    dtype="float16",             # Use half precision\n)\n'})}),"\n",(0,l.jsxs)(n.ol,{start:"2",children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Slow Inference"})}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'# Optimize for throughput\nllm = LLM(\n    model="your-model",\n    tensor_parallel_size=2,      # Use multiple GPUs\n    max_num_batched_tokens=8192, # Increase batch size\n    max_num_seqs=256,           # Process more sequences\n)\n'})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Model Loading Issues"})}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:'# Clear cache and reinstall\npip uninstall vllm\nrm -rf ~/.cache/huggingface\npip install vllm --no-cache-dir\n\n# Check CUDA compatibility\npython -c "import torch; print(torch.cuda.is_available())"\n'})}),"\n",(0,l.jsx)(n.h3,{id:"debug-mode",children:"Debug Mode"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Enable vLLM debug logging\nimport os\nos.environ["VLLM_LOGGING_LEVEL"] = "DEBUG"\n\nfrom vllm import LLM\nllm = LLM(model="your-model")\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-performance-optimization",children:"\ud83d\udcc8 Performance Optimization"}),"\n",(0,l.jsx)(n.h3,{id:"optimal-batch-sizes",children:"Optimal Batch Sizes"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-python",children:'import time\nfrom vllm import LLM, SamplingParams\n\ndef benchmark_batch_sizes(model_name, batch_sizes=[1, 4, 8, 16, 32]):\n    llm = LLM(model=model_name)\n    sampling_params = SamplingParams(temperature=0.8, max_tokens=100)\n    \n    results = {}\n    \n    for batch_size in batch_sizes:\n        prompts = ["Generate a story about"] * batch_size\n        \n        start_time = time.time()\n        outputs = llm.generate(prompts, sampling_params)\n        end_time = time.time()\n        \n        total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n        tokens_per_second = total_tokens / (end_time - start_time)\n        \n        results[batch_size] = tokens_per_second\n        print(f"Batch size {batch_size}: {tokens_per_second:.2f} tokens/sec")\n    \n    return results\n\n# Find optimal batch size\nresults = benchmark_batch_sizes("microsoft/DialoGPT-medium")\noptimal_batch = max(results, key=results.get)\nprint(f"Optimal batch size: {optimal_batch}")\n'})}),"\n",(0,l.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://vllm.readthedocs.io/",children:"vLLM Documentation"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM GitHub Repository"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://blog.vllm.ai/2023/06/20/vllm.html",children:"Performance Benchmarks"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://vllm.readthedocs.io/en/latest/models/supported_models.html",children:"Model Compatibility"})}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"vLLM provides exceptional performance for LLM inference with minimal setup, making it ideal for production deployments requiring high throughput and low latency."})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const l={},o=a.createContext(l);function i(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);