"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9417],{544:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Build-ML-Tools/vertex-ai","title":"\ud83d\ude80 Google Vertex AI - Unified ML Platform","description":"Google Vertex AI is a comprehensive machine learning platform that brings together Google Cloud\'s ML offerings into a unified API, client library, and user interface. It provides tools for every step of the ML workflow, from data preparation to model deployment and monitoring.","source":"@site/docs/Build-ML-Tools/vertex-ai.md","sourceDirName":"Build-ML-Tools","slug":"/Build-ML-Tools/vertex-ai","permalink":"/docs/docs/Build-ML-Tools/vertex-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Build-ML-Tools/vertex-ai.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"CUDA & TensorRT","permalink":"/docs/docs/BuildMLTools/CUDA-TensorRT"},"next":{"title":"\u26a1 vLLM - High-Performance LLM Inference Engine","permalink":"/docs/docs/Build-ML-Tools/vllm"}}');var a=i(4848),o=i(8453);const r={},l="\ud83d\ude80 Google Vertex AI - Unified ML Platform",s={},c=[{value:"\ud83d\udccb Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udee0\ufe0f Installation &amp; Setup",id:"\ufe0f-installation--setup",level:2},{value:"Install Google Cloud SDK",id:"install-google-cloud-sdk",level:3},{value:"Install Vertex AI SDK",id:"install-vertex-ai-sdk",level:3},{value:"Enable Required APIs",id:"enable-required-apis",level:3},{value:"\ud83c\udfd7\ufe0f Basic Configuration",id:"\ufe0f-basic-configuration",level:2},{value:"Set Environment Variables",id:"set-environment-variables",level:3},{value:"Initialize Vertex AI",id:"initialize-vertex-ai",level:3},{value:"\ud83d\ude80 Core Features",id:"-core-features",level:2},{value:"1. AutoML Training",id:"1-automl-training",level:3},{value:"Image Classification",id:"image-classification",level:4},{value:"Tabular Data",id:"tabular-data",level:4},{value:"2. Custom Training",id:"2-custom-training",level:3},{value:"Custom Training Script",id:"custom-training-script",level:4},{value:"Submit Custom Training Job",id:"submit-custom-training-job",level:4},{value:"3. Model Deployment",id:"3-model-deployment",level:3},{value:"Deploy to Endpoint",id:"deploy-to-endpoint",level:4},{value:"Batch Prediction",id:"batch-prediction",level:4},{value:"4. Online Predictions",id:"4-online-predictions",level:3},{value:"\ud83d\udd27 Advanced Features",id:"-advanced-features",level:2},{value:"1. Hyperparameter Tuning",id:"1-hyperparameter-tuning",level:3},{value:"2. Model Monitoring",id:"2-model-monitoring",level:3},{value:"3. Feature Store",id:"3-feature-store",level:3},{value:"4. Pipelines with Kubeflow",id:"4-pipelines-with-kubeflow",level:3},{value:"\ud83d\udc33 Docker Configuration",id:"-docker-configuration",level:2},{value:"Custom Training Container",id:"custom-training-container",level:3},{value:"Build and Push Container",id:"build-and-push-container",level:3},{value:"\ud83d\udcca Monitoring &amp; Logging",id:"-monitoring--logging",level:2},{value:"Model Performance Monitoring",id:"model-performance-monitoring",level:3},{value:"Logging Configuration",id:"logging-configuration",level:3},{value:"\ud83d\udd12 Security Best Practices",id:"-security-best-practices",level:2},{value:"IAM Configuration",id:"iam-configuration",level:3},{value:"Encryption Configuration",id:"encryption-configuration",level:3},{value:"\ud83d\ude80 Production Deployment",id:"-production-deployment",level:2},{value:"Multi-Region Deployment",id:"multi-region-deployment",level:3},{value:"A/B Testing Setup",id:"ab-testing-setup",level:3},{value:"\ud83d\udcc8 Performance Optimization",id:"-performance-optimization",level:2},{value:"Batch Size Optimization",id:"batch-size-optimization",level:3},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"\ud83d\udd0d Troubleshooting",id:"-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"\ud83d\udcda Additional Resources",id:"-additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"-google-vertex-ai---unified-ml-platform",children:"\ud83d\ude80 Google Vertex AI - Unified ML Platform"})}),"\n",(0,a.jsx)(n.p,{children:"Google Vertex AI is a comprehensive machine learning platform that brings together Google Cloud's ML offerings into a unified API, client library, and user interface. It provides tools for every step of the ML workflow, from data preparation to model deployment and monitoring."}),"\n",(0,a.jsx)(n.h2,{id:"-prerequisites",children:"\ud83d\udccb Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Google Cloud Platform account with billing enabled"}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"gcloud"})," CLI installed and configured"]}),"\n",(0,a.jsxs)(n.li,{children:["Python 3.7+ with ",(0,a.jsx)(n.code,{children:"google-cloud-aiplatform"})," library"]}),"\n",(0,a.jsx)(n.li,{children:"Basic understanding of machine learning concepts"}),"\n",(0,a.jsx)(n.li,{children:"Docker (for custom training containers)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-installation--setup",children:"\ud83d\udee0\ufe0f Installation & Setup"}),"\n",(0,a.jsx)(n.h3,{id:"install-google-cloud-sdk",children:"Install Google Cloud SDK"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Install gcloud CLI\ncurl https://sdk.cloud.google.com | bash\nexec -l $SHELL\n\n# Initialize and authenticate\ngcloud init\ngcloud auth application-default login\n"})}),"\n",(0,a.jsx)(n.h3,{id:"install-vertex-ai-sdk",children:"Install Vertex AI SDK"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Install Python SDK\npip install google-cloud-aiplatform\n\n# Install additional ML libraries\npip install pandas scikit-learn tensorflow torch transformers\n"})}),"\n",(0,a.jsx)(n.h3,{id:"enable-required-apis",children:"Enable Required APIs"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Enable Vertex AI API\ngcloud services enable aiplatform.googleapis.com\ngcloud services enable compute.googleapis.com\ngcloud services enable storage.googleapis.com\n"})}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-basic-configuration",children:"\ud83c\udfd7\ufe0f Basic Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"set-environment-variables",children:"Set Environment Variables"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'export PROJECT_ID="your-project-id"\nexport REGION="us-central1"\nexport BUCKET_NAME="your-ml-bucket"\n\n# Create storage bucket\ngsutil mb -p $PROJECT_ID -l $REGION gs://$BUCKET_NAME\n'})}),"\n",(0,a.jsx)(n.h3,{id:"initialize-vertex-ai",children:"Initialize Vertex AI"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from google.cloud import aiplatform\n\n# Initialize Vertex AI\naiplatform.init(\n    project="your-project-id",\n    location="us-central1",\n    staging_bucket="gs://your-ml-bucket"\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-core-features",children:"\ud83d\ude80 Core Features"}),"\n",(0,a.jsx)(n.h3,{id:"1-automl-training",children:"1. AutoML Training"}),"\n",(0,a.jsx)(n.h4,{id:"image-classification",children:"Image Classification"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from google.cloud import aiplatform\n\n# Create AutoML image dataset\ndataset = aiplatform.ImageDataset.create(\n    display_name="image-classification-dataset",\n    gcs_source="gs://your-bucket/image-data.csv",\n    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n)\n\n# Create and run AutoML training job\njob = aiplatform.AutoMLImageTrainingJob(\n    display_name="automl-image-training",\n    prediction_type="classification",\n    multi_label=False,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name="automl-image-model",\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    budget_milli_node_hours=8000,\n)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"tabular-data",children:"Tabular Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create tabular dataset\ndataset = aiplatform.TabularDataset.create(\n    display_name="tabular-dataset",\n    gcs_source="gs://your-bucket/tabular-data.csv",\n)\n\n# AutoML tabular training\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name="automl-tabular-training",\n    optimization_prediction_type="classification",\n    optimization_objective="minimize-log-loss",\n)\n\nmodel = job.run(\n    dataset=dataset,\n    target_column="target",\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    budget_milli_node_hours=1000,\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-custom-training",children:"2. Custom Training"}),"\n",(0,a.jsx)(n.h4,{id:"custom-training-script",children:"Custom Training Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# training_script.py\nimport argparse\nimport os\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport joblib\nfrom google.cloud import storage\n\ndef train_model():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data-path', type=str, required=True)\n    parser.add_argument('--model-dir', type=str, required=True)\n    args = parser.parse_args()\n    \n    # Load data\n    data = pd.read_csv(args.data_path)\n    X = data.drop('target', axis=1)\n    y = data['target']\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X, y)\n    \n    # Save model\n    os.makedirs(args.model_dir, exist_ok=True)\n    joblib.dump(model, os.path.join(args.model_dir, 'model.joblib'))\n    \n    print(f\"Model saved to {args.model_dir}\")\n\nif __name__ == \"__main__\":\n    train_model()\n"})}),"\n",(0,a.jsx)(n.h4,{id:"submit-custom-training-job",children:"Submit Custom Training Job"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from google.cloud import aiplatform\n\n# Create custom training job\njob = aiplatform.CustomTrainingJob(\n    display_name="custom-training-job",\n    script_path="training_script.py",\n    container_uri="gcr.io/cloud-aiplatform/training/scikit-learn-cpu.0-23:latest",\n    requirements=["pandas", "scikit-learn", "joblib"],\n    model_serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/sklearn-cpu.0-23:latest",\n)\n\n# Run training job\nmodel = job.run(\n    dataset=dataset,\n    model_display_name="custom-sklearn-model",\n    args=[\n        "--data-path", "/gcs/your-bucket/training-data.csv",\n        "--model-dir", "/gcs/your-bucket/model-output"\n    ],\n    replica_count=1,\n    machine_type="n1-standard-4",\n    accelerator_type="NVIDIA_TESLA_K80",\n    accelerator_count=1,\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-model-deployment",children:"3. Model Deployment"}),"\n",(0,a.jsx)(n.h4,{id:"deploy-to-endpoint",children:"Deploy to Endpoint"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create endpoint\nendpoint = aiplatform.Endpoint.create(\n    display_name="prediction-endpoint",\n    project=PROJECT_ID,\n    location=REGION,\n)\n\n# Deploy model to endpoint\nmodel.deploy(\n    endpoint=endpoint,\n    deployed_model_display_name="deployed-model",\n    machine_type="n1-standard-2",\n    min_replica_count=1,\n    max_replica_count=3,\n    accelerator_type="NVIDIA_TESLA_T4",\n    accelerator_count=1,\n)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"batch-prediction",children:"Batch Prediction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create batch prediction job\nbatch_prediction_job = aiplatform.BatchPredictionJob.create(\n    job_display_name="batch-prediction-job",\n    model_name=model.resource_name,\n    instances_format="csv",\n    gcs_source="gs://your-bucket/prediction-input.csv",\n    gcs_destination_prefix="gs://your-bucket/predictions/",\n    machine_type="n1-standard-4",\n)\n\n# Wait for completion\nbatch_prediction_job.wait()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"4-online-predictions",children:"4. Online Predictions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Make online predictions\ninstances = [\n    {"feature1": 1.0, "feature2": 2.0, "feature3": 3.0},\n    {"feature1": 4.0, "feature2": 5.0, "feature3": 6.0},\n]\n\npredictions = endpoint.predict(instances=instances)\nprint(predictions)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-advanced-features",children:"\ud83d\udd27 Advanced Features"}),"\n",(0,a.jsx)(n.h3,{id:"1-hyperparameter-tuning",children:"1. Hyperparameter Tuning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from google.cloud.aiplatform import hyperparameter_tuning as hpt\n\n# Define hyperparameter tuning job\nhp_job = aiplatform.HyperparameterTuningJob(\n    display_name="hp-tuning-job",\n    custom_job=job,\n    metric_spec={\n        "accuracy": "maximize",\n    },\n    parameter_spec={\n        "learning_rate": hpt.DoubleParameterSpec(\n            min=0.001, max=0.1, scale="log"\n        ),\n        "batch_size": hpt.DiscreteParameterSpec(\n            values=[16, 32, 64, 128]\n        ),\n    },\n    max_trial_count=20,\n    parallel_trial_count=5,\n)\n\n# Run hyperparameter tuning\nhp_job.run()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-model-monitoring",children:"2. Model Monitoring"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create model monitoring job\nmonitoring_job = aiplatform.ModelDeploymentMonitoringJob.create(\n    display_name="model-monitoring",\n    endpoint=endpoint,\n    deployed_model_ids=[deployed_model.id],\n    alert_config=aiplatform.model_monitoring.AlertConfig(\n        email_alert_config=aiplatform.model_monitoring.EmailAlertConfig(\n            user_emails=["admin@company.com"]\n        )\n    ),\n    schedule_config=aiplatform.model_monitoring.ScheduleConfig(cron="0 */6 * * *"),\n    logging_sampling_strategy=aiplatform.model_monitoring.RandomSampleConfig(\n        sample_rate=0.1\n    ),\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-feature-store",children:"3. Feature Store"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Create feature store\nfeature_store = aiplatform.Featurestore.create(\n    featurestore_id="my-feature-store",\n    online_store_fixed_node_count=1,\n)\n\n# Create entity type\nentity_type = feature_store.create_entity_type(\n    entity_type_id="user",\n    description="User entity for recommendation system",\n)\n\n# Create features\nfeatures = [\n    aiplatform.Feature(\n        feature_id="age",\n        value_type="INT64",\n        description="User age",\n    ),\n    aiplatform.Feature(\n        feature_id="location",\n        value_type="STRING",\n        description="User location",\n    ),\n]\n\nentity_type.create_features(features)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"4-pipelines-with-kubeflow",children:"4. Pipelines with Kubeflow"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from kfp.v2 import dsl\nfrom kfp.v2.dsl import component, pipeline\n\n@component(\n    packages_to_install=["pandas", "scikit-learn"],\n    base_image="python:3.8"\n)\ndef train_model_component(\n    dataset_path: str,\n    model_path: str,\n) -> str:\n    import pandas as pd\n    from sklearn.ensemble import RandomForestClassifier\n    import joblib\n    \n    # Load and train\n    data = pd.read_csv(dataset_path)\n    X = data.drop(\'target\', axis=1)\n    y = data[\'target\']\n    \n    model = RandomForestClassifier()\n    model.fit(X, y)\n    \n    # Save model\n    joblib.dump(model, model_path)\n    return model_path\n\n@pipeline(name="ml-training-pipeline")\ndef ml_pipeline(\n    dataset_path: str = "gs://your-bucket/data.csv",\n    model_path: str = "gs://your-bucket/model.joblib"\n):\n    train_task = train_model_component(\n        dataset_path=dataset_path,\n        model_path=model_path\n    )\n\n# Compile and run pipeline\nfrom kfp.v2 import compiler\n\ncompiler.Compiler().compile(\n    pipeline_func=ml_pipeline,\n    package_path="ml_pipeline.json"\n)\n\n# Submit pipeline\njob = aiplatform.PipelineJob(\n    display_name="ml-training-pipeline",\n    template_path="ml_pipeline.json",\n    pipeline_root="gs://your-bucket/pipeline-root",\n)\n\njob.run()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-docker-configuration",children:"\ud83d\udc33 Docker Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"custom-training-container",children:"Custom Training Container"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile\nFROM gcr.io/deeplearning-platform-release/base-cpu\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY training_script.py .\nCOPY model_utils.py .\n\nENTRYPOINT ["python", "training_script.py"]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"build-and-push-container",children:"Build and Push Container"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Build container\ndocker build -t gcr.io/$PROJECT_ID/custom-trainer:latest .\n\n# Push to Container Registry\ndocker push gcr.io/$PROJECT_ID/custom-trainer:latest\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-monitoring--logging",children:"\ud83d\udcca Monitoring & Logging"}),"\n",(0,a.jsx)(n.h3,{id:"model-performance-monitoring",children:"Model Performance Monitoring"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Set up model monitoring\nfrom google.cloud import aiplatform_v1\n\nclient = aiplatform_v1.ModelServiceClient()\n\n# Get model evaluation metrics\nevaluations = client.list_model_evaluations(\n    parent=model.resource_name\n)\n\nfor evaluation in evaluations:\n    print(f"Evaluation: {evaluation.display_name}")\n    print(f"Metrics: {evaluation.metrics}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"logging-configuration",children:"Logging Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import logging\nfrom google.cloud import logging as cloud_logging\n\n# Set up Cloud Logging\ncloud_logging_client = cloud_logging.Client()\ncloud_logging_client.setup_logging()\n\n# Use standard logging\nlogging.info("Training started")\nlogging.error("Training failed with error: %s", error_message)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-security-best-practices",children:"\ud83d\udd12 Security Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"iam-configuration",children:"IAM Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Create service account\ngcloud iam service-accounts create vertex-ai-sa \\\n    --display-name="Vertex AI Service Account"\n\n# Grant necessary permissions\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member="serviceAccount:vertex-ai-sa@$PROJECT_ID.iam.gserviceaccount.com" \\\n    --role="roles/aiplatform.user"\n\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member="serviceAccount:vertex-ai-sa@$PROJECT_ID.iam.gserviceaccount.com" \\\n    --role="roles/storage.admin"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"encryption-configuration",children:"Encryption Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Use customer-managed encryption keys\nfrom google.cloud.aiplatform import encryption_spec\n\nencryption_spec = encryption_spec.EncryptionSpec(\n    kms_key_name="projects/your-project/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key"\n)\n\n# Apply to training job\njob = aiplatform.CustomTrainingJob(\n    display_name="encrypted-training",\n    script_path="training_script.py",\n    container_uri="gcr.io/your-project/trainer:latest",\n    encryption_spec=encryption_spec,\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-production-deployment",children:"\ud83d\ude80 Production Deployment"}),"\n",(0,a.jsx)(n.h3,{id:"multi-region-deployment",children:"Multi-Region Deployment"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Deploy to multiple regions\nregions = ["us-central1", "europe-west1", "asia-southeast1"]\n\nfor region in regions:\n    aiplatform.init(project=PROJECT_ID, location=region)\n    \n    endpoint = aiplatform.Endpoint.create(\n        display_name=f"production-endpoint-{region}",\n    )\n    \n    model.deploy(\n        endpoint=endpoint,\n        deployed_model_display_name=f"model-{region}",\n        machine_type="n1-standard-4",\n        min_replica_count=2,\n        max_replica_count=10,\n    )\n'})}),"\n",(0,a.jsx)(n.h3,{id:"ab-testing-setup",children:"A/B Testing Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Deploy multiple model versions\nmodel_v1.deploy(\n    endpoint=endpoint,\n    deployed_model_display_name="model-v1",\n    traffic_percentage=80,\n    machine_type="n1-standard-2",\n)\n\nmodel_v2.deploy(\n    endpoint=endpoint,\n    deployed_model_display_name="model-v2",\n    traffic_percentage=20,\n    machine_type="n1-standard-2",\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-performance-optimization",children:"\ud83d\udcc8 Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"batch-size-optimization",children:"Batch Size Optimization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Optimize batch predictions\nbatch_prediction_job = aiplatform.BatchPredictionJob.create(\n    job_display_name="optimized-batch-prediction",\n    model_name=model.resource_name,\n    instances_format="jsonl",\n    gcs_source="gs://your-bucket/large-dataset.jsonl",\n    gcs_destination_prefix="gs://your-bucket/predictions/",\n    machine_type="n1-highmem-8",\n    starting_replica_count=5,\n    max_replica_count=20,\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Use GPU for training\njob = aiplatform.CustomTrainingJob(\n    display_name="gpu-training",\n    script_path="training_script.py",\n    container_uri="gcr.io/deeplearning-platform-release/pytorch-gpu.1-9",\n)\n\nmodel = job.run(\n    replica_count=1,\n    machine_type="n1-standard-8",\n    accelerator_type="NVIDIA_TESLA_V100",\n    accelerator_count=2,\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-troubleshooting",children:"\ud83d\udd0d Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Training Job Failures"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Check job logs\ngcloud ai custom-jobs describe JOB_ID --region=REGION\n\n# View detailed logs\ngcloud logging read "resource.type=ml_job AND resource.labels.job_id=JOB_ID"\n'})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Prediction Errors"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Test endpoint health\ntry:\n    predictions = endpoint.predict(instances=test_instances)\n    print("Endpoint is healthy")\nexcept Exception as e:\n    print(f"Endpoint error: {e}")\n'})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Resource Quotas"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Check quotas\ngcloud compute project-info describe --project=$PROJECT_ID\n\n# Request quota increase\ngcloud alpha compute quotas list --filter="service:compute.googleapis.com"\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-additional-resources",children:"\ud83d\udcda Additional Resources"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai/docs",children:"Vertex AI Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://googleapis.dev/python/aiplatform/latest/",children:"Vertex AI Python SDK"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/GoogleCloudPlatform/vertex-ai-samples",children:"Vertex AI Samples"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning",children:"MLOps with Vertex AI"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Google Vertex AI provides a comprehensive platform for the entire ML lifecycle, from experimentation to production deployment, with enterprise-grade security and scalability features."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const a={},o=t.createContext(a);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);