"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7645],{5012:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>_,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"Data-Processing/apache-kafka","title":"Apache Kafka","description":"Apache Kafka is a distributed streaming platform for building real-time data pipelines and streaming applications. Learn how to set up Kafka with Docker and build streaming applications.","source":"@site/docs/Data-Processing/apache-kafka.md","sourceDirName":"Data-Processing","slug":"/Data-Processing/ApacheKafka","permalink":"/docs/Data-Processing/ApacheKafka","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Data-Processing/apache-kafka.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Apache Kafka","description":"Apache Kafka is a distributed streaming platform for building real-time data pipelines and streaming applications. Learn how to set up Kafka with Docker and build streaming applications.","slug":"/Data-Processing/ApacheKafka","keywords":["Apache Kafka","streaming platform","message broker","event streaming","real-time data","distributed systems","data pipelines","stream processing","pub-sub messaging","event-driven architecture"]},"sidebar":"tutorialSidebar","previous":{"title":"Data-Processing","permalink":"/docs/category/data-processing"},"next":{"title":"Apache Airflow","permalink":"/docs/Data-Processing/ApacheAirflow"}}');var a=t(4848),r=t(8453);const o={sidebar_position:1,title:"Apache Kafka",description:"Apache Kafka is a distributed streaming platform for building real-time data pipelines and streaming applications. Learn how to set up Kafka with Docker and build streaming applications.",slug:"/Data-Processing/ApacheKafka",keywords:["Apache Kafka","streaming platform","message broker","event streaming","real-time data","distributed systems","data pipelines","stream processing","pub-sub messaging","event-driven architecture"]},i="\ud83c\udf0a Apache Kafka - Distributed Streaming Platform",c={},l=[{value:"Set Up Kafka with Docker",id:"set-up-kafka-with-docker",level:2},{value:"Basic Kafka Operations",id:"basic-kafka-operations",level:2},{value:"Topic Management",id:"topic-management",level:3},{value:"Producer and Consumer",id:"producer-and-consumer",level:3},{value:"Python Kafka Applications",id:"python-kafka-applications",level:2},{value:"Simple Producer",id:"simple-producer",level:3},{value:"Simple Consumer",id:"simple-consumer",level:3},{value:"Kafka Streams Application",id:"kafka-streams-application",level:2},{value:"Stream Processing with Python",id:"stream-processing-with-python",level:3},{value:"Kafka Connect Configuration",id:"kafka-connect-configuration",level:2},{value:"Database Source Connector",id:"database-source-connector",level:3},{value:"Elasticsearch Sink Connector",id:"elasticsearch-sink-connector",level:3},{value:"Monitoring and Operations",id:"monitoring-and-operations",level:2},{value:"Kafka Monitoring Script",id:"kafka-monitoring-script",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"-apache-kafka---distributed-streaming-platform",children:"\ud83c\udf0a Apache Kafka - Distributed Streaming Platform"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Apache Kafka"})," is a ",(0,a.jsx)(n.strong,{children:"distributed streaming platform"})," that enables you to build ",(0,a.jsx)(n.strong,{children:"real-time data pipelines"})," and ",(0,a.jsx)(n.strong,{children:"streaming applications"}),". It provides high-throughput, low-latency ",(0,a.jsx)(n.strong,{children:"publish-subscribe messaging"})," and ",(0,a.jsx)(n.strong,{children:"stream processing"})," capabilities."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"set-up-kafka-with-docker",children:"Set Up Kafka with Docker"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create a file named docker-compose.yml"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.4.0\n    container_name: kafka-zookeeper\n    restart: unless-stopped\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    volumes:\n      - zookeeper-data:/var/lib/zookeeper/data\n      - zookeeper-logs:/var/lib/zookeeper/log\n    ports:\n      - "2181:2181"\n\n  kafka-1:\n    image: confluentinc/cp-kafka:7.4.0\n    container_name: kafka-broker-1\n    restart: unless-stopped\n    depends_on:\n      - zookeeper\n    ports:\n      - "9092:9092"\n      - "19092:19092"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_MIN_IN_SYNC_REPLICAS: 2\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19092\n      KAFKA_JMX_HOSTNAME: localhost\n    volumes:\n      - kafka-1-data:/var/lib/kafka/data\n\n  kafka-2:\n    image: confluentinc/cp-kafka:7.4.0\n    container_name: kafka-broker-2\n    restart: unless-stopped\n    depends_on:\n      - zookeeper\n    ports:\n      - "9093:9093"\n      - "19093:19093"\n    environment:\n      KAFKA_BROKER_ID: 2\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29093,PLAINTEXT_HOST://localhost:9093\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:9093\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_MIN_IN_SYNC_REPLICAS: 2\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19093\n      KAFKA_JMX_HOSTNAME: localhost\n    volumes:\n      - kafka-2-data:/var/lib/kafka/data\n\n  kafka-3:\n    image: confluentinc/cp-kafka:7.4.0\n    container_name: kafka-broker-3\n    restart: unless-stopped\n    depends_on:\n      - zookeeper\n    ports:\n      - "9094:9094"\n      - "19094:19094"\n    environment:\n      KAFKA_BROKER_ID: 3\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:29094,PLAINTEXT_HOST://localhost:9094\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29094,PLAINTEXT_HOST://0.0.0.0:9094\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_MIN_IN_SYNC_REPLICAS: 2\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19094\n      KAFKA_JMX_HOSTNAME: localhost\n    volumes:\n      - kafka-3-data:/var/lib/kafka/data\n\n  # Kafka Connect\n  kafka-connect:\n    image: confluentinc/cp-kafka-connect:7.4.0\n    container_name: kafka-connect\n    restart: unless-stopped\n    depends_on:\n      - kafka-1\n      - kafka-2\n      - kafka-3\n    ports:\n      - "8083:8083"\n    environment:\n      CONNECT_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094\n      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect\n      CONNECT_GROUP_ID: kafka-connect-cluster\n      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs\n      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets\n      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status\n      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3\n      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3\n      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3\n      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter\n      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"\n      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components\n    volumes:\n      - kafka-connect-data:/var/lib/kafka-connect\n\n  # Schema Registry\n  schema-registry:\n    image: confluentinc/cp-schema-registry:7.4.0\n    container_name: kafka-schema-registry\n    restart: unless-stopped\n    depends_on:\n      - kafka-1\n      - kafka-2\n      - kafka-3\n    ports:\n      - "8081:8081"\n    environment:\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094\n      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081\n\n  # Kafka UI\n  kafka-ui:\n    image: provectuslabs/kafka-ui:latest\n    container_name: kafka-ui\n    restart: unless-stopped\n    depends_on:\n      - kafka-1\n      - kafka-2\n      - kafka-3\n    ports:\n      - "8080:8080"\n    environment:\n      KAFKA_CLUSTERS_0_NAME: local\n      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094\n      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181\n      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081\n      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect\n      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083\n\nvolumes:\n  zookeeper-data:\n  zookeeper-logs:\n  kafka-1-data:\n  kafka-2-data:\n  kafka-3-data:\n  kafka-connect-data:\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Start Kafka cluster:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker compose up -d\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Access Kafka UI:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'echo "Kafka UI: http://localhost:8080"\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"basic-kafka-operations",children:"Basic Kafka Operations"}),"\n",(0,a.jsx)(n.h3,{id:"topic-management",children:"Topic Management"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create and manage topics:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Create a topic\ndocker exec kafka-broker-1 kafka-topics --create \\\n  --bootstrap-server localhost:9092 \\\n  --topic user-events \\\n  --partitions 3 \\\n  --replication-factor 3\n\n# List topics\ndocker exec kafka-broker-1 kafka-topics --list \\\n  --bootstrap-server localhost:9092\n\n# Describe topic\ndocker exec kafka-broker-1 kafka-topics --describe \\\n  --bootstrap-server localhost:9092 \\\n  --topic user-events\n\n# Delete topic\ndocker exec kafka-broker-1 kafka-topics --delete \\\n  --bootstrap-server localhost:9092 \\\n  --topic user-events\n"})}),"\n",(0,a.jsx)(n.h3,{id:"producer-and-consumer",children:"Producer and Consumer"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Test producer and consumer:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Start a console producer\ndocker exec -it kafka-broker-1 kafka-console-producer \\\n  --bootstrap-server localhost:9092 \\\n  --topic user-events\n\n# Start a console consumer\ndocker exec -it kafka-broker-1 kafka-console-consumer \\\n  --bootstrap-server localhost:9092 \\\n  --topic user-events \\\n  --from-beginning\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"python-kafka-applications",children:"Python Kafka Applications"}),"\n",(0,a.jsx)(n.h3,{id:"simple-producer",children:"Simple Producer"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create producer.py:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nfrom kafka import KafkaProducer\nimport json\nimport time\nimport random\nfrom datetime import datetime\n\nclass EventProducer:\n    def __init__(self, bootstrap_servers=['localhost:9092']):\n        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n            key_serializer=lambda k: k.encode('utf-8') if k else None,\n            acks='all',  # Wait for all replicas to acknowledge\n            retries=3,\n            batch_size=16384,\n            linger_ms=10,\n            buffer_memory=33554432\n        )\n    \n    def send_user_event(self, user_id, event_type, data=None):\n        \"\"\"Send a user event to Kafka\"\"\"\n        event = {\n            'user_id': user_id,\n            'event_type': event_type,\n            'timestamp': datetime.utcnow().isoformat(),\n            'data': data or {}\n        }\n        \n        # Use user_id as partition key for ordering\n        future = self.producer.send(\n            topic='user-events',\n            key=str(user_id),\n            value=event\n        )\n        \n        # Optional: wait for confirmation\n        try:\n            record_metadata = future.get(timeout=10)\n            print(f\"Event sent to topic {record_metadata.topic} \"\n                  f\"partition {record_metadata.partition} \"\n                  f\"offset {record_metadata.offset}\")\n        except Exception as e:\n            print(f\"Failed to send event: {e}\")\n    \n    def send_order_event(self, order_id, user_id, event_type, order_data):\n        \"\"\"Send an order event to Kafka\"\"\"\n        event = {\n            'order_id': order_id,\n            'user_id': user_id,\n            'event_type': event_type,\n            'timestamp': datetime.utcnow().isoformat(),\n            'order_data': order_data\n        }\n        \n        self.producer.send(\n            topic='order-events',\n            key=str(order_id),\n            value=event\n        )\n    \n    def generate_sample_events(self, num_events=100):\n        \"\"\"Generate sample events for testing\"\"\"\n        event_types = ['login', 'logout', 'page_view', 'purchase', 'search']\n        \n        for i in range(num_events):\n            user_id = random.randint(1, 1000)\n            event_type = random.choice(event_types)\n            \n            data = {}\n            if event_type == 'page_view':\n                data = {'page': f'/page/{random.randint(1, 100)}'}\n            elif event_type == 'purchase':\n                data = {\n                    'product_id': random.randint(1, 500),\n                    'amount': round(random.uniform(10, 1000), 2)\n                }\n            elif event_type == 'search':\n                data = {'query': f'search term {random.randint(1, 50)}'}\n            \n            self.send_user_event(user_id, event_type, data)\n            time.sleep(0.1)  # Small delay between events\n    \n    def close(self):\n        \"\"\"Close the producer\"\"\"\n        self.producer.flush()\n        self.producer.close()\n\n# Usage\nif __name__ == \"__main__\":\n    producer = EventProducer()\n    \n    try:\n        # Send individual events\n        producer.send_user_event(123, 'login', {'ip': '192.168.1.1'})\n        producer.send_user_event(123, 'page_view', {'page': '/dashboard'})\n        \n        # Generate sample events\n        print(\"Generating sample events...\")\n        producer.generate_sample_events(50)\n        \n    finally:\n        producer.close()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"simple-consumer",children:"Simple Consumer"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create consumer.py:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nfrom kafka import KafkaConsumer\nimport json\nimport logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EventConsumer:\n    def __init__(self, topics, group_id, bootstrap_servers=[\'localhost:9092\']):\n        self.consumer = KafkaConsumer(\n            *topics,\n            bootstrap_servers=bootstrap_servers,\n            group_id=group_id,\n            value_deserializer=lambda m: json.loads(m.decode(\'utf-8\')),\n            key_deserializer=lambda k: k.decode(\'utf-8\') if k else None,\n            auto_offset_reset=\'earliest\',\n            enable_auto_commit=True,\n            auto_commit_interval_ms=1000,\n            session_timeout_ms=30000,\n            heartbeat_interval_ms=10000\n        )\n    \n    def process_user_event(self, event):\n        """Process a user event"""\n        user_id = event.get(\'user_id\')\n        event_type = event.get(\'event_type\')\n        timestamp = event.get(\'timestamp\')\n        data = event.get(\'data\', {})\n        \n        logger.info(f"Processing user event: {event_type} for user {user_id}")\n        \n        # Process different event types\n        if event_type == \'login\':\n            self.handle_login(user_id, data)\n        elif event_type == \'purchase\':\n            self.handle_purchase(user_id, data)\n        elif event_type == \'page_view\':\n            self.handle_page_view(user_id, data)\n        else:\n            logger.info(f"Unknown event type: {event_type}")\n    \n    def handle_login(self, user_id, data):\n        """Handle login event"""\n        ip = data.get(\'ip\', \'unknown\')\n        logger.info(f"User {user_id} logged in from {ip}")\n        \n        # Example: Update user last login time\n        # database.update_user_last_login(user_id, datetime.utcnow())\n    \n    def handle_purchase(self, user_id, data):\n        """Handle purchase event"""\n        product_id = data.get(\'product_id\')\n        amount = data.get(\'amount\')\n        logger.info(f"User {user_id} purchased product {product_id} for ${amount}")\n        \n        # Example: Update analytics, send notifications\n        # analytics.track_purchase(user_id, product_id, amount)\n        # notification.send_purchase_confirmation(user_id)\n    \n    def handle_page_view(self, user_id, data):\n        """Handle page view event"""\n        page = data.get(\'page\')\n        logger.info(f"User {user_id} viewed page {page}")\n        \n        # Example: Update page view analytics\n        # analytics.track_page_view(user_id, page)\n    \n    def consume_events(self):\n        """Start consuming events"""\n        logger.info("Starting event consumer...")\n        \n        try:\n            for message in self.consumer:\n                try:\n                    topic = message.topic\n                    partition = message.partition\n                    offset = message.offset\n                    key = message.key\n                    value = message.value\n                    \n                    logger.debug(f"Received message from {topic}[{partition}] at offset {offset}")\n                    \n                    if topic == \'user-events\':\n                        self.process_user_event(value)\n                    elif topic == \'order-events\':\n                        self.process_order_event(value)\n                    else:\n                        logger.warning(f"Unknown topic: {topic}")\n                        \n                except Exception as e:\n                    logger.error(f"Error processing message: {e}")\n                    # In production, you might want to send to a dead letter queue\n                    \n        except KeyboardInterrupt:\n            logger.info("Consumer interrupted by user")\n        finally:\n            self.consumer.close()\n    \n    def process_order_event(self, event):\n        """Process an order event"""\n        order_id = event.get(\'order_id\')\n        user_id = event.get(\'user_id\')\n        event_type = event.get(\'event_type\')\n        \n        logger.info(f"Processing order event: {event_type} for order {order_id}")\n\n# Usage\nif __name__ == "__main__":\n    # Create consumer for user events\n    consumer = EventConsumer(\n        topics=[\'user-events\', \'order-events\'],\n        group_id=\'event-processor-group\'\n    )\n    \n    # Start consuming\n    consumer.consume_events()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"kafka-streams-application",children:"Kafka Streams Application"}),"\n",(0,a.jsx)(n.h3,{id:"stream-processing-with-python",children:"Stream Processing with Python"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create stream_processor.py:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nfrom kafka import KafkaConsumer, KafkaProducer\nimport json\nimport logging\nfrom collections import defaultdict, deque\nfrom datetime import datetime, timedelta\nimport threading\nimport time\n\nlogger = logging.getLogger(__name__)\n\nclass StreamProcessor:\n    def __init__(self, bootstrap_servers=['localhost:9092']):\n        self.bootstrap_servers = bootstrap_servers\n        \n        # Consumer for input streams\n        self.consumer = KafkaConsumer(\n            'user-events',\n            bootstrap_servers=bootstrap_servers,\n            group_id='stream-processor',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n            auto_offset_reset='earliest'\n        )\n        \n        # Producer for output streams\n        self.producer = KafkaProducer(\n            bootstrap_servers=bootstrap_servers,\n            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n        )\n        \n        # In-memory state stores (in production, use external stores)\n        self.user_sessions = defaultdict(list)\n        self.page_view_counts = defaultdict(int)\n        self.purchase_totals = defaultdict(float)\n        self.recent_events = defaultdict(lambda: deque(maxlen=100))\n        \n        # Start background tasks\n        self.running = True\n        self.start_background_tasks()\n    \n    def start_background_tasks(self):\n        \"\"\"Start background processing tasks\"\"\"\n        # Windowed aggregations\n        threading.Thread(target=self.windowed_aggregations, daemon=True).start()\n        \n        # Session detection\n        threading.Thread(target=self.session_detection, daemon=True).start()\n    \n    def process_events(self):\n        \"\"\"Main event processing loop\"\"\"\n        logger.info(\"Starting stream processor...\")\n        \n        try:\n            for message in self.consumer:\n                event = message.value\n                self.process_single_event(event)\n                \n        except KeyboardInterrupt:\n            logger.info(\"Stream processor interrupted\")\n        finally:\n            self.cleanup()\n    \n    def process_single_event(self, event):\n        \"\"\"Process a single event\"\"\"\n        user_id = event.get('user_id')\n        event_type = event.get('event_type')\n        timestamp = datetime.fromisoformat(event.get('timestamp'))\n        \n        # Add to recent events for windowed processing\n        self.recent_events[user_id].append({\n            'event': event,\n            'timestamp': timestamp\n        })\n        \n        # Real-time processing\n        if event_type == 'page_view':\n            self.process_page_view(event)\n        elif event_type == 'purchase':\n            self.process_purchase(event)\n        elif event_type == 'login':\n            self.process_login(event)\n        elif event_type == 'logout':\n            self.process_logout(event)\n    \n    def process_page_view(self, event):\n        \"\"\"Process page view events\"\"\"\n        page = event.get('data', {}).get('page', 'unknown')\n        self.page_view_counts[page] += 1\n        \n        # Emit aggregated page view counts\n        if self.page_view_counts[page] % 10 == 0:  # Every 10 views\n            self.emit_page_view_stats(page, self.page_view_counts[page])\n    \n    def process_purchase(self, event):\n        \"\"\"Process purchase events\"\"\"\n        user_id = event.get('user_id')\n        amount = event.get('data', {}).get('amount', 0)\n        \n        self.purchase_totals[user_id] += amount\n        \n        # Emit purchase analytics\n        self.emit_purchase_analytics(user_id, amount, self.purchase_totals[user_id])\n        \n        # Check for high-value customers\n        if self.purchase_totals[user_id] > 1000:\n            self.emit_high_value_customer_alert(user_id, self.purchase_totals[user_id])\n    \n    def process_login(self, event):\n        \"\"\"Process login events\"\"\"\n        user_id = event.get('user_id')\n        timestamp = datetime.fromisoformat(event.get('timestamp'))\n        \n        # Start new session\n        self.user_sessions[user_id].append({\n            'start_time': timestamp,\n            'end_time': None,\n            'events': []\n        })\n    \n    def process_logout(self, event):\n        \"\"\"Process logout events\"\"\"\n        user_id = event.get('user_id')\n        timestamp = datetime.fromisoformat(event.get('timestamp'))\n        \n        # End current session\n        if self.user_sessions[user_id]:\n            current_session = self.user_sessions[user_id][-1]\n            current_session['end_time'] = timestamp\n            \n            # Emit session analytics\n            self.emit_session_analytics(user_id, current_session)\n    \n    def windowed_aggregations(self):\n        \"\"\"Perform windowed aggregations every minute\"\"\"\n        while self.running:\n            try:\n                current_time = datetime.utcnow()\n                window_start = current_time - timedelta(minutes=5)\n                \n                # Process 5-minute windows\n                for user_id, events in self.recent_events.items():\n                    window_events = [\n                        e for e in events \n                        if e['timestamp'] >= window_start\n                    ]\n                    \n                    if window_events:\n                        self.process_windowed_events(user_id, window_events)\n                \n                time.sleep(60)  # Run every minute\n                \n            except Exception as e:\n                logger.error(f\"Error in windowed aggregations: {e}\")\n    \n    def process_windowed_events(self, user_id, events):\n        \"\"\"Process events in a time window\"\"\"\n        event_counts = defaultdict(int)\n        \n        for event_data in events:\n            event = event_data['event']\n            event_type = event.get('event_type')\n            event_counts[event_type] += 1\n        \n        # Emit windowed statistics\n        self.emit_windowed_stats(user_id, dict(event_counts))\n    \n    def session_detection(self):\n        \"\"\"Detect user sessions based on activity patterns\"\"\"\n        while self.running:\n            try:\n                current_time = datetime.utcnow()\n                session_timeout = timedelta(minutes=30)\n                \n                for user_id, events in self.recent_events.items():\n                    if events:\n                        last_event_time = events[-1]['timestamp']\n                        \n                        # Check for session timeout\n                        if current_time - last_event_time > session_timeout:\n                            self.emit_session_timeout(user_id, last_event_time)\n                \n                time.sleep(300)  # Check every 5 minutes\n                \n            except Exception as e:\n                logger.error(f\"Error in session detection: {e}\")\n    \n    def emit_page_view_stats(self, page, count):\n        \"\"\"Emit page view statistics\"\"\"\n        stats = {\n            'type': 'page_view_stats',\n            'page': page,\n            'count': count,\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('analytics-events', value=stats)\n        logger.info(f\"Page {page} has {count} views\")\n    \n    def emit_purchase_analytics(self, user_id, amount, total):\n        \"\"\"Emit purchase analytics\"\"\"\n        analytics = {\n            'type': 'purchase_analytics',\n            'user_id': user_id,\n            'purchase_amount': amount,\n            'total_spent': total,\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('analytics-events', value=analytics)\n    \n    def emit_high_value_customer_alert(self, user_id, total_spent):\n        \"\"\"Emit high-value customer alert\"\"\"\n        alert = {\n            'type': 'high_value_customer',\n            'user_id': user_id,\n            'total_spent': total_spent,\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('alerts', value=alert)\n        logger.info(f\"High-value customer alert: User {user_id} spent ${total_spent}\")\n    \n    def emit_session_analytics(self, user_id, session):\n        \"\"\"Emit session analytics\"\"\"\n        duration = None\n        if session['end_time']:\n            duration = (session['end_time'] - session['start_time']).total_seconds()\n        \n        analytics = {\n            'type': 'session_analytics',\n            'user_id': user_id,\n            'session_start': session['start_time'].isoformat(),\n            'session_end': session['end_time'].isoformat() if session['end_time'] else None,\n            'duration_seconds': duration,\n            'event_count': len(session['events']),\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('analytics-events', value=analytics)\n    \n    def emit_windowed_stats(self, user_id, event_counts):\n        \"\"\"Emit windowed statistics\"\"\"\n        stats = {\n            'type': 'windowed_stats',\n            'user_id': user_id,\n            'window_duration_minutes': 5,\n            'event_counts': event_counts,\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('analytics-events', value=stats)\n    \n    def emit_session_timeout(self, user_id, last_activity):\n        \"\"\"Emit session timeout event\"\"\"\n        timeout_event = {\n            'type': 'session_timeout',\n            'user_id': user_id,\n            'last_activity': last_activity.isoformat(),\n            'timestamp': datetime.utcnow().isoformat()\n        }\n        \n        self.producer.send('session-events', value=timeout_event)\n    \n    def cleanup(self):\n        \"\"\"Cleanup resources\"\"\"\n        self.running = False\n        self.consumer.close()\n        self.producer.close()\n\n# Usage\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    \n    processor = StreamProcessor()\n    processor.process_events()\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"kafka-connect-configuration",children:"Kafka Connect Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"database-source-connector",children:"Database Source Connector"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create connectors/jdbc-source.json:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "name": "jdbc-source-connector",\n  "config": {\n    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",\n    "connection.url": "jdbc:postgresql://postgres:5432/myapp",\n    "connection.user": "postgres",\n    "connection.password": "password",\n    "table.whitelist": "users,orders,products",\n    "mode": "incrementing",\n    "incrementing.column.name": "id",\n    "topic.prefix": "db-",\n    "poll.interval.ms": 5000,\n    "batch.max.rows": 1000,\n    "transforms": "createKey,extractInt",\n    "transforms.createKey.type": "org.apache.kafka.connect.transforms.ValueToKey",\n    "transforms.createKey.fields": "id",\n    "transforms.extractInt.type": "org.apache.kafka.connect.transforms.ExtractField$Key",\n    "transforms.extractInt.field": "id"\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"elasticsearch-sink-connector",children:"Elasticsearch Sink Connector"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create connectors/elasticsearch-sink.json:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "name": "elasticsearch-sink-connector",\n  "config": {\n    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",\n    "topics": "user-events,order-events,analytics-events",\n    "connection.url": "http://elasticsearch:9200",\n    "type.name": "_doc",\n    "key.ignore": "false",\n    "schema.ignore": "true",\n    "batch.size": 100,\n    "max.buffered.records": 1000,\n    "flush.timeout.ms": 10000,\n    "transforms": "addTimestamp",\n    "transforms.addTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",\n    "transforms.addTimestamp.timestamp.field": "kafka_timestamp"\n  }\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Deploy connectors:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Deploy JDBC source connector\ncurl -X POST http://localhost:8083/connectors \\\n  -H "Content-Type: application/json" \\\n  -d @connectors/jdbc-source.json\n\n# Deploy Elasticsearch sink connector\ncurl -X POST http://localhost:8083/connectors \\\n  -H "Content-Type: application/json" \\\n  -d @connectors/elasticsearch-sink.json\n\n# Check connector status\ncurl http://localhost:8083/connectors/jdbc-source-connector/status\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"monitoring-and-operations",children:"Monitoring and Operations"}),"\n",(0,a.jsx)(n.h3,{id:"kafka-monitoring-script",children:"Kafka Monitoring Script"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"Create monitoring/kafka-monitor.py:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport json\nimport requests\nfrom kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType\nfrom kafka import KafkaConsumer\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass KafkaMonitor:\n    def __init__(self, bootstrap_servers=['localhost:9092']):\n        self.bootstrap_servers = bootstrap_servers\n        self.admin_client = KafkaAdminClient(\n            bootstrap_servers=bootstrap_servers,\n            client_id='kafka-monitor'\n        )\n    \n    def get_cluster_metadata(self):\n        \"\"\"Get cluster metadata\"\"\"\n        metadata = self.admin_client._client.cluster\n        \n        return {\n            'brokers': [\n                {\n                    'id': broker.nodeId,\n                    'host': broker.host,\n                    'port': broker.port\n                }\n                for broker in metadata.brokers()\n            ],\n            'topics': list(metadata.topics()),\n            'controller': metadata.controller.nodeId if metadata.controller else None\n        }\n    \n    def get_topic_details(self, topic_name):\n        \"\"\"Get detailed topic information\"\"\"\n        metadata = self.admin_client._client.cluster\n        topic = metadata.topics.get(topic_name)\n        \n        if not topic:\n            return None\n        \n        partitions = []\n        for partition_id, partition in topic.partitions.items():\n            partitions.append({\n                'partition_id': partition_id,\n                'leader': partition.leader,\n                'replicas': partition.replicas,\n                'isr': partition.isr\n            })\n        \n        return {\n            'name': topic_name,\n            'partitions': partitions,\n            'partition_count': len(partitions),\n            'replication_factor': len(partitions[0]['replicas']) if partitions else 0\n        }\n    \n    def get_consumer_group_info(self, group_id):\n        \"\"\"Get consumer group information\"\"\"\n        try:\n            # This would require additional libraries for full implementation\n            # For now, return basic structure\n            return {\n                'group_id': group_id,\n                'state': 'Stable',  # Would need to query actual state\n                'members': [],  # Would need to query actual members\n                'lag': {}  # Would need to calculate lag\n            }\n        except Exception as e:\n            logger.error(f\"Error getting consumer group info: {e}\")\n            return None\n    \n    def check_broker_health(self):\n        \"\"\"Check broker health\"\"\"\n        try:\n            metadata = self.get_cluster_metadata()\n            healthy_brokers = []\n            \n            for broker in metadata['brokers']:\n                # Simple health check - try to connect\n                try:\n                    consumer = KafkaConsumer(\n                        bootstrap_servers=[f\"{broker['host']}:{broker['port']}\"],\n                        consumer_timeout_ms=5000\n                    )\n                    consumer.close()\n                    healthy_brokers.append(broker['id'])\n                except:\n                    pass\n            \n            return {\n                'total_brokers': len(metadata['brokers']),\n                'healthy_brokers': len(healthy_brokers),\n                'unhealthy_brokers': len(metadata['brokers']) - len(healthy_brokers),\n                'healthy_broker_ids': healthy_brokers\n            }\n        except Exception as e:\n            logger.error(f\"Error checking broker health: {e}\")\n            return None\n    \n    def generate_health_report(self):\n        \"\"\"Generate comprehensive health report\"\"\"\n        report = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'cluster_metadata': self.get_cluster_metadata(),\n            'broker_health': self.check_broker_health(),\n            'topics': {}\n        }\n        \n        # Get details for each topic\n        for topic in report['cluster_metadata']['topics']:\n            report['topics'][topic] = self.get_topic_details(topic)\n        \n        return report\n\n# Usage\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    \n    monitor = KafkaMonitor()\n    health_report = monitor.generate_health_report()\n    \n    print(json.dumps(health_report, indent=2))\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Analytics"}),": Stream processing for real-time dashboards and metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Event-Driven Architecture"}),": Microservices communication through events"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Integration"}),": ETL pipelines and data synchronization between systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Log Aggregation"}),": Centralized logging and log processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IoT Data Processing"}),": High-throughput ingestion and processing of sensor data"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"\u2705 Apache Kafka is now configured for distributed streaming and event processing!"})]})}function _(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var s=t(6540);const a={},r=s.createContext(a);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);